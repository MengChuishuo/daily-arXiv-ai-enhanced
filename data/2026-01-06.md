<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 91]
- [cs.AI](#cs.AI) [Total: 42]
- [stat.ML](#stat.ML) [Total: 10]
- [cs.MA](#cs.MA) [Total: 3]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Horizon Reduction as Information Loss in Offline Reinforcement Learning](https://arxiv.org/abs/2601.00831)
*Uday Kumar Nidadala,Venkata Bhumika Guthi*

Main category: cs.LG

TL;DR: 本文证明离线强化学习中的视野缩减策略会导致不可恢复的信息损失，即使在无限数据和完美函数逼近下，最优策略也可能与次优策略统计不可区分。


<details>
  <summary>Details</summary>
Motivation: 虽然视野缩减是离线强化学习中常用的设计策略，用于缓解长视野信用分配、提高稳定性并实现可扩展学习，但其理论影响尚未得到充分发展。本文旨在揭示视野缩减可能导致的基本信息损失问题。

Method: 将视野缩减形式化为从固定长度轨迹片段中学习，通过理论证明和最小反例马尔可夫决策过程（MDPs）分析，识别了三种不同的结构失效模式。

Result: 证明在固定长度轨迹片段的学习范式下，即使有无限数据和完美函数逼近，最优策略也可能与次优策略统计不可区分。识别了三种结构失效模式：前缀不可区分性导致可识别性失败、截断回报引起的目标错误指定、离线数据集支持和表示混叠。

Conclusion: 视野缩减可能引入不可恢复的信息损失，本文建立了视野缩减安全应用的必要条件，并强调了无法仅通过算法改进克服的内在限制，补充了关于保守目标和分布偏移的算法工作。

Abstract: Horizon reduction is a common design strategy in offline reinforcement learning (RL), used to mitigate long-horizon credit assignment, improve stability, and enable scalable learning through truncated rollouts, windowed training, or hierarchical decomposition (Levine et al., 2020; Prudencio et al., 2023; Park et al., 2025). Despite recent empirical evidence that horizon reduction can improve scaling on challenging offline RL benchmarks, its theoretical implications remain underdeveloped (Park et al., 2025). In this paper, we show that horizon reduction can induce fundamental and irrecoverable information loss in offline RL. We formalize horizon reduction as learning from fixed-length trajectory segments and prove that, under this paradigm and any learning interface restricted to fixed-length trajectory segments, optimal policies may be statistically indistinguishable from suboptimal ones even with infinite data and perfect function approximation. Through a set of minimal counterexample Markov decision processes (MDPs), we identify three distinct structural failure modes: (i) prefix indistinguishability leading to identifiability failure, (ii) objective misspecification induced by truncated returns, and (iii) offline dataset support and representation aliasing. Our results establish necessary conditions under which horizon reduction can be safe and highlight intrinsic limitations that cannot be overcome by algorithmic improvements alone, complementing algorithmic work on conservative objectives and distribution shift that addresses a different axis of offline RL difficulty (Fujimoto et al., 2019; Kumar et al., 2020; Gulcehre et al., 2020).

</details>


### [2] [Intrinsic-Metric Physics-Informed Neural Networks (IM-PINN) for Reaction-Diffusion Dynamics on Complex Riemannian Manifolds](https://arxiv.org/abs/2601.00834)
*Julian Evan Chrisnanto,Salsabila Rahma Alia,Nurfauzi Fadillah,Yulison Herry Chrisnanto*

Main category: cs.LG

TL;DR: IM-PINN：一种无网格几何深度学习框架，通过将黎曼度量张量嵌入自动微分图，在连续参数域中直接求解复杂流形上的非线性反应-扩散方程，解决了传统方法的高保真网格生成成本和辛漂移问题。


<details>
  <summary>Details</summary>
Motivation: 在复杂非欧几里得流形上模拟非线性反应-扩散动力学面临两大挑战：高保真网格生成的高昂计算成本，以及离散时间步进方案中的辛漂移问题。传统自适应细化方法无法解析极端高斯曲率波动下的各向异性图灵不稳定性。

Method: 提出IM-PINN框架：1）将黎曼度量张量嵌入自动微分图，解析重建拉普拉斯-贝尔特拉米算子；2）采用双流架构和傅里叶特征嵌入缓解谱偏差；3）在具有极端高斯曲率波动的"随机布料"流形上验证，恢复Gray-Scott模型的"分裂斑点"和"迷宫"模式。

Result: 与表面有限元法相比，IM-PINN展现出更优的物理严谨性：全局质量守恒误差为0.157（SFEM为0.258），作为热力学一致的全局求解器消除了半隐式积分固有的质量漂移。框架为演化表面上生物模式形成提供了内存高效、分辨率无关的模拟范式。

Conclusion: IM-PINN框架通过将微分几何与物理信息机器学习相结合，为复杂流形上的反应-扩散动力学模拟提供了无网格、内存高效且物理严谨的解决方案，克服了传统数值方法在几何离散化和时间积分方面的局限性。

Abstract: Simulating nonlinear reaction-diffusion dynamics on complex, non-Euclidean manifolds remains a fundamental challenge in computational morphogenesis, constrained by high-fidelity mesh generation costs and symplectic drift in discrete time-stepping schemes. This study introduces the Intrinsic-Metric Physics-Informed Neural Network (IM-PINN), a mesh-free geometric deep learning framework that solves partial differential equations directly in the continuous parametric domain. By embedding the Riemannian metric tensor into the automatic differentiation graph, our architecture analytically reconstructs the Laplace-Beltrami operator, decoupling solution complexity from geometric discretization. We validate the framework on a "Stochastic Cloth" manifold with extreme Gaussian curvature fluctuations ($K \in [-2489, 3580]$), where traditional adaptive refinement fails to resolve anisotropic Turing instabilities. Using a dual-stream architecture with Fourier feature embeddings to mitigate spectral bias, the IM-PINN recovers the "splitting spot" and "labyrinthine" regimes of the Gray-Scott model. Benchmarking against the Surface Finite Element Method (SFEM) reveals superior physical rigor: the IM-PINN achieves global mass conservation error of $\mathcal{E}_{mass} \approx 0.157$ versus SFEM's $0.258$, acting as a thermodynamically consistent global solver that eliminates mass drift inherent in semi-implicit integration. The framework offers a memory-efficient, resolution-independent paradigm for simulating biological pattern formation on evolving surfaces, bridging differential geometry and physics-informed machine learning.

</details>


### [3] [SLO-Conditioned Action Routing for Retrieval-Augmented Generation: Objective Ablation and Failure Modes](https://arxiv.org/abs/2601.00841)
*Bharath Nunepalli*

Main category: cs.LG

TL;DR: RAG控制问题研究：通过离线数据集学习查询级策略，选择检索深度和生成模式以满足服务级别目标，发现固定基线表现良好，学习策略主要在质量导向SLO下节省成本


<details>
  <summary>Details</summary>
Motivation: 检索增强生成(RAG)面临实际控制问题：需要为每个查询选择合适的检索深度和生成行为以满足服务级别目标(SLOs)，如成本、拒绝率和幻觉风险

Method: 将查询级控制建模为离散动作选择（检索深度、生成模式或拒绝），使用SQuAD 2.0构建离线数据集，记录准确性、代币成本、幻觉/拒绝指标和SLO加权奖励，评估两种策略学习目标：监督分类和奖励加权变体

Result: 固定基线（低k值、防护提示）表现竞争力强；学习策略主要在质量导向SLO下提供额外成本节省；在廉价SLO下，当拒绝被高度奖励时可能出现拒绝崩溃

Conclusion: 本研究提供了RAG管道SLO感知控制的可重复案例研究，强调失败模式和报告规范，而非提出新的检索器或语言模型

Abstract: Retrieval-augmented generation (RAG) introduces a practical control problem: retrieval depth and generation behavior must be chosen per query to satisfy service-level objectives (SLOs) such as cost, refusal rate, and hallucination risk. This work models per-query control as a small discrete action: choose a retrieval depth and a generation mode (guarded vs. auto), or refuse. An offline logged dataset is constructed from SQuAD 2.0 by executing each action and recording accuracy, token cost, hallucination/refusal indicators, and an SLO-weighted reward. Two simple policy-learning objectives are evaluated: supervised classification of the per-state best action (Argmax-CE) and a reward-weighted variant (Argmax-CE-WT). Across the evaluated settings, a strong fixed baseline (low k, guarded prompting) performs competitively; learned policies mainly provide additional cost savings under a quality-focused SLO and can exhibit refusal collapse under a cheap SLO when refusal is heavily rewarded. The contribution is a reproducible case study of SLO-aware control for RAG pipelines, emphasizing failure modes and reporting conventions rather than proposing a new retriever or language model.

</details>


### [4] [Warp-Cortex: An Asynchronous, Memory-Efficient Architecture for Million-Agent Cognitive Scaling on Consumer Hardware](https://arxiv.org/abs/2601.01298)
*Jorge L. Ruiz Williams*

Main category: cs.LG

TL;DR: Warp Cortex是一个异步多智能体LLM框架，通过解耦智能体逻辑与物理内存，实现了从O(N*L)到O(1)权重的内存复杂度降低，在单张RTX 4090上支持100个并发智能体，理论容量超过1000个智能体。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体大型语言模型框架存在线性内存扩展问题，使得"系统2"并行推理在消费级硬件上不切实际，需要解决内存瓶颈以实现大规模智能体认知扩展。

Method: 采用异步架构，通过单例权重共享和拓扑突触技术（受拓扑数据分析启发），将KV缓存视为潜在空间中的点云，应用见证复形稀疏化来保持上下文流形的持久同调特征，并引入参考注入机制实现非侵入式KV缓存更新。

Result: 在单张NVIDIA RTX 4090上，实现了100个并发智能体仅占用2.2GB总VRAM，理论容量超过1000个智能体，内存复杂度从O(N*L)降低到O(1)权重和O(N*k)上下文（其中k<<L）。

Conclusion: Warp Cortex通过创新的内存优化架构，解决了多智能体LLM框架的内存扩展瓶颈，为实现大规模并行推理系统提供了可行的技术路径，为消费级硬件上的百万级智能体认知扩展奠定了理论基础。

Abstract: Current multi-agent Large Language Model (LLM) frameworks suffer from linear memory scaling, rendering "System 2" parallel reasoning impractical on consumer hardware. We present Warp Cortex, an asynchronous architecture that theoretically enables million-agent cognitive scaling by decoupling agent logic from physical memory. Through Singleton Weight Sharing and a novel Topological Synapse--inspired by hybrid landmarking techniques from Topological Data Analysis (TDA)--we reduce memory complexity from O(N * L) to O(1) for weights and O(N * k) for context, where k << L. By treating the KV-cache as a point cloud in latent space, we apply witness-complex-inspired sparsification to preserve persistent homological features of the context manifold. On a single NVIDIA RTX 4090, we empirically demonstrate 100 concurrent agents at 2.2 GB total VRAM, with theoretical capacity exceeding 1,000 agents before compute latency becomes the bottleneck. We further introduce Referential Injection, a non-intrusive KV-cache update mechanism that allows asynchronous sub-agents to influence primary generation without stream disruption.

</details>


### [5] [Value-guided action planning with JEPA world models](https://arxiv.org/abs/2601.00844)
*Matthieu Destrade,Oumayma Bounou,Quentin Le Lidec,Jean Ponce,Yann LeCun*

Main category: cs.LG

TL;DR: 提出一种增强JEPA世界模型规划能力的方法，通过塑造表示空间使状态嵌入间的距离近似负目标条件值函数，从而显著提升简单控制任务中的规划性能


<details>
  <summary>Details</summary>
Motivation: JEPA框架能通过自监督预测目标学习环境动态表示，但其支持有效行动规划的能力有限，需要增强其规划功能

Method: 通过塑造JEPA表示空间，使状态嵌入间的距离（或准距离）能近似给定环境中到达成本的负目标条件值函数，并在训练中强制实施这一约束

Result: 相比标准JEPA模型，该方法在简单控制任务上显著提升了规划性能

Conclusion: 通过将表示空间与目标条件值函数对齐，可以增强JEPA世界模型的规划能力，为构建能推理环境动态的深度学习模型提供有效途径

Abstract: Building deep learning models that can reason about their environment requires capturing its underlying dynamics. Joint-Embedded Predictive Architectures (JEPA) provide a promising framework to model such dynamics by learning representations and predictors through a self-supervised prediction objective. However, their ability to support effective action planning remains limited. We propose an approach to enhance planning with JEPA world models by shaping their representation space so that the negative goal-conditioned value function for a reaching cost in a given environment is approximated by a distance (or quasi-distance) between state embeddings. We introduce a practical method to enforce this constraint during training and show that it leads to significantly improved planning performance compared to standard JEPA models on simple control tasks.

</details>


### [6] [You Only Need Your Transformer 25% of the Time: Meaning-First Execution for Eliminating Unnecessary Inference](https://arxiv.org/abs/2601.00847)
*Ryan Shamim*

Main category: cs.LG

TL;DR: MFEE框架将AI推理重构为控制平面决策问题，通过语义分析选择性执行transformer，在保持100%准确率的同时减少78.1%的计算量。


<details>
  <summary>Details</summary>
Motivation: 现有AI推理系统将transformer执行视为强制性操作，混淆了模型能力与执行必要性。作者认为许多情况下可以通过替代路径保持正确性，无需完整执行transformer。

Method: 提出Meaning-First Execution (MFEE)控制平面架构，作为现有堆栈之上的门控层，不修改模型权重或参数。通过语义分析判断何时需要执行transformer，何时可以通过替代路径保持正确性。

Result: 在1000个多样化提示的确定性解码测试中，MFEE实现78.1%的执行减少，同时保持100%的精确匹配等价性。相比基于模式的路由器最多只能达到53.3%的避免率且存在正确性失败，MFEE通过语义分析实现100%避免率且零失败。

Conclusion: 证明了执行治理应作为ML系统基础设施的基础层，与模型级优化技术正交。通过定理1证明任何仅基于有限特征图的路由器无法同时保证零假跳和正避免率。

Abstract: Modern AI inference systems treat transformer execution as mandatory, conflating model capability with execution necessity. We reframe inference as a control-plane decision problem: determining when execution is necessary versus when correctness can be preserved through alternative pathways. We introduce Meaning-First Execution (MFEE), a control-plane architecture implementing this framework, selectively invoking transformer inference only when required. MFEE operates as a gating layer above existing stacks without modifying models, weights, or parameters. Across 1,000 diverse prompts under deterministic decoding, MFEE achieves 78.1% execution reduction while maintaining 100% exact-match equivalence for invoked executions. Comparative evaluation reveals pattern-based routers achieve at most 53.3% avoidance with correctness failures, while MFEE reaches 100% avoidance with zero failures through semantic analysis. We prove this limitation via Theorem 1: any router operating solely on finite feature maps cannot simultaneously guarantee zero false skips and positive avoidance on feature-collision pairs. These results establish execution governance as a foundational layer in ML systems infrastructure, orthogonal to model-level optimization techniques.

</details>


### [7] [EdgeJury: Cross-Reviewed Small-Model Ensembles for Truthful Question Answering on Serverless Edge Inference](https://arxiv.org/abs/2601.00850)
*Aayush Kumar*

Main category: cs.LG

TL;DR: EdgeJury是一个轻量级集成框架，使用小型指令调优语言模型（3B-8B）在边缘计算环境中提高问答的真实性和鲁棒性，通过四阶段协调流程显著减少幻觉错误。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的边缘部署场景中，前沿规模模型或检索管道可能不切实际，而幻觉问题会严重影响问答系统的可靠性。需要一种轻量级解决方案来提升小型语言模型的真实性和鲁棒性。

Method: EdgeJury采用四阶段协调框架：1）并行角色专业化生成；2）匿名交叉评审，包含结构化批评和排名；3）主席合成，整合最强内容并解决标记问题；4）基于模型间一致性的声明级一致性标记。

Result: 在TruthfulQA（MC1）上达到76.2%准确率，相比单个8B基线（62.8%）相对提升21.4%；在200个问题的对抗性EdgeCases集上获得48.2%相对增益；手动分析显示事实性幻觉错误减少约55%；在Cloudflare Workers AI上实现8.4秒中位端到端延迟。

Conclusion: 协调的小型模型集成可以在不依赖外部检索或专有大型模型API的情况下，显著改善在误解密集的QA基准上的真实性，适合边缘部署场景。

Abstract: Hallucinations hinder reliable question answering, especially in resource-constrained deployments where frontier-scale models or retrieval pipelines may be impractical. We present EdgeJury, a lightweight ensemble framework that improves truthfulness and robustness using only small instruction-tuned language models (3B-8B) suitable for serverless edge inference. EdgeJury orchestrates four stages: (1) parallel role-specialized generation, (2) anonymized cross-review with structured critiques and rankings, (3) chairman synthesis that integrates the strongest content while addressing flagged issues, and (4) claim-level consistency labeling based on inter-model agreement. On TruthfulQA (MC1), EdgeJury achieves 76.2% accuracy (95% CI: 72.8-79.6%), a +21.4% relative improvement over a single 8B baseline (62.8%), and outperforms standard baselines including self-consistency and majority voting under transparent compute accounting (total tokens and platform cost reported). On a 200-question adversarial EdgeCases set, EdgeJury yields +48.2% relative gains (95% CI: 44.0-52.4%). Manual analysis on 100 incorrect answers shows an approximately 55% reduction in factual hallucination errors versus the single-model baseline. Deployed on Cloudflare Workers AI, EdgeJury achieves 8.4 s median end-to-end latency, demonstrating that coordinated small-model ensembles can improve truthfulness on misconception-heavy QA benchmarks without external retrieval or proprietary large-model APIs.

</details>


### [8] [FedSCAM (Federated Sharpness-Aware Minimization with Clustered Aggregation and Modulation): Scam-resistant SAM for Robust Federated Optimization in Heterogeneous Environments](https://arxiv.org/abs/2601.00853)
*Sameer Rahil,Zain Abdullah Ahmad,Talha Asif*

Main category: cs.LG

TL;DR: FedSCAM算法通过基于客户端异质性动态调整SAM扰动半径和聚合权重，解决联邦学习中非IID数据分布带来的挑战，在收敛速度和最终精度上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在边缘设备上实现协作训练时面临统计异质性挑战，特别是非IID标签分布会影响收敛和泛化。现有SAM方法对所有客户端使用统一扰动半径，忽略了客户端特定的异质性。

Method: 提出FedSCAM算法：1）计算每个客户端的异质性指标；2）根据异质性分数反向调制SAM扰动半径，防止高方差客户端破坏全局模型；3）引入异质性感知加权聚合机制，优先考虑与全局优化方向一致的客户端更新。

Result: 在CIFAR-10和Fashion-MNIST数据集上，使用不同程度的狄利克雷标签偏斜进行实验，FedSCAM在收敛速度和最终测试精度方面优于包括FedSAM、FedLESAM等在内的最先进基线方法。

Conclusion: FedSCAM通过动态调整扰动半径和聚合权重来适应客户端异质性，有效解决了联邦学习中的统计异质性问题，在非IID数据分布下表现出优越性能。

Abstract: Federated Learning (FL) enables collaborative model training across decentralized edge devices while preserving data privacy. However, statistical heterogeneity among clients, often manifested as non-IID label distributions, poses significant challenges to convergence and generalization. While Sharpness-Aware Minimization (SAM) has been introduced to FL to seek flatter, more robust minima, existing approaches typically apply a uniform perturbation radius across all clients, ignoring client-specific heterogeneity. In this work, we propose \textbf{FedSCAM} (Federated Sharpness-Aware Minimization with Clustered Aggregation and Modulation), a novel algorithm that dynamically adjusts the SAM perturbation radius and aggregation weights based on client-specific heterogeneity scores. By calculating a heterogeneity metric for each client and modulating the perturbation radius inversely to this score, FedSCAM prevents clients with high variance from destabilizing the global model. Furthermore, we introduce a heterogeneity-aware weighted aggregation mechanism that prioritizes updates from clients that align with the global optimization direction. Extensive experiments on CIFAR-10 and Fashion-MNIST under various degrees of Dirichlet-based label skew demonstrate that FedSCAM achieves competitive performance among state-of-the-art baselines, including FedSAM, FedLESAM, etc. in terms of convergence speed and final test accuracy.

</details>


### [9] [Harvesting AlphaEarth: Benchmarking the Geospatial Foundation Model for Agricultural Downstream Tasks](https://arxiv.org/abs/2601.00857)
*Yuchi Ma,Yawen Shen,Anu Swatantran,David B. Lobell*

Main category: cs.LG

TL;DR: 本研究评估了AlphaEarth Foundation (AEF)地理空间基础模型在农业监测任务中的表现，包括作物产量预测、耕作制图、覆盖作物制图，并与传统遥感模型进行比较。


<details>
  <summary>Details</summary>
Motivation: 虽然AEF等地理空间基础模型在土地覆盖分类任务中表现出色，但在农业监测应用方面缺乏深入评估，且缺乏与传统遥感模型的全面比较。

Method: 使用AEF嵌入在美国进行三个农业下游任务评估：作物产量预测、耕作制图、覆盖作物制图；编译公共和私人数据集在不同尺度和位置进行评估；训练传统遥感模型作为对比。

Result: AEF模型在所有任务中表现良好，在产量预测和县级耕作制图方面与专门构建的遥感模型竞争力相当；但也发现AEF嵌入存在空间可转移性有限、可解释性低、时间敏感性有限等局限性。

Conclusion: AEF嵌入在农业应用中需谨慎使用，特别是在时间敏感性、泛化能力和可解释性要求较高的场景中，虽然表现良好但存在明显局限性。

Abstract: Geospatial foundation models (GFMs) have emerged as a promising approach to overcoming the limitations in existing featurization methods. More recently, Google DeepMind has introduced AlphaEarth Foundation (AEF), a GFM pre-trained using multi-source EOs across continuous time. An annual and global embedding dataset is produced using AEF that is ready for analysis and modeling. The internal experiments show that AEF embeddings have outperformed operational models in 15 EO tasks without re-training. However, those experiments are mostly about land cover and land use classification. Applying AEF and other GFMs to agricultural monitoring require an in-depth evaluation in critical agricultural downstream tasks. There is also a lack of comprehensive comparison between the AEF-based models and traditional remote sensing (RS)-based models under different scenarios, which could offer valuable guidance for researchers and practitioners. This study addresses some of these gaps by evaluating AEF embeddings in three agricultural downstream tasks in the U.S., including crop yield prediction, tillage mapping, and cover crop mapping. Datasets are compiled from both public and private sources to comprehensively evaluate AEF embeddings across tasks at different scales and locations, and RS-based models are trained as comparison models. AEF-based models generally exhibit strong performance on all tasks and are competitive with purpose-built RS-based models in yield prediction and county-level tillage mapping when trained on local data. However, we also find several limitations in current AEF embeddings, such as limited spatial transferability compared to RS-based models, low interpretability, and limited time sensitivity. These limitations recommend caution when applying AEF embeddings in agriculture, where time sensitivity, generalizability, and interpretability is important.

</details>


### [10] [Path Integral Solution for Dissipative Generative Dynamics](https://arxiv.org/abs/2601.00860)
*Xidi Wang*

Main category: cs.LG

TL;DR: 该论文证明纯机械系统通过耗散量子动力学和非局域上下文聚合可以生成智能语言，而守恒定律会导致根本性失败。语言生成本质上是耗散量子场论。


<details>
  <summary>Details</summary>
Motivation: 探索纯机械系统是否能生成智能语言，研究量子动力学中的耗散与非局域性在语言生成中的作用，挑战传统基于守恒定律的计算模型。

Method: 使用Koopman算子与闭式路径积分传播子分析耗散量子动力学，通过谱分析揭示特征值结构（衰减模式、增长模式、中性模式），研究不可逆计算的基本要求。

Result: 耗散量子动力学能产生连贯文本生成，而哈密顿约束会消除耗散模式并降低性能。谱分析显示特征值结构分离为遗忘、放大和保持模式，这些是定向信息流的基本要素。

Conclusion: 语言生成本质上是耗散量子场论，机械系统通过耗散和非局域性的结合获得智能，而非通过守恒定律。不可逆计算需要受控信息耗散和因果上下文聚合。

Abstract: Can purely mechanical systems generate intelligent language? We prove that dissipative quantum dynamics with analytically tractable non-local context aggregation produce coherent text generation, while conservation laws cause fundamental failure. Employing Koopman operators with closed-form path integral propagators, we show irreversible computation fundamentally requires both controlled information dissipation and causal context aggregation. Spectral analysis reveals emergent eigenvalue structure, separating into decay modes (forgetting), growth modes (amplification), and neutral modes (preservation) -- the essential ingredients for directed information flow. Hamiltonian constraints force the elimination of these dissipative modes and degrading performance despite unchanged model capacity. This establishes language generation as dissipative quantum field theory, proving mechanical systems acquire intelligence through the combination of dissipation and non-locality, not through conservation.

</details>


### [11] [Hierarchical topological clustering](https://arxiv.org/abs/2601.00892)
*Ana Carpio,Gema Duro*

Main category: cs.LG

TL;DR: 提出一种分层拓扑聚类算法，可使用任意距离度量，通过层次结构推断异常值和任意形状簇的持续性，在图像、医疗和经济数据中验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 拓扑方法能够在不对数据结构做假设的情况下探索数据云。现有聚类技术在处理异常值和任意形状簇时存在局限，需要一种更灵活的方法。

Method: 提出分层拓扑聚类算法，可使用任意距离度量，通过构建层次结构来推断异常值和任意形状簇的持续性。

Result: 在包含图像、医疗和经济数据的选定数据集上验证了算法潜力，这些数据中异常值起重要作用。算法在其他技术失败的情况下仍能提供有意义的聚类。

Conclusion: 分层拓扑聚类算法具有处理任意形状簇和异常值的能力，在多种实际应用场景中展现出优势，为数据探索提供了新的拓扑方法。

Abstract: Topological methods have the potential of exploring data clouds without making assumptions on their the structure. Here we propose a hierarchical topological clustering algorithm that can be implemented with any distance choice. The persistence of outliers and clusters of arbitrary shape is inferred from the resulting hierarchy. We demonstrate the potential of the algorithm on selected datasets in which outliers play relevant roles, consisting of images, medical and economic data. These methods can provide meaningful clusters in situations in which other techniques fail to do so.

</details>


### [12] [Conformal Prediction Under Distribution Shift: A COVID-19 Natural Experiment](https://arxiv.org/abs/2601.00908)
*Chorok Lee*

Main category: cs.LG

TL;DR: 研究分布偏移下保形预测性能下降问题，使用COVID-19供应链数据作为自然实验，发现特征重要性集中度是灾难性失败的关键指标，并提出基于SHAP集中度的决策框架


<details>
  <summary>Details</summary>
Motivation: 保形预测在分布偏移下的性能保证会下降，但现有研究缺乏对具体失败模式的深入理解。COVID-19疫情为研究极端分布偏移提供了自然实验场景，特别是在供应链任务中观察到的特征剧烈变化

Method: 使用8个供应链任务作为自然实验，通过SHAP分析特征重要性分布，计算特征集中度指标，并评估季度再训练的效果。还分析了4个具有中等特征稳定性的额外任务进行对比

Result: 覆盖率下降从0%到86.7%不等，灾难性失败与单特征依赖性高度相关（rho=0.714）。灾难性任务的特征重要性集中在单一特征上（增加4.5倍），而稳健任务则分布在多个特征上（10-20倍）。季度再训练可将灾难性任务的覆盖率从22%提升到41%，但对稳健任务无显著帮助

Conclusion: 特征重要性集中度是预测分布偏移下性能下降的关键指标。提出决策框架：部署前监控SHAP集中度；如果集中度>40%则进行季度再训练；如果任务稳健则可跳过再训练。特征稳定性而非集中度决定中等偏移下的稳健性

Abstract: Conformal prediction guarantees degrade under distribution shift. We study this using COVID-19 as a natural experiment across 8 supply chain tasks. Despite identical severe feature turnover (Jaccard approximately 0), coverage drops vary from 0% to 86.7%, spanning two orders of magnitude. Using SHapley Additive exPlanations (SHAP) analysis, we find catastrophic failures correlate with single-feature dependence (rho = 0.714, p = 0.047). Catastrophic tasks concentrate importance in one feature (4.5x increase), while robust tasks redistribute across many (10-20x). Quarterly retraining restores catastrophic task coverage from 22% to 41% (+19 pp, p = 0.04), but provides no benefit for robust tasks (99.8% coverage). Exploratory analysis of 4 additional tasks with moderate feature stability (Jaccard 0.13-0.86) reveals feature stability, not concentration, determines robustness, suggesting concentration effects apply specifically to severe shifts. We provide a decision framework: monitor SHAP concentration before deployment; retrain quarterly if vulnerable (>40% concentration); skip retraining if robust.

</details>


### [13] [Revisiting Weighted Strategy for Non-stationary Parametric Bandits and MDPs](https://arxiv.org/abs/2601.01069)
*Jing Wang,Peng Zhao,Zhi-Hua Zhou*

Main category: cs.LG

TL;DR: 本文重新审视非平稳参数化赌博机中的加权策略，提出了一个精炼的分析框架，简化了算法设计并改进了后悔界，同时将该框架扩展到具有函数逼近的非平稳马尔可夫决策过程。


<details>
  <summary>Details</summary>
Motivation: 非平稳参数化赌博机中，加权策略在处理渐进漂移模式时常用，但先前理论分析复杂且算法效率低或统计次优。本文旨在通过改进分析框架来解决这些问题。

Method: 提出精炼分析框架，简化加权策略的推导，设计更简单的加权算法。将该框架应用于线性赌博机、广义线性赌博机、自协调赌博机，并扩展到具有函数逼近的非平稳马尔可夫决策过程（线性混合MDP和多项Logit混合MDP）。

Result: 在线性赌博机中，新框架产生与窗口/重启算法同样高效但更简单的加权算法，后悔界与先前研究相同。在广义线性赌博机中，获得改进的后悔界 $\tilde{O}(k_μ^{5/4} c_μ^{-3/4} d^{3/4} P_T^{1/4}T^{3/4})$，优于先前的 $\tilde{O}(k_μ^{2} c_μ^{-1}d^{9/10} P_T^{1/5}T^{4/5})$。框架成功扩展到非平稳MDPs。

Conclusion: 精炼分析框架解决了加权策略在非平稳参数化赌博机中的复杂性问题，简化了算法设计并改进了后悔界，同时展示了框架在多种赌博机模型和非平稳MDPs中的广泛适用性。

Abstract: Non-stationary parametric bandits have attracted much attention recently. There are three principled ways to deal with non-stationarity, including sliding-window, weighted, and restart strategies. As many non-stationary environments exhibit gradual drifting patterns, the weighted strategy is commonly adopted in real-world applications. However, previous theoretical studies show that its analysis is more involved and the algorithms are either computationally less efficient or statistically suboptimal. This paper revisits the weighted strategy for non-stationary parametric bandits. In linear bandits (LB), we discover that this undesirable feature is due to an inadequate regret analysis, which results in an overly complex algorithm design. We propose a \emph{refined analysis framework}, which simplifies the derivation and, importantly, produces a simpler weight-based algorithm that is as efficient as window/restart-based algorithms while retaining the same regret as previous studies. Furthermore, our new framework can be used to improve regret bounds of other parametric bandits, including Generalized Linear Bandits (GLB) and Self-Concordant Bandits (SCB). For example, we develop a simple weighted GLB algorithm with an $\tilde{O}(k_μ^{5/4} c_μ^{-3/4} d^{3/4} P_T^{1/4}T^{3/4})$ regret, improving the $\tilde{O}(k_μ^{2} c_μ^{-1}d^{9/10} P_T^{1/5}T^{4/5})$ bound in prior work, where $k_μ$ and $c_μ$ characterize the reward model's nonlinearity, $P_T$ measures the non-stationarity, $d$ and $T$ denote the dimension and time horizon. Moreover, we extend our framework to non-stationary Markov Decision Processes (MDPs) with function approximation, focusing on Linear Mixture MDP and Multinomial Logit (MNL) Mixture MDP. For both classes, we propose algorithms based on the weighted strategy and establish dynamic regret guarantees using our analysis framework.

</details>


### [14] [Wittgenstein's Family Resemblance Clustering Algorithm](https://arxiv.org/abs/2601.01127)
*Golbahar Amanpour,Benyamin Ghojogh*

Main category: cs.LG

TL;DR: 该论文将维特根斯坦的家族相似性哲学概念转化为机器学习聚类算法，提出了WFR算法及其核变体，无需预设聚类数量或形状假设


<details>
  <summary>Details</summary>
Motivation: 从分析哲学中维特根斯坦的家族相似性概念获得启发，该概念认为类别成员通过重叠的相似性而非单一定义属性连接，这种思想天然适合图基础的机器学习方法

Method: 提出WFR聚类算法及其核变体kernel WFR：计算相邻数据实例之间的相似度分数，阈值处理后构建相似图，该图的连通分量即为最终聚类结果

Result: 在基准数据集上的仿真实验表明，WFR是一种有效的非线性聚类算法，不需要预先知道聚类数量或对其形状做出假设

Conclusion: 成功将哲学概念转化为实用的机器学习算法，证明了跨学科方法在开发新型聚类技术方面的价值

Abstract: This paper, introducing a novel method in philomatics, draws on Wittgenstein's concept of family resemblance from analytic philosophy to develop a clustering algorithm for machine learning. According to Wittgenstein's Philosophical Investigations (1953), family resemblance holds that members of a concept or category are connected by overlapping similarities rather than a single defining property. Consequently, a family of entities forms a chain of items sharing overlapping traits. This philosophical idea naturally lends itself to a graph-based approach in machine learning. Accordingly, we propose the Wittgenstein's Family Resemblance (WFR) clustering algorithm and its kernel variant, kernel WFR. This algorithm computes resemblance scores between neighboring data instances, and after thresholding these scores, a resemblance graph is constructed. The connected components of this graph define the resulting clusters. Simulations on benchmark datasets demonstrate that WFR is an effective nonlinear clustering algorithm that does not require prior knowledge of the number of clusters or assumptions about their shapes.

</details>


### [15] [Adaptive Conformal Prediction via Bayesian Uncertainty Weighting for Hierarchical Healthcare Data](https://arxiv.org/abs/2601.01223)
*Marzieh Amiri Shahbazi,Ali Baheri,Nasibeh Azadeh-Fard*

Main category: cs.LG

TL;DR: 提出混合贝叶斯-保形框架，结合贝叶斯层次随机森林和群体感知保形校准，为医疗预测提供分布自由的覆盖保证和风险自适应精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法同时满足临床决策对不确定性量化的两个关键要求：分布自由的覆盖保证和风险自适应的精度。医疗预测需要既能提供严格覆盖保证，又能根据风险水平调整预测精度的不确定性量化方法。

Method: 提出混合贝叶斯-保形框架，整合贝叶斯层次随机森林与群体感知保形校准。使用后验不确定性对保形分数进行加权，同时保持严格的覆盖有效性。该方法在61,538个入院病例、3,793家美国医院和4个地区进行评估。

Result: 方法达到目标覆盖（94.3% vs 95%目标），并实现自适应精度：低不确定性病例的区间宽度减少21%，同时适当扩大高风险预测的区间。研究发现，单独使用校准良好的贝叶斯不确定性严重覆盖不足（14.1%），凸显了混合方法的必要性。

Conclusion: 该框架支持风险分层临床协议、高效资源规划和高置信度预测，以及对不确定病例的保守分配和增强监督，为多样化医疗环境提供不确定性感知的决策支持。

Abstract: Clinical decision-making demands uncertainty quantification that provides both distribution-free coverage guarantees and risk-adaptive precision, requirements that existing methods fail to jointly satisfy. We present a hybrid Bayesian-conformal framework that addresses this fundamental limitation in healthcare predictions. Our approach integrates Bayesian hierarchical random forests with group-aware conformal calibration, using posterior uncertainties to weight conformity scores while maintaining rigorous coverage validity. Evaluated on 61,538 admissions across 3,793 U.S. hospitals and 4 regions, our method achieves target coverage (94.3% vs 95% target) with adaptive precision: 21% narrower intervals for low-uncertainty cases while appropriately widening for high-risk predictions. Critically, we demonstrate that well-calibrated Bayesian uncertainties alone severely under-cover (14.1%), highlighting the necessity of our hybrid approach. This framework enables risk-stratified clinical protocols, efficient resource planning for high-confidence predictions, and conservative allocation with enhanced oversight for uncertain cases, providing uncertainty-aware decision support across diverse healthcare settings.

</details>


### [16] [Learning with Monotone Adversarial Corruptions](https://arxiv.org/abs/2601.02193)
*Kasper Green Larsen,Chirag Pabbaraju,Abhishek Shetty*

Main category: cs.LG

TL;DR: 研究机器学习算法对数据可交换性和独立性的依赖程度，通过引入单调对抗性腐败模型，发现已知最优分类算法在看似有益的单调腐败下表现变差，而基于一致收敛的算法不受影响。


<details>
  <summary>Details</summary>
Motivation: 探究标准机器学习算法在多大程度上依赖于数据的可交换性和独立性，通过引入一种新的对抗性腐败模型来测试算法的鲁棒性。

Method: 引入单调对抗性腐败模型：攻击者观察"干净"的i.i.d.数据集后，插入自己选择的"腐败"数据点，这些腐败点被约束为单调腐败（即按照真实目标函数标记）。

Result: 所有已知的二元分类最优学习算法在单调腐败下都会在新测试点上获得次优的期望误差；而基于一致收敛的算法则不受影响，其保证不会退化。

Conclusion: 最优学习算法在面对看似有益的单调腐败时会崩溃，暴露了它们对数据可交换性的过度依赖；而基于一致收敛的算法更加鲁棒。

Abstract: We study the extent to which standard machine learning algorithms rely on exchangeability and independence of data by introducing a monotone adversarial corruption model. In this model, an adversary, upon looking at a "clean" i.i.d. dataset, inserts additional "corrupted" points of their choice into the dataset. These added points are constrained to be monotone corruptions, in that they get labeled according to the ground-truth target function. Perhaps surprisingly, we demonstrate that in this setting, all known optimal learning algorithms for binary classification can be made to achieve suboptimal expected error on a new independent test point drawn from the same distribution as the clean dataset. On the other hand, we show that uniform convergence-based algorithms do not degrade in their guarantees. Our results showcase how optimal learning algorithms break down in the face of seemingly helpful monotone corruptions, exposing their overreliance on exchangeability.

</details>


### [17] [LearnAD: Learning Interpretable Rules for Brain Networks in Alzheimer's Disease Classification](https://arxiv.org/abs/2601.00877)
*Thomas Andrews,Mark Law,Sara Ahmadi-Abhari,Alessandra Russo*

Main category: cs.LG

TL;DR: LearnAD是一种神经符号方法，用于从脑部MRI数据预测阿尔茨海默病，学习完全可解释的规则。它结合统计模型、决策树、随机森林或GNN识别相关脑连接，然后使用FastLAS学习全局规则。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够预测阿尔茨海默病同时保持完全可解释性的方法，解决传统机器学习模型（如GNN）在临床神经科学中缺乏可解释性的问题。

Method: 神经符号方法：1）使用统计模型、决策树、随机森林或GNN识别相关的脑连接特征；2）应用FastLAS符号学习系统从这些特征中学习全局可解释规则。

Result: 最佳实例性能优于决策树，与支持向量机准确率相当，仅略低于使用所有特征的随机森林和GNN，同时保持完全可解释性。消融研究表明神经符号方法在保持可比性能的同时提高了可解释性。

Conclusion: LearnAD展示了符号学习如何加深对GNN在临床神经科学中行为的理解，为阿尔茨海默病预测提供了一种既准确又可解释的神经符号解决方案。

Abstract: We introduce LearnAD, a neuro-symbolic method for predicting Alzheimer's disease from brain magnetic resonance imaging data, learning fully interpretable rules. LearnAD applies statistical models, Decision Trees, Random Forests, or GNNs to identify relevant brain connections, and then employs FastLAS to learn global rules. Our best instance outperforms Decision Trees, matches Support Vector Machine accuracy, and performs only slightly below Random Forests and GNNs trained on all features, all while remaining fully interpretable. Ablation studies show that our neuro-symbolic approach improves interpretability with comparable performance to pure statistical models. LearnAD demonstrates how symbolic learning can deepen our understanding of GNN behaviour in clinical neuroscience.

</details>


### [18] [Outlier Detection Using Vector Cosine Similarity by Adding a Dimension](https://arxiv.org/abs/2601.00883)
*Zhongyang Shen*

Main category: cs.LG

TL;DR: 提出了一种基于向量余弦相似度的多维异常值检测方法，通过在原数据中添加零值维度构建新数据集，利用观测点和测量点之间的向量相似性比较来识别异常数据。


<details>
  <summary>Details</summary>
Motivation: 需要一种有效的多维异常值检测方法，能够处理复杂的高维数据分布，传统方法可能在高维空间中表现不佳，因此需要基于向量相似性的新方法。

Method: 在原数据中添加一个零值维度构建新数据集，选择测量点并创建观测点（原点），观测点与测量点仅在新维度上有非零值差异。从观测点向测量点和其他数据点构建向量，通过比较这些向量的余弦相似度来识别异常数据。

Result: 开发了优化的实现MDOD，并已在PyPI上发布（https://pypi.org/project/mdod/），为多维异常值检测提供了实用工具。

Conclusion: 该方法提供了一种基于向量余弦相似度的有效多维异常值检测方法，通过创新的数据转换和向量比较机制，能够识别复杂高维数据中的异常模式。

Abstract: We propose a new outlier detection method for multi-dimensional data. The method detects outliers based on vector cosine similarity, using a new dataset constructed by adding a dimension with zero values to the original data. When a point in the new dataset is selected as the measured point, an observation point is created as the origin, differing only in the new dimension by having a non-zero value compared to the measured point. Vectors are then formed from the observation point to the measured point and to other points in the dataset. By comparing the cosine similarities of these vectors, abnormal data can be identified. An optimized implementation (MDOD) is available on PyPI: https://pypi.org/project/mdod/.

</details>


### [19] [FANoS: Friction-Adaptive Nosé--Hoover Symplectic Momentum for Stiff Objectives](https://arxiv.org/abs/2601.00889)
*Nalin Dhiman*

Main category: cs.LG

TL;DR: FANoS是一种受物理启发的优化器，结合了二阶动力系统、Nosé-Hoover热浴变量和半隐式积分器，在Rosenbrock-100D基准测试中表现优于AdamW和SGD+momentum，但在其他问题上表现不稳定，不是现代基线的通用替代品。


<details>
  <summary>Details</summary>
Motivation: 研究受分子动力学中结构保持积分和热浴思想启发的优化器，将其纯粹作为优化启发式方法使用，旨在开发能够处理非凸优化问题的物理启发式优化算法。

Method: FANoS结合了：(1) 作为离散化二阶动力系统的动量更新，(2) 使用动能反馈调整标量摩擦系数的Nosé-Hoover类热浴变量，(3) 半隐式（辛欧拉）积分器，可选配对角RMS预处理器。

Result: 在Rosenbrock-100D基准测试中，FANoS-RMS达到平均最终目标值1.74×10⁻²，显著优于未剪裁的AdamW(48.50)和SGD+momentum(90.76)，但剪裁后的AdamW更强(1.87×10⁻³)，L-BFGS表现最佳(≈4.4×10⁻¹⁰)。在病态凸二次问题和PINN预热测试中，FANoS表现不佳且不稳定。

Conclusion: FANoS是对现有思想的可解释性综合，在某些刚性非凸谷中可能有帮助，但不是现代基线的通用替代品，其行为对温度调度和超参数选择敏感，需要谨慎使用。

Abstract: We study a physics-inspired optimizer, \emph{FANoS} (Friction-Adaptive Nosé--Hoover Symplectic momentum), which combines (i) a momentum update written as a discretized second-order dynamical system, (ii) a Nosé--Hoover-like thermostat variable that adapts a scalar friction coefficient using kinetic-energy feedback, and (iii) a semi-implicit (symplectic-Euler) integrator, optionally with a diagonal RMS preconditioner. The method is motivated by structure-preserving integration and thermostat ideas from molecular dynamics, but is used here purely as an optimization heuristic.
  We provide the algorithm and limited theoretical observations in idealized settings. On the deterministic Rosenbrock-100D benchmark with 3000 gradient evaluations, FANoS-RMS attains a mean final objective value of $1.74\times 10^{-2}$, improving substantially over unclipped AdamW ($48.50$) and SGD+momentum ($90.76$) in this protocol. However, AdamW with gradient clipping is stronger, reaching $1.87\times 10^{-3}$, and L-BFGS reaches $\approx 4.4\times 10^{-10}$. On ill-conditioned convex quadratics and in a small PINN warm-start suite (Burgers and Allen--Cahn), the default FANoS configuration underperforms AdamW and can be unstable or high-variance.
  Overall, the evidence supports a conservative conclusion: FANoS is an interpretable synthesis of existing ideas that can help on some stiff nonconvex valleys, but it is not a generally superior replacement for modern baselines, and its behavior is sensitive to temperature-schedule and hyperparameter choices.

</details>


### [20] [Dichotomous Diffusion Policy Optimization](https://arxiv.org/abs/2601.00898)
*Ruiming Liang,Yinan Zheng,Kexin Zheng,Tianyi Tan,Jianxiong Li,Liyuan Mao,Zhihao Wang,Guang Chen,Hangjun Ye,Jingjing Liu,Jinqiao Wang,Xianyuan Zhan*

Main category: cs.LG

TL;DR: DIPOLE是一种新颖的强化学习算法，通过将最优策略分解为一对稳定学习的二分策略（一个最大化奖励，一个最小化奖励），解决了扩散策略在强化学习中训练不稳定和计算效率低的问题。


<details>
  <summary>Details</summary>
Motivation: 扩散策略在决策任务中表现出色，但在强化学习训练中存在两个主要问题：直接最大化价值目标导致训练不稳定，以及基于高斯似然近似的方法需要大量小步去噪导致计算效率低下。

Method: 提出DIPOLE算法，重新审视KL正则化目标，设计贪婪化策略正则化方案，将最优策略分解为一对二分策略（奖励最大化和最小化策略），在推理时通过线性组合两个策略的分数生成优化动作。

Result: 在ExORL和OGBench的离线和离线到在线强化学习设置中验证了方法的有效性，并使用DIPOLE训练了一个大型视觉-语言-动作模型用于端到端自动驾驶，在NAVSIM大规模真实世界自动驾驶基准上表现出色。

Conclusion: DIPOLE为扩散策略的强化学习训练提供了稳定且可控的优化方法，在复杂真实世界应用中展现出巨大潜力。

Abstract: Diffusion-based policies have gained growing popularity in solving a wide range of decision-making tasks due to their superior expressiveness and controllable generation during inference. However, effectively training large diffusion policies using reinforcement learning (RL) remains challenging. Existing methods either suffer from unstable training due to directly maximizing value objectives, or face computational issues due to relying on crude Gaussian likelihood approximation, which requires a large amount of sufficiently small denoising steps. In this work, we propose DIPOLE (Dichotomous diffusion Policy improvement), a novel RL algorithm designed for stable and controllable diffusion policy optimization. We begin by revisiting the KL-regularized objective in RL, which offers a desirable weighted regression objective for diffusion policy extraction, but often struggles to balance greediness and stability. We then formulate a greedified policy regularization scheme, which naturally enables decomposing the optimal policy into a pair of stably learned dichotomous policies: one aims at reward maximization, and the other focuses on reward minimization. Under such a design, optimized actions can be generated by linearly combining the scores of dichotomous policies during inference, thereby enabling flexible control over the level of greediness.Evaluations in offline and offline-to-online RL settings on ExORL and OGBench demonstrate the effectiveness of our approach. We also use DIPOLE to train a large vision-language-action (VLA) model for end-to-end autonomous driving (AD) and evaluate it on the large-scale real-world AD benchmark NAVSIM, highlighting its potential for complex real-world applications.

</details>


### [21] [Latent-Constrained Conditional VAEs for Augmenting Large-Scale Climate Ensembles](https://arxiv.org/abs/2601.00915)
*Jacquelyn Shelton,Przemyslaw Polewski,Alexander Robel,Matthew Hoffman,Stephen Price*

Main category: cs.LG

TL;DR: 提出LC-CVAE方法，通过强制潜在空间在共享锚点位置的一致性，解决多气候模型集成生成中的泛化问题，实现从有限模型运行生成统计一致的新气候变量实现。


<details>
  <summary>Details</summary>
Motivation: 大规模气候模型集成计算成本高昂，但许多下游分析需要更多统计一致的气候变量实现。现有方法在从有限模型运行生成新实现时存在泛化问题。

Method: 提出潜在约束条件变分自编码器(LC-CVAE)，在共享地理锚点位置强制潜在嵌入的跨实现同质性，然后使用多输出高斯过程回归在潜在空间预测新实现中未采样位置的坐标，最后解码生成完整时间序列场。

Result: 实验表明：1)在单个实现上训练不稳定；2)纳入约5个实现后收益递减；3)空间覆盖与重建质量之间存在权衡，这与潜在空间中的平均邻居距离密切相关。

Conclusion: LC-CVAE方法能有效从有限气候模型集成中生成统计一致的新实现，解决了传统CVAE在跨实现泛化上的局限性，为气候建模提供了一种计算高效的生成方法。

Abstract: Large climate-model ensembles are computationally expensive; yet many downstream analyses would benefit from additional, statistically consistent realizations of spatiotemporal climate variables. We study a generative modeling approach for producing new realizations from a limited set of available runs by transferring structure learned across an ensemble. Using monthly near-surface temperature time series from ten independent reanalysis realizations (ERA5), we find that a vanilla conditional variational autoencoder (CVAE) trained jointly across realizations yields a fragmented latent space that fails to generalize to unseen ensemble members. To address this, we introduce a latent-constrained CVAE (LC-CVAE) that enforces cross-realization homogeneity of latent embeddings at a small set of shared geographic 'anchor' locations. We then use multi-output Gaussian process regression in the latent space to predict latent coordinates at unsampled locations in a new realization, followed by decoding to generate full time series fields. Experiments and ablations demonstrate (i) instability when training on a single realization, (ii) diminishing returns after incorporating roughly five realizations, and (iii) a trade-off between spatial coverage and reconstruction quality that is closely linked to the average neighbor distance in latent space.

</details>


### [22] [Attention Needs to Focus: A Unified Perspective on Attention Allocation](https://arxiv.org/abs/2601.00919)
*Zichuan Fu,Wentao Song,Guojing Li,Yejing Wang,Xian Wu,Yimin Deng,Hanyu Yan,Yefeng Zheng,Xiangyu Zhao*

Main category: cs.LG

TL;DR: 本文提出Lazy Attention机制，通过统一视角解决Transformer注意力机制中的表示坍塌和注意力沉没问题，引入位置区分和弹性Softmax实现更聚焦的注意力分布。


<details>
  <summary>Details</summary>
Motivation: Transformer架构中的标准注意力机制存在两个已知问题：表示坍塌和注意力沉没。先前的研究通常孤立地处理这些问题，未能揭示它们之间的深层联系。本文旨在提供一个统一视角，认为这两个问题都源于注意力分配不当。

Method: 提出Lazy Attention机制：1) 针对注意力过载问题，使用跨头和维度的位置区分来增强token区分度；2) 针对注意力欠载问题，引入Elastic-Softmax归一化函数，放松标准softmax约束以抑制对无关token的关注。

Result: 在FineWeb-Edu语料库上的实验表明，Lazy Attention成功缓解了注意力沉没问题，在九个不同基准测试中与标准注意力和现代架构相比具有竞争力，同时实现了高达59.58%的注意力稀疏度。

Conclusion: Lazy Attention通过统一视角解决了注意力机制的两个核心问题，提供了一种更聚焦的注意力分配方法，在保持性能的同时实现了显著的注意力稀疏性。

Abstract: The Transformer architecture, a cornerstone of modern Large Language Models (LLMs), has achieved extraordinary success in sequence modeling, primarily due to its attention mechanism. However, despite its power, the standard attention mechanism is plagued by well-documented issues: representational collapse and attention sink. Although prior work has proposed approaches for these issues, they are often studied in isolation, obscuring their deeper connection. In this paper, we present a unified perspective, arguing that both can be traced to a common root -- improper attention allocation. We identify two failure modes: 1) Attention Overload, where tokens receive comparable high weights, blurring semantic features that lead to representational collapse; 2) Attention Underload, where no token is semantically relevant, yet attention is still forced to distribute, resulting in spurious focus such as attention sink. Building on this insight, we introduce Lazy Attention, a novel mechanism designed for a more focused attention distribution. To mitigate overload, it employs positional discrimination across both heads and dimensions to sharpen token distinctions. To counteract underload, it incorporates Elastic-Softmax, a modified normalization function that relaxes the standard softmax constraint to suppress attention on irrelevant tokens. Experiments on the FineWeb-Edu corpus, evaluated across nine diverse benchmarks, demonstrate that Lazy Attention successfully mitigates attention sink and achieves competitive performance compared to both standard attention and modern architectures, while reaching up to 59.58% attention sparsity.

</details>


### [23] [MODE: Efficient Time Series Prediction with Mamba Enhanced by Low-Rank Neural ODEs](https://arxiv.org/abs/2601.00920)
*Xingsheng Chen,Regina Zhang,Bo Gao,Xingwei He,Xiaofeng Liu,Pietro Lio,Kwok-Yan Lam,Siu-Ming Yiu*

Main category: cs.LG

TL;DR: MODE是一个统一的时间序列预测框架，结合了低秩神经常微分方程和增强Mamba架构，旨在高效处理长程依赖和不规则采样数据，在准确性和计算效率上超越现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列预测方法在处理长程依赖和不规则采样数据时，难以平衡效率、可扩展性和准确性。特别是在金融、医疗、能源系统和环境建模等领域，这些问题尤为突出。

Method: 提出MODE框架：1）使用线性标记化层转换输入序列；2）通过多个Mamba编码器块处理，每个块包含增强Mamba层（采用因果卷积、SiLU激活和低秩神经ODE增强）；3）引入分段选择性扫描机制，受伪ODE动力学启发，自适应关注重要子序列；4）低秩公式减少计算开销同时保持表达能力。

Result: 在基准数据集上的广泛实验表明，MODE在预测准确性和计算效率方面均超越了现有基线方法。

Conclusion: MODE为长期时间序列建模提供了一个统一高效架构，通过将Mamba的选择性扫描与低秩神经ODE集成，增强了时间表示能力，并通过低秩近似和动态选择性扫描实现了效率和可扩展性的显著提升。

Abstract: Time series prediction plays a pivotal role across diverse domains such as finance, healthcare, energy systems, and environmental modeling. However, existing approaches often struggle to balance efficiency, scalability, and accuracy, particularly when handling long-range dependencies and irregularly sampled data. To address these challenges, we propose MODE, a unified framework that integrates Low-Rank Neural Ordinary Differential Equations (Neural ODEs) with an Enhanced Mamba architecture. As illustrated in our framework, the input sequence is first transformed by a Linear Tokenization Layer and then processed through multiple Mamba Encoder blocks, each equipped with an Enhanced Mamba Layer that employs Causal Convolution, SiLU activation, and a Low-Rank Neural ODE enhancement to efficiently capture temporal dynamics. This low-rank formulation reduces computational overhead while maintaining expressive power. Furthermore, a segmented selective scanning mechanism, inspired by pseudo-ODE dynamics, adaptively focuses on salient subsequences to improve scalability and long-range sequence modeling. Extensive experiments on benchmark datasets demonstrate that MODE surpasses existing baselines in both predictive accuracy and computational efficiency. Overall, our contributions include: (1) a unified and efficient architecture for long-term time series modeling, (2) integration of Mamba's selective scanning with low-rank Neural ODEs for enhanced temporal representation, and (3) substantial improvements in efficiency and scalability enabled by low-rank approximation and dynamic selective scanning.

</details>


### [24] [Practical Geometric and Quantum Kernel Methods for Predicting Skeletal Muscle Outcomes in chronic obstructive pulmonary disease](https://arxiv.org/abs/2601.00921)
*Azadeh Alavi,Hamidreza Khalili,Stanley H. Chan,Fatemeh Kouchmeshki,Ross Vlahos*

Main category: cs.LG

TL;DR: 该研究使用量子核回归和几何感知方法，在慢性阻塞性肺疾病小鼠模型中，利用少量生物标志物预测肌肉功能障碍，相比传统方法获得性能提升。


<details>
  <summary>Details</summary>
Motivation: 慢性阻塞性肺疾病常伴随骨骼肌功能障碍，与全身和气道炎症相关。研究旨在通过微创生物标志物建立预测模型，实现肌肉结局的纵向监测。

Method: 使用213只小鼠的临床前数据集，比较传统基准模型、几何感知对称正定描述符（使用Stein散度）和量子核模型。特别关注量子核岭回归在四个可解释特征上的表现。

Result: 在肌肉重量预测中，量子核回归的测试均方根误差为4.41mg，决定系数0.605，优于传统岭回归（4.70mg，0.553）。几何方法在仅使用生物标志物时也有稳定提升。筛查评估的ROC-AUC最高达0.90。

Conclusion: 几何和量子核提升方法在低数据、低特征生物医学预测问题中能提供可测量的优势，同时保持可解释性和透明的模型选择。

Abstract: Skeletal muscle dysfunction is a clinically relevant extra-pulmonary manifestation of chronic obstructive pulmonary disease (COPD) and is closely linked to systemic and airway inflammation. This motivates predictive modelling of muscle outcomes from minimally invasive biomarkers that can be acquired longitudinally. We study a small-sample preclinical dataset comprising 213 animals across two conditions (Sham versus cigarette-smoke exposure), with blood and bronchoalveolar lavage fluid measurements and three continuous targets: tibialis anterior muscle weight (milligram: mg), specific force (millinewton: mN), and a derived muscle quality index (mN per mg). We benchmark tuned classical baselines, geometry-aware symmetric positive definite (SPD) descriptors with Stein divergence, and quantum kernel models designed for low-dimensional tabular data. In the muscle-weight setting, quantum kernel ridge regression using four interpretable inputs (blood C-reactive protein, neutrophil count, bronchoalveolar lavage cellularity, and condition) attains a test root mean squared error of 4.41 mg and coefficient of determination of 0.605, improving over a matched ridge baseline on the same feature set (4.70 mg and 0.553). Geometry-informed Stein-divergence prototype distances yield a smaller but consistent gain in the biomarker-only setting (4.55 mg versus 4.79 mg). Screening-style evaluation, obtained by thresholding the continuous outcome at 0.8 times the training Sham mean, achieves an area under the receiver operating characteristic curve (ROC-AUC) of up to 0.90 for detecting low muscle weight. These results indicate that geometric and quantum kernel lifts can provide measurable benefits in low-data, low-feature biomedical prediction problems, while preserving interpretability and transparent model selection.

</details>


### [25] [Complexity-based code embeddings](https://arxiv.org/abs/2601.00924)
*Rares Folea,Radu Iacob,Emil Slusanschi,Traian Rebedea*

Main category: cs.LG

TL;DR: 提出了一种将算法源代码转换为数值嵌入的通用方法，通过动态分析程序在不同输入下的行为，并为分析指标定制多个通用复杂度函数，基于r-Complexity构建代码嵌入，并在Codeforces真实代码片段数据集上实现XGBoost算法，在11类多标签分类中取得了良好效果。


<details>
  <summary>Details</summary>
Motivation: 需要一种通用的方法来将算法源代码转换为数值表示（嵌入），以便能够对程序行为进行量化分析和机器学习应用。传统的静态代码分析难以捕捉程序在不同输入下的动态行为特征。

Method: 通过动态分析程序在不同输入下的行为，为分析指标定制多个通用复杂度函数，基于r-Complexity构建代码嵌入。使用这些嵌入实现XGBoost算法进行多标签分类。

Result: 在基于Codeforces平台真实代码片段构建的11类多标签数据集上，使用提出的代码嵌入方法实现的XGBoost算法取得了良好的平均F1分数。

Conclusion: 提出的方法能够有效地将算法源代码转换为数值嵌入，为代码分析和机器学习应用提供了有效的表示方法，在真实世界的编程竞赛代码数据集上验证了其有效性。

Abstract: This paper presents a generic method for transforming the source code of various algorithms to numerical embeddings, by dynamically analysing the behaviour of computer programs against different inputs and by tailoring multiple generic complexity functions for the analysed metrics. The used algorithms embeddings are based on r-Complexity . Using the proposed code embeddings, we present an implementation of the XGBoost algorithm that achieves an average F1-score on a multi-label dataset with 11 classes, built using real-world code snippets submitted for programming competitions on the Codeforces platform.

</details>


### [26] [Reliability Under Randomness: An Empirical Analysis of Sparse and Dense Language Models Across Decoding Temperatures](https://arxiv.org/abs/2601.00942)
*Kabir Grover*

Main category: cs.LG

TL;DR: 稀疏MoE架构在随机解码下的可靠性研究表明，指令微调而非架构稀疏性是决定模型在确定性任务上对解码随机性鲁棒性的主要因素。


<details>
  <summary>Details</summary>
Motivation: 随着稀疏MoE架构在大型语言模型中的普及，需要研究其在随机解码下的可靠性。虽然条件计算带来了计算效率的提升，但稀疏路由与基于温度的采样之间的相互作用是否会损害输出稳定性尚不明确。

Method: 评估三个代表性模型：OLMoE-7B（稀疏基础模型）、Mixtral-8x7B（稀疏指令微调模型）和Qwen2.5-3B（密集指令微调模型）。在具有客观可验证答案的确定性算术推理任务上进行测试，涵盖四种解码配置（从贪婪解码到T=1.0），总计9,360次模型生成。

Result: 稀疏指令微调模型在所有解码温度下表现出与密集指令微调模型相当的稳定性，而稀疏基础模型随着温度升高出现系统性性能下降。这表明指令微调而非架构稀疏性是决定模型对解码随机性鲁棒性的主要因素。

Conclusion: 指令微调是确保稀疏语言模型在可靠性关键应用中稳定性的关键因素。研究结果为在特定场景下安全采用稀疏架构而不牺牲输出稳定性提供了指导。

Abstract: The increasing prevalence of sparse Mixture-of-Experts (MoE) architectures in large language models raises important questions regarding their reliability under stochastic decoding. While conditional computation enables substantial gains in computational efficiency, it remains unclear whether the interaction between sparse routing and temperature-based sampling compromises output stability relative to dense architectures. This work investigates whether conditional computation in MoE models amplifies decoding-induced randomness, leading to reduced reliability as temperature increases. We evaluate three representative models: OLMoE-7B (sparse base), Mixtral-8x7B (sparse instruction-tuned), and Qwen2.5-3B (dense instruction-tuned) on deterministic arithmetic reasoning tasks with objectively verifiable answers. Experiments span four decoding configurations, ranging from greedy decoding to T=1.0. Our evaluation encompasses accuracy, format compliance, output consistency across repeated generations, and confidence metrics, totaling 9,360 model generations. Results demonstrate that the sparse instruction-tuned model exhibits stability comparable to the dense instruction-tuned model across all decoding temperatures, while the sparse base model shows systematic degradation as temperature increases. These findings indicate that instruction tuning, rather than architectural sparsity, is the primary determinant of robustness to decoding randomness on deterministic tasks. We discuss the implications of these results for deploying sparse language models in reliability-critical applications, highlighting scenarios in which sparse architectures can be safely adopted without sacrificing output stability.

</details>


### [27] [Explainability-Guided Defense: Attribution-Aware Model Refinement Against Adversarial Data Attacks](https://arxiv.org/abs/2601.00968)
*Longwei Wang,Mohammad Navid Nayyem,Abdullah Al Rakin,KC Santosh,Chaowei Zhang,Yang Zhou*

Main category: cs.LG

TL;DR: 提出了一种基于LIME解释的对抗鲁棒性训练框架，通过抑制虚假特征提升模型鲁棒性和泛化能力


<details>
  <summary>Details</summary>
Motivation: 在医疗和自动驾驶等安全关键领域，深度学习模型需要同时具备对抗鲁棒性和决策透明度。研究发现LIME识别的虚假、不稳定或语义无关特征对对抗脆弱性有不成比例的贡献。

Method: 提出属性引导的精炼框架，将LIME从被动诊断工具转变为主动训练信号。通过特征掩码、敏感度感知正则化和对抗增强在闭环精炼流程中系统抑制虚假特征。

Result: 在CIFAR-10、CIFAR-10-C和CIFAR-100上的实验表明，该方法显著提升了对抗鲁棒性和分布外泛化能力，无需额外数据集或模型架构。

Conclusion: 建立了可解释性与鲁棒性之间的直接联系，通过利用解释信息主动指导训练过程，可以同时提升模型的透明度和对抗鲁棒性。

Abstract: The growing reliance on deep learning models in safety-critical domains such as healthcare and autonomous navigation underscores the need for defenses that are both robust to adversarial perturbations and transparent in their decision-making. In this paper, we identify a connection between interpretability and robustness that can be directly leveraged during training. Specifically, we observe that spurious, unstable, or semantically irrelevant features identified through Local Interpretable Model-Agnostic Explanations (LIME) contribute disproportionately to adversarial vulnerability. Building on this insight, we introduce an attribution-guided refinement framework that transforms LIME from a passive diagnostic into an active training signal. Our method systematically suppresses spurious features using feature masking, sensitivity-aware regularization, and adversarial augmentation in a closed-loop refinement pipeline. This approach does not require additional datasets or model architectures and integrates seamlessly into standard adversarial training. Theoretically, we derive an attribution-aware lower bound on adversarial distortion that formalizes the link between explanation alignment and robustness. Empirical evaluations on CIFAR-10, CIFAR-10-C, and CIFAR-100 demonstrate substantial improvements in adversarial robustness and out-of-distribution generalization.

</details>


### [28] [Contractive Diffusion Policies: Robust Action Diffusion via Contractive Score-Based Sampling with Differential Equations](https://arxiv.org/abs/2601.01003)
*Amin Abyaneh,Charlotte Morissette,Mohamad H. Danesh,Anas El Houssaini,David Meger,Gregory Dudek,Hsiu-Chin Lin*

Main category: cs.LG

TL;DR: 本文提出收缩扩散策略（CDPs），通过诱导扩散采样动力学的收缩行为来增强离线策略学习的鲁棒性，减少求解器和分数匹配误差的影响，在数据稀缺时表现尤为突出。


<details>
  <summary>Details</summary>
Motivation: 扩散策略虽然作为强大的生成模型在离线策略学习中表现出色，但其基于分数的SDE建模会带来求解器误差、分数匹配误差、大数据需求以及动作生成不一致等问题。这些在图像生成中不太关键的问题在连续控制环境中会累积并导致失败。

Method: 引入收缩扩散策略（CDPs），在扩散采样动力学中诱导收缩行为。收缩将附近的流拉近，增强对求解器和分数匹配误差的鲁棒性，同时减少不必要的动作方差。开发了深入的理论分析，并提供了实用的实现方法，可以最小化修改和计算成本地整合到现有扩散策略架构中。

Result: 在仿真和真实世界环境中进行了广泛的实验评估。在多个基准测试中，CDPs通常优于基线策略，在数据稀缺条件下表现出更显著的优势。

Conclusion: 收缩扩散策略通过引入收缩行为有效解决了扩散策略在连续控制中的误差累积问题，提高了鲁棒性和性能，特别是在数据有限的情况下，为离线策略学习提供了更可靠的解决方案。

Abstract: Diffusion policies have emerged as powerful generative models for offline policy learning, whose sampling process can be rigorously characterized by a score function guiding a Stochastic Differential Equation (SDE). However, the same score-based SDE modeling that grants diffusion policies the flexibility to learn diverse behavior also incurs solver and score-matching errors, large data requirements, and inconsistencies in action generation. While less critical in image generation, these inaccuracies compound and lead to failure in continuous control settings. We introduce Contractive Diffusion Policies (CDPs) to induce contractive behavior in the diffusion sampling dynamics. Contraction pulls nearby flows closer to enhance robustness against solver and score-matching errors while reducing unwanted action variance. We develop an in-depth theoretical analysis along with a practical implementation recipe to incorporate CDPs into existing diffusion policy architectures with minimal modification and computational cost. We evaluate CDPs for offline learning by conducting extensive experiments in simulation and real-world settings. Across benchmarks, CDPs often outperform baseline policies, with pronounced benefits under data scarcity.

</details>


### [29] [Data-Driven Assessment of Concrete Mixture Compositions on Chloride Transport via Standalone Machine Learning Algorithms](https://arxiv.org/abs/2601.01009)
*Mojtaba Aliasghar-Mamaghani,Mohammadreza Khalafi*

Main category: cs.LG

TL;DR: 该研究采用数据驱动方法，通过多种机器学习算法分析混凝土配合比对氯离子侵蚀时间演化的影响，以评估基础设施在侵蚀环境下的使用寿命。


<details>
  <summary>Details</summary>
Motivation: 评估混凝土结构在侵蚀环境下的使用寿命至关重要，需要理解混凝土配合比对氯离子侵蚀时间演化的影响，传统方法可能无法充分揭示其中的复杂关系。

Method: 采用多种简单和复杂的机器学习算法，包括线性回归、KNN回归、核岭回归、支持向量回归、高斯过程回归、多层感知机和门控循环单元，使用综合数据集进行分析。

Result: 核岭回归、高斯过程回归和多层感知机表现出高精度；大多数配合比组分与氯离子含量呈负相关，少数呈正相关；门控循环单元在测试集上表现不佳。

Conclusion: 机器学习方法能够有效揭示混凝土配合比与氯离子侵蚀时间演化之间的潜在关系，为描述氯离子侵蚀物理过程提供替代方法，有助于提高基础设施的使用寿命。

Abstract: This paper employs a data-driven approach to determine the impact of concrete mixture compositions on the temporal evolution of chloride in concrete structures. This is critical for assessing the service life of civil infrastructure subjected to aggressive environments. The adopted methodology relies on several simple and complex standalone machine learning (ML) algorithms, with the primary objective of establishing confidence in the unbiased prediction of the underlying hidden correlations. The simple algorithms include linear regression (LR), k-nearest neighbors (KNN) regression, and kernel ridge regression (KRR). The complex algorithms entail support vector regression (SVR), Gaussian process regression (GPR), and two families of artificial neural networks, including a feedforward network (multilayer perceptron, MLP) and a gated recurrent unit (GRU). The MLP architecture cannot explicitly handle sequential data, a limitation addressed by the GRU. A comprehensive dataset is considered. The performance of ML algorithms is evaluated, with KRR, GPR, and MLP exhibiting high accuracy. Given the diversity of the adopted concrete mixture proportions, the GRU was unable to accurately reproduce the response in the test set. Further analyses elucidate the contributions of mixture compositions to the temporal evolution of chloride. The results obtained from the GPR model unravel latent correlations through clear and explainable trends. The MLP, SVR, and KRR also provide acceptable estimates of the overall trends. The majority of mixture components exhibit an inverse relation with chloride content, while a few components demonstrate a direct correlation. These findings highlight the potential of surrogate approaches for describing the physical processes involved in chloride ingress and the associated correlations, toward the ultimate goal of enhancing the service life of civil infrastructure.

</details>


### [30] [Expanding the Chaos: Neural Operator for Stochastic (Partial) Differential Equations](https://arxiv.org/abs/2601.01021)
*Dai Shi,Lequan Lin,Andi Han,Luke Thompson,José Miguel Hernández-Lobato,Zhiyong Wang,Junbin Gao*

Main category: cs.LG

TL;DR: 基于Wiener混沌展开的神经算子架构，用于学习SDE/SPDE的解算子，通过正交Hermite特征投影噪声路径，用神经算子参数化确定性混沌系数，实现单次前向传播从噪声重建完整解轨迹。


<details>
  <summary>Details</summary>
Motivation: SDE和SPDE是建模随机动力学的基础工具，开发深度学习模型近似其解算子不仅能提供快速实用的求解器，还能从新视角解决经典学习任务。传统Wiener混沌展开方法为设计神经算子架构提供了理论基础。

Method: 基于经典Wiener混沌展开，将驱动噪声路径投影到正交Wick Hermite特征上，用神经算子参数化得到的确定性混沌系数，从而可以从噪声单次前向传播重建完整解轨迹。理论方面分析了多维SDE和半线性SPDE的混沌系数耦合ODE/PDE系统。

Result: 在多样化问题上验证了模型：经典SPDE基准测试、图像扩散一步采样、图拓扑插值、金融外推、参数估计以及洪水预测的流形SDE，展示了竞争性精度和广泛适用性。

Conclusion: 基于Wiener混沌展开的神经算子为学习SDE/SPDE解算子提供了实用且可扩展的方法，在多个领域表现出良好性能，表明该方法具有广泛的应用潜力。

Abstract: Stochastic differential equations (SDEs) and stochastic partial differential equations (SPDEs) are fundamental tools for modeling stochastic dynamics across the natural sciences and modern machine learning. Developing deep learning models for approximating their solution operators promises not only fast, practical solvers, but may also inspire models that resolve classical learning tasks from a new perspective. In this work, we build on classical Wiener chaos expansions (WCE) to design neural operator (NO) architectures for SPDEs and SDEs: we project the driving noise paths onto orthonormal Wick Hermite features and parameterize the resulting deterministic chaos coefficients with neural operators, so that full solution trajectories can be reconstructed from noise in a single forward pass. On the theoretical side, we investigate the classical WCE results for the class of multi-dimensional SDEs and semilinear SPDEs considered here by explicitly writing down the associated coupled ODE/PDE systems for their chaos coefficients, which makes the separation between stochastic forcing and deterministic dynamics fully explicit and directly motivates our model designs. On the empirical side, we validate our models on a diverse suite of problems: classical SPDE benchmarks, diffusion one-step sampling on images, topological interpolation on graphs, financial extrapolation, parameter estimation, and manifold SDEs for flood prediction, demonstrating competitive accuracy and broad applicability. Overall, our results indicate that WCE-based neural operators provide a practical and scalable way to learn SDE/SPDE solution operators across diverse domains.

</details>


### [31] [Wireless Dataset Similarity: Measuring Distances in Supervised and Unsupervised Machine Learning](https://arxiv.org/abs/2601.01023)
*João Morais,Sadjad Alikhani,Akshay Malhotra,Shahab Hamidi-Rad,Ahmed Alkhateeb*

Main category: cs.LG

TL;DR: 本文提出了一个任务和模型感知的无线数据集相似性测量框架，通过数据集距离预测模型跨数据集可迁移性，在CSI压缩和波束预测任务中验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 无线通信中需要比较不同数据集的相似性，以支持数据集选择/增强、仿真到真实场景比较、任务特定合成数据生成等应用，但缺乏系统性的测量框架。

Method: 提出任务和模型感知框架，通过数据集距离度量预测跨数据集可迁移性；在无监督CSI压缩任务中使用基于UMAP嵌入结合Wasserstein和欧氏距离的度量；在有监督波束预测任务中通过集成监督UMAP和数据集不平衡惩罚推导标签感知距离。

Result: 在CSI压缩任务中，使用UMAP嵌入结合Wasserstein和欧氏距离的度量实现了超过0.85的Pearson相关性；在波束预测任务中，标签感知距离优于传统基线，与模型可迁移性表现出更强的相关性。

Conclusion: 该框架能够有效测量无线数据集的相似性，支持任务相关的数据集比较，为数据集选择、模型训练和适应新部署提供决策依据。

Abstract: This paper introduces a task- and model-aware framework for measuring similarity between wireless datasets, enabling applications such as dataset selection/augmentation, simulation-to-real (sim2real) comparison, task-specific synthetic data generation, and informing decisions on model training/adaptation to new deployments. We evaluate candidate dataset distance metrics by how well they predict cross-dataset transferability: if two datasets have a small distance, a model trained on one should perform well on the other. We apply the framework on an unsupervised task, channel state information (CSI) compression, using autoencoders. Using metrics based on UMAP embeddings, combined with Wasserstein and Euclidean distances, we achieve Pearson correlations exceeding 0.85 between dataset distances and train-on-one/test-on-another task performance. We also apply the framework to a supervised beam prediction in the downlink using convolutional neural networks. For this task, we derive a label-aware distance by integrating supervised UMAP and penalties for dataset imbalance. Across both tasks, the resulting distances outperform traditional baselines and consistently exhibit stronger correlations with model transferability, supporting task-relevant comparisons between wireless datasets.

</details>


### [32] [Coarse-Grained Kullback--Leibler Control of Diffusion-Based Generative AI](https://arxiv.org/abs/2601.01045)
*Tatsuaki Tsuruyama*

Main category: cs.LG

TL;DR: 该研究将信息论Lyapunov函数框架移植到生成模型的逆向扩散过程，提出V-delta投影逆向扩散方法，实现对粗粒度量（如图像块强度）的显式控制。


<details>
  <summary>Details</summary>
Motivation: 扩散模型和基于分数的生成模型缺乏描述粗粒度量（如空间块强度）在逆向扩散过程中如何保留和演化的理论框架。需要一种方法来显式控制这些宏观特征。

Method: 将信息论Lyapunov函数V移植到逆向扩散过程，提出V-delta投影逆向扩散方法。该方法使用带容差的泄漏容忍势函数V-delta作为投影算子，扩展了V对非齐次块保持马尔可夫核的单调性。

Result: 在块常数图像和简化逆向核的玩具模型中，数值实验表明该方法能将块质量误差和泄漏容忍势控制在预设容差内，同时保持与非投影动态相当的像素级精度和视觉质量。

Conclusion: 该研究将生成采样重新解释为从噪声到数据的信息势下降过程，为具有粗粒度量显式控制的逆向扩散过程提供了设计原则。

Abstract: Diffusion models and score-based generative models provide a powerful framework for synthesizing high-quality images from noise. However, there is still no satisfactory theory that describes how coarse-grained quantities, such as blockwise intensity or class proportions after partitioning an image into spatial blocks, are preserved and evolve along the reverse diffusion dynamics. In previous work, the author introduced an information-theoretic Lyapunov function V for non-ergodic Markov processes on a state space partitioned into blocks, defined as the minimal Kullback-Leibler divergence to the set of stationary distributions reachable from a given initial condition, and showed that a leak-tolerant potential V-delta with a prescribed tolerance for block masses admits a closed-form expression as a scaling-and-clipping operation on block masses.
  In this paper, I transplant this framework to the reverse diffusion process in generative models and propose a reverse diffusion scheme that is projected by the potential V-delta (referred to as the V-delta projected reverse diffusion). I extend the monotonicity of V to time-inhomogeneous block-preserving Markov kernels and show that, under small leakage and the V-delta projection, V-delta acts as an approximate Lyapunov function. Furthermore, using a toy model consisting of block-constant images and a simplified reverse kernel, I numerically demonstrate that the proposed method keeps the block-mass error and the leak-tolerant potential within the prescribed tolerance, while achieving pixel-wise accuracy and visual quality comparable to the non-projected dynamics. This study reinterprets generative sampling as a decrease of an information potential from noise to data, and provides a design principle for reverse diffusion processes with explicit control of coarse-grained quantities.

</details>


### [33] [A UCB Bandit Algorithm for General ML-Based Estimators](https://arxiv.org/abs/2601.01061)
*Yajing Liu,Erkao Bao,Linqi Song*

Main category: cs.LG

TL;DR: ML-UCB算法将任意机器学习模型集成到多臂老虎机框架中，通过直接建模估计器的学习曲线行为，无需模型特定的理论分析即可实现次线性遗憾。


<details>
  <summary>Details</summary>
Motivation: 在序列决策中部署复杂ML模型面临的主要挑战是缺乏可处理的集中不等式来进行原则性探索。现有方法通常需要针对特定模型的理论分析，限制了通用性。

Method: 提出ML-UCB算法，假设均方误差随训练样本数呈幂律下降，推导出广义集中不等式。通过直接建模底层估计器的学习曲线行为，将任意ML模型集成到多臂老虎机框架中。

Result: ML-UCB算法实现了次线性遗憾。在协同过滤推荐系统的实验中，使用在线矩阵分解和模拟简化双塔模型的合成数据，相比LinUCB有显著改进。

Conclusion: ML-UCB框架实现了任意ML模型的原则性集成，只要其学习曲线可以经验表征，无需模型特定的理论分析，为序列决策中ML模型的部署提供了通用解决方案。

Abstract: We present ML-UCB, a generalized upper confidence bound algorithm that integrates arbitrary machine learning models into multi-armed bandit frameworks. A fundamental challenge in deploying sophisticated ML models for sequential decision-making is the lack of tractable concentration inequalities required for principled exploration. We overcome this limitation by directly modeling the learning curve behavior of the underlying estimator. Specifically, assuming the Mean Squared Error decreases as a power law in the number of training samples, we derive a generalized concentration inequality and prove that ML-UCB achieves sublinear regret. This framework enables the principled integration of any ML model whose learning curve can be empirically characterized, eliminating the need for model-specific theoretical analysis. We validate our approach through experiments on a collaborative filtering recommendation system using online matrix factorization with synthetic data designed to simulate a simplified two-tower model, demonstrating substantial improvements over LinUCB

</details>


### [34] [SPoRC-VIST: A Benchmark for Evaluating Generative Natural Narrative in Vision-Language Models](https://arxiv.org/abs/2601.01062)
*Yunlin Zeng*

Main category: cs.LG

TL;DR: 本文提出了一种端到端的视觉播客生成管道，通过微调Qwen3-VL-32B模型，使用合成到真实的训练策略，在4000个图像-对话对数据集上进行训练，显著提升了多说话者播客对话的生成质量。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在描述性任务上表现出色，但在生成引人入胜的长篇叙事（特别是多说话者播客对话）方面仍待探索且难以评估。传统评估指标如BLEU和ROUGE无法捕捉对话自然度、个性和叙事流畅性等细微差别。

Method: 1. 提出端到端视觉播客生成管道；2. 在4000个图像-对话对数据集上微调Qwen3-VL-32B模型；3. 采用合成到真实的训练策略：使用SPoRC高质量播客对话与合成生成图像进行训练，在VIST真实世界照片序列上评估；4. 提出综合评估框架，使用AI作为评判和新型风格指标。

Result: 微调的32B模型在对话自然度上显著优于235B基础模型（胜率>80%），叙事深度增加50%（平均对话轮次长度），同时保持相同的视觉基础能力（CLIPScore: 20.39）。

Conclusion: 通过合成到真实的训练策略和综合评估框架，可以有效提升视觉语言模型在生成引人入胜的多说话者播客对话方面的能力，证明了较小模型经过适当微调后可以在特定叙事任务上超越更大规模的基础模型。

Abstract: Vision-Language Models (VLMs) have achieved remarkable success in descriptive tasks such as image captioning and visual question answering (VQA). However, their ability to generate engaging, long-form narratives -- specifically multi-speaker podcast dialogues -- remains under-explored and difficult to evaluate. Standard metrics like BLEU and ROUGE fail to capture the nuances of conversational naturalness, personality, and narrative flow, often rewarding safe, repetitive outputs over engaging storytelling. In this work, we present a novel pipeline for end-to-end visual podcast generation, and fine-tune a Qwen3-VL-32B model on a curated dataset of 4,000 image-dialogue pairs. Crucially, we use a synthetic-to-real training strategy: we train on high-quality podcast dialogues from the Structured Podcast Research Corpus (SPoRC) paired with synthetically generated imagery, and evaluate on real-world photo sequences from the Visual Storytelling Dataset (VIST). This rigorous setup tests the model's ability to generalize from synthetic training data to real-world visual domains. We propose a comprehensive evaluation framework that moves beyond textual overlap, and use AI-as-a-judge (Gemini 3 Pro, Claude Opus 4.5, GPT 5.2) and novel style metrics (average turn length, speaker switch rate) to assess quality. Our experiments demonstrate that our fine-tuned 32B model significantly outperforms a 235B base model in conversational naturalness ($>$80\% win rate) and narrative depth (+50\% turn length), while maintaining identical visual grounding capabilities (CLIPScore: 20.39).

</details>


### [35] [Tiny Machine Learning for Real-Time Aquaculture Monitoring: A Case Study in Morocco](https://arxiv.org/abs/2601.01065)
*Achraf Hsain,Yahya Zaki,Othman Abaakil,Hibat-allah Bekkar,Yousra Chtouki*

Main category: cs.LG

TL;DR: 该研究提出将基于TinyML的低功耗边缘设备集成到水产养殖系统中，实现实时自动化监测和控制，以解决传统人工监测效率低、响应慢的问题。


<details>
  <summary>Details</summary>
Motivation: 水产养殖业面临水质波动、疾病爆发和饲料管理低效等挑战，传统人工监测方法耗时且可能导致问题处理延迟，需要更高效的自动化解决方案。

Method: 采用TinyML（微型机器学习）技术结合低功耗边缘设备，通过传感器实时监测pH值、温度、溶解氧、氨氮水平等关键参数，实现异常检测和自动报警。

Result: 系统能够提供实时水质数据，支持异常警报，收集的数据可用于优化水处理过程、饲料分配和饲料模式分析，提高饲料效率并降低运营成本。

Conclusion: 研究表明TinyML技术在水产养殖监测中具有可行性，通过考虑传感器选择、算法设计、硬件约束和伦理因素，可为开发更可持续、高效的水产养殖实践做出贡献。

Abstract: Aquaculture, the farming of aquatic organisms, is a rapidly growing industry facing challenges such as water quality fluctuations, disease outbreaks, and inefficient feed management. Traditional monitoring methods often rely on manual labor and are time consuming, leading to potential delays in addressing issues. This paper proposes the integration of low-power edge devices using Tiny Machine Learning (TinyML) into aquaculture systems to enable real-time automated monitoring and control, such as collecting data and triggering alarms, and reducing labor requirements. The system provides real-time data on the required parameters such as pH levels, temperature, dissolved oxygen, and ammonia levels to control water quality, nutrient levels, and environmental conditions enabling better maintenance, efficient resource utilization, and optimal management of the enclosed aquaculture space. The system enables alerts in case of anomaly detection. The data collected by the sensors over time can serve for important decision-making regarding optimizing water treatment processes, feed distribution, feed pattern analysis and improve feed efficiency, reducing operational costs. This research explores the feasibility of developing TinyML-based solutions for aquaculture monitoring, considering factors such as sensor selection, algorithm design, hardware constraints, and ethical considerations. By demonstrating the potential benefits of TinyML in aquaculture, our aim is to contribute to the development of more sustainable and efficient farming practices.

</details>


### [36] [Discount Model Search for Quality Diversity Optimization in High-Dimensional Measure Spaces](https://arxiv.org/abs/2601.01082)
*Bryon Tjanaka,Henry Chen,Matthew C. Fontaine,Stefanos Nikolaidis*

Main category: cs.LG

TL;DR: DMS（Discount Model Search）是一种新的质量多样性优化算法，通过使用连续折扣模型解决高维度量空间中的探索停滞问题，优于现有的CMA-MAE等算法。


<details>
  <summary>Details</summary>
Motivation: 当前QD算法主要针对低维度度量空间，因为高维度度量容易导致失真问题——许多解决方案映射到相似的度量值。CMA-MAE等算法在高维空间中停滞，因为相似度量的解决方案落入同一直方图单元并获得相同的折扣值。

Method: 提出折扣模型搜索（DMS），使用提供平滑连续折扣值表示的模型来指导探索。在高维度量空间中，该模型能够区分具有相似度量的解决方案，从而持续探索。

Result: DMS在图像作为高维度度量空间的新应用领域和高维度基准测试中，表现优于CMA-MAE和其他黑盒QD算法。

Conclusion: DMS通过连续折扣模型解决了高维度量空间中的探索停滞问题，使QD能够应用于图像等高维度量空间，用户只需提供图像数据集而无需手动设计度量函数。

Abstract: Quality diversity (QD) optimization searches for a collection of solutions that optimize an objective while attaining diverse outputs of a user-specified, vector-valued measure function. Contemporary QD algorithms focus on low-dimensional measures because high-dimensional measures are prone to distortion, where many solutions found by the QD algorithm map to similar measures. For example, the CMA-MAE algorithm guides measure space exploration with a histogram in measure space that records so-called discount values. However, CMA-MAE stagnates in domains with high-dimensional measure spaces because solutions with similar measures fall into the same histogram cell and thus receive identical discount values. To address these limitations, we propose Discount Model Search (DMS), which guides exploration with a model that provides a smooth, continuous representation of discount values. In high-dimensional measure spaces, this model enables DMS to distinguish between solutions with similar measures and thus continue exploration. We show that DMS facilitates new QD applications by introducing two domains where the measure space is the high-dimensional space of images, which enables users to specify their desired measures by providing a dataset of images rather than hand-designing the measure function. Results in these domains and on high-dimensional benchmarks show that DMS outperforms CMA-MAE and other black-box QD algorithms.

</details>


### [37] [Community-Based Early-Stage Chronic Kidney Disease Screening using Explainable Machine Learning for Low-Resource Settings](https://arxiv.org/abs/2601.01119)
*Muhammad Ashad Kabir,Sirajam Munira,Dewan Tasnia Azad,Saleh Mohammed Ikram,Mohammad Habibur Rahman Sarker,Syed Manzoor Ahmed Hanifi*

Main category: cs.LG

TL;DR: 开发了一个针对孟加拉国和南亚人群的可解释机器学习框架，用于早期慢性肾病筛查，在资源有限的环境中表现优于现有工具。


<details>
  <summary>Details</summary>
Motivation: 现有慢性肾病筛查工具主要基于高收入国家人群开发，在孟加拉国和南亚地区表现不佳，这些地区的风险特征不同。现有工具依赖简单的加性评分函数，基于晚期肾病患者数据，无法捕捉风险因素间的复杂交互作用，在预测早期肾病方面有限。

Method: 使用孟加拉国社区数据集（南亚首个此类数据集），评估12种机器学习分类器，应用10种互补的特征选择技术识别稳健、可泛化的预测因子。采用10折交叉验证评估最终模型，并在印度、阿联酋和孟加拉国的三个独立数据集上进行外部验证。使用SHAP提供模型可解释性。

Result: 基于RFECV选择特征子集的机器学习模型达到90.40%的平衡准确率，而最少非病理检测特征表现出色，达到89.23%的平衡准确率，通常优于更大或完整特征集。相比现有筛查工具，所提模型实现了显著更高的准确率和敏感性，同时需要更少且更易获取的输入。外部验证显示78%至98%的敏感性，证实了强泛化能力。SHAP解释识别出与已确立的慢性肾病风险因素一致的临床意义预测因子。

Conclusion: 开发了一个针对孟加拉国和南亚人群的可解释机器学习框架，用于早期慢性肾病筛查，在资源有限的环境中表现优于现有工具，具有强泛化能力和临床可解释性。

Abstract: Early detection of chronic kidney disease (CKD) is essential for preventing progression to end-stage renal disease. However, existing screening tools - primarily developed using populations from high-income countries - often underperform in Bangladesh and South Asia, where risk profiles differ. Most of these tools rely on simple additive scoring functions and are based on data from patients with advanced-stage CKD. Consequently, they fail to capture complex interactions among risk factors and are limited in predicting early-stage CKD. Our objective was to develop and evaluate an explainable machine learning (ML) framework for community-based early-stage CKD screening for low-resource settings, tailored to the Bangladeshi and South Asian population context. We used a community-based dataset from Bangladesh, the first such CKD dataset in South and South Asia, and evaluated twelve ML classifiers across multiple feature domains. Ten complementary feature selection techniques were applied to identify robust, generalizable predictors. The final models were assessed using 10-fold cross-validation. External validation was conducted on three independent datasets from India, the UAE, and Bangladesh. SHAP (SHapley Additive exPlanations) was used to provide model explainability. An ML model trained on an RFECV-selected feature subset achieved a balanced accuracy of 90.40%, whereas minimal non-pathology-test features demonstrated excellent predictive capability with a balanced accuracy of 89.23%, often outperforming larger or full feature sets. Compared with existing screening tools, the proposed models achieved substantially higher accuracy and sensitivity while requiring fewer and more accessible inputs. External validation confirmed strong generalizability with 78% to 98% sensitivity. SHAP interpretation identified clinically meaningful predictors consistent with established CKD risk factors.

</details>


### [38] [Self-Training the Neurochaos Learning Algorithm](https://arxiv.org/abs/2601.01146)
*Anusree M,Akhila Henry,Pramod P Nair*

Main category: cs.LG

TL;DR: 本文提出了一种结合神经混沌学习与自训练的半监督学习架构，用于解决标注数据稀缺和不平衡数据集的问题，在多个基准数据集上取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 在许多实际应用中，获取大量标注数据既困难又昂贵，而未标注数据却容易获得。传统的监督学习方法在标注数据稀少或数据集不平衡的情况下表现不佳，需要一种能够有效利用未标注数据的解决方案。

Method: 提出了一种混合半监督学习架构，将神经混沌学习与基于阈值的自训练方法相结合。神经混沌学习将输入特征转换为混沌发放率表示，捕捉数据中的非线性关系；自训练则通过高置信度的伪标注样本来逐步扩大标注数据集。

Result: 在10个基准数据集和5种机器学习分类器上进行评估，其中85%的训练数据作为未标注数据，仅15%作为标注数据。提出的NL+ST架构相比单独的自训练模型获得了显著性能提升，特别是在有限、非线性和不平衡数据集上，如Iris（188.66%）、Wine（158.58%）和Glass Identification（110.48%）。

Conclusion: 结果表明，将基于混沌的特征提取与半监督学习结合使用，在低数据情境下能够提高模型的泛化能力、鲁棒性和分类准确性，为解决标注数据稀缺问题提供了有效方案。

Abstract: In numerous practical applications, acquiring substantial quantities of labelled data is challenging and expensive, but unlabelled data is readily accessible. Conventional supervised learning methods frequently underperform in scenarios characterised by little labelled data or imbalanced datasets. This study introduces a hybrid semi-supervised learning (SSL) architecture that integrates Neurochaos Learning (NL) with a threshold-based Self-Training (ST) method to overcome this constraint. The NL architecture converts input characteristics into chaos-based ring-rate representations that encapsulate nonlinear relationships within the data, whereas ST progressively enlarges the labelled set utilising high-confidence pseudo-labelled samples. The model's performance is assessed using ten benchmark datasets and five machine learning classifiers, with 85% of the training data considered unlabelled and just 15% utilised as labelled data. The proposed Self-Training Neurochaos Learning (NL+ST) architecture consistently attains superior performance gain relative to standalone ST models, especially on limited, nonlinear and imbalanced datasets like Iris (188.66%), Wine (158.58%) and Glass Identification (110.48%). The results indicate that using chaos-based feature extraction with SSL improves generalisation, resilience, and classification accuracy in low-data contexts.

</details>


### [39] [Evo-TFS: Evolutionary Time-Frequency Domain-Based Synthetic Minority Oversampling Approach to Imbalanced Time Series Classification](https://arxiv.org/abs/2601.01150)
*Wenbin Pei,Ruohao Dai,Bing Xue,Mengjie Zhang,Qiang Zhang,Yiu-Ming Cheung*

Main category: cs.LG

TL;DR: Evo-TFS是一种新颖的进化过采样方法，结合时域和频域特征来处理不平衡时间序列分类问题，通过强类型遗传编程生成多样化的高质量时间序列样本。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法假设数据分布平衡，在不平衡分布下会忽略少数类（通常具有更高实际意义）。传统过采样方法依赖线性插值，难以保持时间动态性和生成多样化样本。

Method: 提出Evo-TFS方法，使用强类型遗传编程进化生成多样化的高质量时间序列，通过包含时域和频域特征的适应度函数进行指导。

Result: 在不平衡时间序列数据集上的实验表明，Evo-TFS优于现有过采样方法，显著提升了时域和频域分类器的性能。

Conclusion: Evo-TFS通过结合时域和频域特征的进化过采样方法，有效解决了不平衡时间序列分类问题，生成了更高质量和多样性的少数类样本。

Abstract: Time series classification is a fundamental machine learning task with broad real-world applications. Although many deep learning methods have proven effective in learning time-series data for classification, they were originally developed under the assumption of balanced data distributions. Once data distribution is uneven, these methods tend to ignore the minority class that is typically of higher practical significance. Oversampling methods have been designed to address this by generating minority-class samples, but their reliance on linear interpolation often hampers the preservation of temporal dynamics and the generation of diverse samples. Therefore, in this paper, we propose Evo-TFS, a novel evolutionary oversampling method that integrates both time- and frequency-domain characteristics. In Evo-TFS, strongly typed genetic programming is employed to evolve diverse, high-quality time series, guided by a fitness function that incorporates both time-domain and frequency-domain characteristics. Experiments conducted on imbalanced time series datasets demonstrate that Evo-TFS outperforms existing oversampling methods, significantly enhancing the performance of time-domain and frequency-domain classifiers.

</details>


### [40] [Bridging the Semantic Gap for Categorical Data Clustering via Large Language Models](https://arxiv.org/abs/2601.01162)
*Zihua Yang,Xin Liao,Yiqun Zhang,Yiu-ming Cheung*

Main category: cs.LG

TL;DR: ARISE利用大语言模型的外部语义知识增强分类数据聚类，通过注意力加权表示整合语义嵌入，在8个基准数据集上比7个对比方法提升19-27%


<details>
  <summary>Details</summary>
Motivation: 分类数据聚类面临相似性度量困难，传统方法将无序属性值视为等距，造成语义鸿沟；现有基于共现模式的方法在样本有限时不可靠，未能充分利用数据的语义上下文

Method: 提出ARISE方法，利用大语言模型获取外部语义知识，构建语义感知表示；采用注意力机制将LLM增强的嵌入与原始数据结合，探索语义显著的聚类结构

Result: 在8个基准数据集上的实验表明，ARISE相比7个代表性对比方法取得一致改进，性能提升达19-27%

Conclusion: 通过整合大语言模型的外部语义知识，ARISE能够有效弥补分类数据聚类的语义鸿沟，显著提升聚类质量

Abstract: Categorical data are prevalent in domains such as healthcare, marketing, and bioinformatics, where clustering serves as a fundamental tool for pattern discovery. A core challenge in categorical data clustering lies in measuring similarity among attribute values that lack inherent ordering or distance. Without appropriate similarity measures, values are often treated as equidistant, creating a semantic gap that obscures latent structures and degrades clustering quality. Although existing methods infer value relationships from within-dataset co-occurrence patterns, such inference becomes unreliable when samples are limited, leaving the semantic context of the data underexplored. To bridge this gap, we present ARISE (Attention-weighted Representation with Integrated Semantic Embeddings), which draws on external semantic knowledge from Large Language Models (LLMs) to construct semantic-aware representations that complement the metric space of categorical data for accurate clustering. That is, LLM is adopted to describe attribute values for representation enhancement, and the LLM-enhanced embeddings are combined with the original data to explore semantically prominent clusters. Experiments on eight benchmark datasets demonstrate consistent improvements over seven representative counterparts, with gains of 19-27%. Code is available at https://github.com/develop-yang/ARISE

</details>


### [41] [MentalGame: Predicting Personality-Job Fitness for Software Developers Using Multi-Genre Games and Machine Learning Approaches](https://arxiv.org/abs/2601.01206)
*Soroush Elyasi,Arya VarastehNezhad,Fattaneh Taghiyareh*

Main category: cs.LG

TL;DR: 该研究开发了一个多类型严肃游戏框架，结合机器学习技术，通过分析游戏中的行为数据来预测软件开发岗位的适合度，避免了传统自我报告问卷的偏见问题。


<details>
  <summary>Details</summary>
Motivation: 传统职业评估中使用的自我报告问卷存在响应偏差、疲劳和故意扭曲等问题，需要一种更客观、无偏见的替代方法。游戏化评估通过捕捉游戏过程中的隐式行为信号，为职业适合度评估提供了新途径。

Method: 通过系统文献综述和软件工程师实证研究确定与开发相关的个性和行为特征；设计定制移动游戏来引出问题解决、规划、适应性、坚持性、时间管理和信息寻求等行为；采用两阶段建模策略，仅从游戏行为特征预测适合度。

Result: 模型达到最高97%的精确度和94%的准确度；行为分析显示合适的候选人表现出独特的游戏模式：在解谜游戏中获胜更多、完成更多侧边挑战、更频繁导航菜单、暂停、重试和放弃行为更少。

Conclusion: 游戏过程中捕捉的隐式行为痕迹能够有效预测软件开发适合度，无需显式个性测试，支持严肃游戏作为职业评估的可扩展、吸引人且偏见较少的替代方案。

Abstract: Personality assessment in career guidance and personnel selection traditionally relies on self-report questionnaires, which are susceptible to response bias, fatigue, and intentional distortion. Game-based assessment offers a promising alternative by capturing implicit behavioral signals during gameplay. This study proposes a multi-genre serious-game framework combined with machine-learning techniques to predict suitability for software development roles. Developer-relevant personality and behavioral traits were identified through a systematic literature review and an empirical study of professional software engineers. A custom mobile game was designed to elicit behaviors related to problem solving, planning, adaptability, persistence, time management, and information seeking. Fine-grained gameplay event data were collected and analyzed using a two-phase modeling strategy where suitability was predicted exclusively from gameplay-derived behavioral features. Results show that our model achieved up to 97% precision and 94% accuracy. Behavioral analysis revealed that proper candidates exhibited distinct gameplay patterns, such as more wins in puzzle-based games, more side challenges, navigating menus more frequently, and exhibiting fewer pauses, retries, and surrender actions. These findings demonstrate that implicit behavioral traces captured during gameplay is promising in predicting software-development suitability without explicit personality testing, supporting serious games as a scalable, engaging, and less biased alternative for career assessment.

</details>


### [42] [The Alchemy of Thought: Understanding In-Context Learning Through Supervised Classification](https://arxiv.org/abs/2601.01290)
*Harshita Narnoli,Mihai Surdeanu*

Main category: cs.LG

TL;DR: 本文通过比较LLM的上下文学习与基于相同示例训练的分类器行为，研究了上下文学习的工作原理，发现当演示相关性高时，LLM行为类似于分类器，且更接近kNN而非逻辑回归；当相关性低时，LLM表现更好，因为它可以依赖参数化记忆。


<details>
  <summary>Details</summary>
Motivation: 尽管上下文学习在实践中被证明有效，但其工作机制仍不清楚。本文旨在通过比较LLM的上下文学习行为与基于相同示例训练的分类器，探究上下文学习的工作原理，特别是它更接近梯度下降还是k近邻算法，以及在什么条件下会产生行为差异。

Method: 使用文本分类作为用例，在六个数据集和三个LLM上进行实验。将LLM的上下文学习行为与基于相同演示示例训练的分类器进行比较，包括基于梯度下降的逻辑回归和基于k近邻的分类器。分析不同条件下（特别是演示相关性高低）的行为相似性和差异。

Result: 当演示相关性高时，LLM的行为与这些分类器相似，且更接近kNN而非逻辑回归，这为注意力机制更类似于kNN而非梯度下降提供了实证证据。当演示相关性低时，LLM表现优于这些分类器，可能是因为LLM可以依赖其参数化记忆，而这些分类器没有这种优势。

Conclusion: 上下文学习在演示相关性高时类似于基于示例的分类器，特别是更接近kNN算法；而在演示相关性低时，LLM能够利用其参数化记忆获得更好性能。这有助于我们理解上下文学习的工作机制，并区分其何时依赖于演示示例，何时依赖于预训练知识。

Abstract: In-context learning (ICL) has become a prominent paradigm to rapidly customize LLMs to new tasks without fine-tuning. However, despite the empirical evidence of its usefulness, we still do not truly understand how ICL works. In this paper, we compare the behavior of in-context learning with supervised classifiers trained on ICL demonstrations to investigate three research questions: (1) Do LLMs with ICL behave similarly to classifiers trained on the same examples? (2) If so, which classifiers are closer, those based on gradient descent (GD) or those based on k-nearest neighbors (kNN)? (3) When they do not behave similarly, what conditions are associated with differences in behavior? Using text classification as a use case, with six datasets and three LLMs, we observe that LLMs behave similarly to these classifiers when the relevance of demonstrations is high. On average, ICL is closer to kNN than logistic regression, giving empirical evidence that the attention mechanism behaves more similarly to kNN than GD. However, when demonstration relevance is low, LLMs perform better than these classifiers, likely because LLMs can back off to their parametric memory, a luxury these classifiers do not have.

</details>


### [43] [ARGUS: Adaptive Rotation-Invariant Geometric Unsupervised System](https://arxiv.org/abs/2601.01297)
*Anantha Sharma*

Main category: cs.LG

TL;DR: Argus框架将高维数据流中的分布漂移检测重新定义为在数据流形固定空间划分上跟踪局部统计量，解决了现有方法在可扩展性、几何结构保持和身份稳定性方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 检测高维数据流中的分布漂移面临三个主要挑战：全局比较方法扩展性差、基于投影的方法丢失几何结构、重新聚类方法存在身份不稳定性。需要一种既能保持高维结构又计算高效的方法。

Method: Argus框架通过固定空间划分（Voronoi镶嵌）跟踪局部统计量来检测漂移。使用规范正交基上的Voronoi镶嵌确保正交变换不变性，引入图论方法区分连贯分布漂移与孤立扰动，采用乘积量化镶嵌扩展到超高维度（d>500）。

Result: 该框架实现了O(N)复杂度，提供单元级空间定位，正确识别坐标旋转下的漂移（现有方法会产生误报），在超高维度下保持可扩展性，并能够区分不同类型的分布变化。

Conclusion: Argus为分布监控提供了有原则的几何基础，既保持了高维结构又避免了成对比较的计算负担，通过固定空间划分和局部统计跟踪实现了高效准确的漂移检测。

Abstract: Detecting distributional drift in high-dimensional data streams presents fundamental challenges: global comparison methods scale poorly, projection-based approaches lose geometric structure, and re-clustering methods suffer from identity instability. This paper introduces Argus, A framework that reconceptualizes drift detection as tracking local statistics over a fixed spatial partition of the data manifold.
  The key contributions are fourfold. First, it is proved that Voronoi tessellations over canonical orthonormal frames yield drift metrics that are invariant to orthogonal transformations. The rotations and reflections that preserve Euclidean geometry. Second, it is established that this framework achieves O(N) complexity per snapshot while providing cell-level spatial localization of distributional change. Third, a graph-theoretic characterization of drift propagation is developed that distinguishes coherent distributional shifts from isolated perturbations. Fourth, product quantization tessellation is introduced for scaling to very high dimensions (d>500) by decomposing the space into independent subspaces and aggregating drift signals across subspaces.
  This paper formalizes the theoretical foundations, proves invariance properties, and presents experimental validation demonstrating that the framework correctly identifies drift under coordinate rotation while existing methods produce false positives. The tessellated approach offers a principled geometric foundation for distribution monitoring that preserves high-dimensional structure without the computational burden of pairwise comparisons.

</details>


### [44] [Towards a Principled Muon under $μ\mathsf{P}$: Ensuring Spectral Conditions throughout Training](https://arxiv.org/abs/2601.01306)
*John Zhao*

Main category: cs.LG

TL;DR: 本文提出Muon++，一种改进的矩阵优化器，能够在整个训练过程中可靠地保证μP（μ-参数化）所需的光谱条件，无需对权重进行显式光谱归一化，从而在长时程LLM训练中实现理论μP与实际部署的桥梁。


<details>
  <summary>Details</summary>
Motivation: μP为大型语言模型训练提供了理论基础，但现有研究在矩阵优化器（如Muon）的实际训练中难以保证μP所需的光谱条件：要么无法确保整个训练过程中条件成立，要么需要重复的光谱归一化操作，导致计算开销大且实用性降低。

Method: 提出Muon++变体，其关键洞察是：对于中等大型模型，仅需在优化器更新层面保持光谱控制就足以维持μP兼容的缩放，无需对权重进行显式光谱归一化。同时首次尝试将数据依赖效应纳入自适应光谱条件，使其更适合长时程LLM训练。

Result: 开发出能够在整个训练过程中满足光谱条件的Muon++，填补了μP理论承诺与矩阵优化器实际部署之间的差距，同时通过自适应光谱条件提高了长时程训练的适用性。

Conclusion: Muon++为矩阵优化器在μP框架下的实际应用提供了可行的解决方案，通过仅控制优化器更新的光谱特性，既保证了理论要求的满足，又避免了过大的计算开销，推动了μP理论在长时程LLM训练中的实际部署。

Abstract: The $μ$-parameterization ($μ$P) provides a principled foundation for large language model (LLM) training by prescribing width-independent learning dynamics, which in turn enables predictable scaling behavior and robust hyperparameter transfer across model sizes. A central requirement of $μ$P is the satisfaction of certain spectral conditions on weight matrices, which ensure consistent feature learning and optimization behavior as model width grows. While these conditions are well understood in theory, guaranteeing their validity in practical training for matrix-based optimizers such as Muon is still under studied. Existing works that study Muon under $μ$P exhibit important limitations: they either do not ensure that the spectral conditions hold throughout the entire training horizon, or require repeated spectral normalization (or Newton-Schulz iterations) applied to both weights and updates, leading to significant computational overhead and reduced practicality. In this work, we show how to reliably guarantee the spectral conditions required by $μ$P for Muon during the entire training process. Our key insight is that for moderately large models, maintaining spectral control at the level of optimizer updates alone is sufficient to preserve $μ$P-compatible scaling, eliminating the need for explicit spectral normalization of the weights. Based on this principle, we develop a variant of Muon, namely Muon++, that satisfies spectral condition throughout the training process. Our results bridge the gap between the theoretical promises of $μ$P and the practical deployment of matrix-based optimizers in long-horizon training. We also take the first step towards an adaptive spectral condition by incorporating data-dependent effects, making it better suited for long-horizon LLM training.

</details>


### [45] [Spectral-Window Hybrid (SWH)](https://arxiv.org/abs/2601.01313)
*Vladimer Khasia*

Main category: cs.LG

TL;DR: 提出SWH架构，通过全局分支（卷积定理）和局部分支（滑动窗口注意力）并行建模序列，实现线性扩展至长序列，同时保持局部精度。


<details>
  <summary>Details</summary>
Motivation: Transformer的二次方复杂度限制了其在长序列任务中的应用，需要在计算效率和表示表达能力之间取得平衡。

Method: 提出Spectral-Window Hybrid架构，将序列建模解耦为两个并行流：全局分支利用卷积定理在O(T log T)时间内建模长程衰减动态，局部分支使用滑动窗口注意力处理有限上下文内的token交互。

Result: SWH在短上下文上达到标准Transformer的困惑度水平，同时能够高效线性扩展到长序列。

Conclusion: SWH架构避免了全局注意力的计算瓶颈，同时保留了局部精度，为长序列建模提供了高效解决方案。

Abstract: Scaling sequence modeling to extreme contexts requires balancing computational efficiency with representational expressivity. While Transformers provide precise retrieval via the attention mechanism, their quadratic $\mathcal{O}(T^2)$ complexity limits their application to long-horizon tasks. In this work, we propose the \textbf{Spectral-Window Hybrid (SWH)}, an architecture that decouples sequence modeling into two \textit{parallel} streams: a global branch utilizing the Convolution Theorem to model long-range decay dynamics in $\mathcal{O}(T \log T)$ time, and a local branch employing sliding-window attention for token interactions within a bounded context. By aggregating these representations, SWH avoids the computational bottleneck of global attention while retaining local precision. We demonstrate that SWH matches the perplexity of standard Transformers on short contexts while enabling efficient linear scaling to extended sequences. The code is available at https://github.com/VladimerKhasia/SWH

</details>


### [46] [From Classification to Generation: An Open-Ended Paradigm for Adverse Drug Reaction Prediction Based on Graph-Motif Feature Fusion](https://arxiv.org/abs/2601.01347)
*Yuyan Pi,Min Jin,Wentao Xie,Xinhua Liu*

Main category: cs.LG

TL;DR: 提出GM-MLG方法，将药物不良反应预测从多标签分类转化为基于Transformer解码器的多标签生成，通过图-基序特征融合解决数据稀缺和标签依赖性问题，显著提升预测性能并扩展预测空间。


<details>
  <summary>Details</summary>
Motivation: 当前药物不良反应预测方法面临药物数据稀缺导致的冷启动问题、封闭标签集以及标签依赖关系建模不足等挑战，限制了预测效果和应用范围。

Method: 基于图-基序特征融合和多标签生成(GM-MLG)：1) 构建原子级、局部分子级(通过BRICS算法动态提取精细基序)和全局分子级的双图表示架构；2) 将ADR预测转化为基于Transformer解码器的多标签生成任务，将ADR标签视为离散标记序列，使用位置嵌入显式捕获标签依赖关系，通过自回归解码动态扩展预测空间。

Result: GM-MLG实现了最高38%的性能提升，平均增益20%，将预测空间从200种扩展到超过10,000种。通过逆合成基序分析阐明了ADR与基序之间的非线性结构-活性关系。

Conclusion: GM-MLG为药物不良反应预测提供了开放式的预测范式，有效解决了冷启动和标签依赖性问题，显著提升了预测性能并扩展了预测范围，为药物安全系统风险降低提供了可解释的创新支持。

Abstract: Computational biology offers immense potential for reducing the high costs and protracted cycles of new drug development through adverse drug reaction (ADR) prediction. However, current methods remain impeded by drug data scarcity-induced cold-start challenge, closed label sets, and inadequate modeling of label dependencies. Here we propose an open-ended ADR prediction paradigm based on Graph-Motif feature fusion and Multi-Label Generation (GM-MLG). Leveraging molecular structure as an intrinsic and inherent feature, GM-MLG constructs a dual-graph representation architecture spanning the atomic level, the local molecular level (utilizing fine-grained motifs dynamically extracted via the BRICS algorithm combined with additional fragmentation rules), and the global molecular level. Uniquely, GM-MLG pioneers transforming ADR prediction from multi-label classification into Transformer Decoder-based multi-label generation. By treating ADR labels as discrete token sequences, it employs positional embeddings to explicitly capture dependencies and co-occurrence relationships within large-scale label spaces, generating predictions via autoregressive decoding to dynamically expand the prediction space. Experiments demonstrate GM-MLG achieves up to 38% improvement and an average gain of 20%, expanding the prediction space from 200 to over 10,000 types. Furthermore, it elucidates non-linear structure-activity relationships between ADRs and motifs via retrosynthetic motif analysis, providing interpretable and innovative support for systematic risk reduction in drug safety.

</details>


### [47] [Causal discovery for linear causal model with correlated noise: an Adversarial Learning Approach](https://arxiv.org/abs/2601.01368)
*Mujin Zhou,Junzhe Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种基于f-GAN框架的因果发现方法，用于处理存在未测量混杂因素的数据，通过将结构学习问题转化为最小化贝叶斯自由能量，并证明其等价于最小化真实数据分布与模型生成分布之间的f-散度。


<details>
  <summary>Details</summary>
Motivation: 从存在未测量混杂因素的数据中进行因果发现是一个具有挑战性的问题。现有方法在处理这种情况时存在局限性，需要一种能够学习与具体权重值无关的二元因果结构的方法。

Method: 基于f-GAN框架，将结构学习问题重新表述为最小化贝叶斯自由能量，并证明该问题等价于最小化真实数据分布与模型生成分布之间的f-散度。利用f-GAN框架将此目标转化为最小最大对抗优化问题，并使用Gumbel-Softmax松弛在离散图空间中进行梯度搜索。

Result: 该方法能够从存在未测量混杂因素的数据中学习二元因果结构，而不依赖于具体的权重值。通过对抗优化和Gumbel-Softmax技术实现了在离散图空间的有效搜索。

Conclusion: 提出的基于f-GAN框架的方法为处理存在未测量混杂因素的因果发现问题提供了新的解决方案，通过理论证明和算法实现展示了其可行性和有效性。

Abstract: Causal discovery from data with unmeasured confounding factors is a challenging problem. This paper proposes an approach based on the f-GAN framework, learning the binary causal structure independent of specific weight values. We reformulate the structure learning problem as minimizing Bayesian free energy and prove that this problem is equivalent to minimizing the f-divergence between the true data distribution and the model-generated distribution. Using the f-GAN framework, we transform this objective into a min-max adversarial optimization problem. We implement the gradient search in the discrete graph space using Gumbel-Softmax relaxation.

</details>


### [48] [Data Complexity-aware Deep Model Performance Forecasting](https://arxiv.org/abs/2601.01383)
*Yen-Chia Chen,Hsing-Kuo Pao,Hanjuan Huang*

Main category: cs.LG

TL;DR: 提出一个轻量级的两阶段框架，在训练前基于数据集特性和模型结构预测深度学习模型性能，用于指导架构选择和数据预处理。


<details>
  <summary>Details</summary>
Motivation: 传统通过试错选择深度学习架构的方法耗时耗资源且难以自动化，现有性能预测方法要么计算开销大，要么缺乏泛化能力。

Method: 两阶段框架：第一阶段通过分析数据集的可测量属性预测基线性能；第二阶段结合模型架构和超参数细节调整预测。框架设计轻量且能跨数据集和模型类型泛化。

Result: 框架不仅能预测模型性能，还能指导架构选择、告知必要的预处理步骤，并在训练开始前检测潜在有问题的数据集。数据集方差等特征可作为数据质量的早期指标。

Conclusion: 提出的轻量级两阶段框架为深度学习模型选择提供了高效替代方案，减少了试错需求，并能提供数据质量洞察，具有实际指导价值。

Abstract: Deep learning models are widely used across computer vision and other domains. When working on the model induction, selecting the right architecture for a given dataset often relies on repetitive trial-and-error procedures. This procedure is time-consuming, resource-intensive, and difficult to automate. While previous work has explored performance prediction using partial training or complex simulations, these methods often require significant computational overhead or lack generalizability. In this work, we propose an alternative approach: a lightweight, two-stage framework that can estimate model performance before training given the understanding of the dataset and the focused deep model structures. The first stage predicts a baseline based on the analysis of some measurable properties of the dataset, while the second stage adjusts the estimation with additional information on the model's architectural and hyperparameter details. The setup allows the framework to generalize across datasets and model types. Moreover, we find that some of the underlying features used for prediction - such as dataset variance - can offer practical guidance for model selection, and can serve as early indicators of data quality. As a result, the framework can be used not only to forecast model performance, but also to guide architecture choices, inform necessary preprocessing procedures, and detect potentially problematic datasets before training begins.

</details>


### [49] [Scale-Adaptive Power Flow Analysis with Local Topology Slicing and Multi-Task Graph Learning](https://arxiv.org/abs/2601.01387)
*Yongzhe Li,Lin Guan,Zihan Cai,Zuxian Lin,Jiyu Huang,Liukai Chen*

Main category: cs.LG

TL;DR: 该论文提出了一种尺度自适应多任务电力潮流分析框架，通过局部拓扑切片采样和多任务图学习模型，提升模型在不同系统规模下的适应性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 开发具有强大拓扑变化适应性的深度学习模型对于电力潮流分析具有重要实际意义。现有模型在可变系统规模下的性能有限，分支功率预测的鲁棒性有待提高。

Method: 提出SaMPFA框架：1）局部拓扑切片采样技术，从完整电网中提取不同尺度的子图；2）设计无参考多任务图学习模型，预测母线电压和分支功率而非相角；3）损失函数中加入额外项，鼓励模型学习相角差和功率传输的物理模式。

Result: 在IEEE 39节点系统和实际省级电网上的仿真表明，该模型在可变系统规模下具有优越的适应性和泛化能力，准确率分别提高了4.47%和36.82%。

Conclusion: SaMPFA框架通过局部拓扑切片采样和多任务图学习，显著提升了电力潮流分析模型对拓扑变化的适应性，避免了相角预测带来的误差放大风险，同时保持了与物理定律的一致性。

Abstract: Developing deep learning models with strong adaptability to topological variations is of great practical significance for power flow analysis. To enhance model performance under variable system scales and improve robustness in branch power prediction, this paper proposes a Scale-adaptive Multi-task Power Flow Analysis (SaMPFA) framework. SaMPFA introduces a Local Topology Slicing (LTS) sampling technique that extracts subgraphs of different scales from the complete power network to strengthen the model's cross-scale learning capability. Furthermore, a Reference-free Multi-task Graph Learning (RMGL) model is designed for robust power flow prediction. Unlike existing approaches, RMGL predicts bus voltages and branch powers instead of phase angles. This design not only avoids the risk of error amplification in branch power calculation but also guides the model to learn the physical relationships of phase angle differences. In addition, the loss function incorporates extra terms that encourage the model to capture the physical patterns of angle differences and power transmission, further improving consistency between predictions and physical laws. Simulations on the IEEE 39-bus system and a real provincial grid in China demonstrate that the proposed model achieves superior adaptability and generalization under variable system scales, with accuracy improvements of 4.47% and 36.82%, respectively.

</details>


### [50] [A Graph-based Framework for Online Time Series Anomaly Detection Using Model Ensemble](https://arxiv.org/abs/2601.01403)
*Zewei Yu,Jianqiu Xu,Caimin Li*

Main category: cs.LG

TL;DR: GDME是一个基于图的无监督在线时间序列异常检测框架，通过动态模型池和图结构进行模型集成，能够有效处理异构流数据并检测概念漂移。


<details>
  <summary>Details</summary>
Motivation: 工业系统中流数据量不断增加，在线异常检测变得至关重要。现有方法多为离线设计或难以有效处理异构流数据，需要能够适应快速变化数据模式的在线检测方法。

Method: 提出GDME框架：1）维护动态模型池，持续更新（修剪表现不佳模型并引入新模型）；2）使用动态图结构表示模型间关系；3）通过图上的社区检测选择适当的集成子集；4）利用图结构变化监测概念漂移，适应流数据演化。

Result: 在七个异构时间序列数据集上的实验表明，GDME优于现有在线异常检测方法，提升幅度高达24%。其集成策略相比单个模型和平均集成方法具有更优的检测性能，同时保持有竞争力的计算效率。

Conclusion: GDME通过动态图结构和模型集成策略，有效解决了在线时间序列异常检测中的异构流数据适应问题，能够检测概念漂移并持续优化检测性能，为工业系统在线监控提供了有效解决方案。

Abstract: With the increasing volume of streaming data in industrial systems, online anomaly detection has become a critical task. The diverse and rapidly evolving data patterns pose significant challenges for online anomaly detection. Many existing anomaly detection methods are designed for offline settings or have difficulty in handling heterogeneous streaming data effectively. This paper proposes GDME, an unsupervised graph-based framework for online time series anomaly detection using model ensemble. GDME maintains a dynamic model pool that is continuously updated by pruning underperforming models and introducing new ones. It utilizes a dynamic graph structure to represent relationships among models and employs community detection on the graph to select an appropriate subset for ensemble. The graph structure is also used to detect concept drift by monitoring structural changes, allowing the framework to adapt to evolving streaming data. Experiments on seven heterogeneous time series demonstrate that GDME outperforms existing online anomaly detection methods, achieving improvements of up to 24%. In addition, its ensemble strategy provides superior detection performance compared with both individual models and average ensembles, with competitive computational efficiency.

</details>


### [51] [Unveiling the Heart-Brain Connection: An Analysis of ECG in Cognitive Performance](https://arxiv.org/abs/2601.01424)
*Akshay Sasi,Malavika Pradeep,Nusaibah Farrukh,Rahul Venugopal,Elizabeth Sherly*

Main category: cs.LG

TL;DR: 研究验证ECG信号能否可靠反映认知负荷并作为EEG指标的替代方案，提出跨模态XGBoost框架将ECG特征映射到EEG认知空间，实现仅用ECG进行认知负荷推断。


<details>
  <summary>Details</summary>
Motivation: 虽然EEG是评估心理负荷的金标准，但其便携性有限限制了实际应用。通过可穿戴设备广泛可用的ECG提供了一个实用的替代方案，需要验证ECG信号能否可靠反映认知负荷。

Method: 收集工作记忆和被动听力任务的多模态数据，提取ECG时域HRV指标和Catch22描述符，对应EEG频谱和Catch22特征。提出跨模态XGBoost框架，将ECG特征投影到EEG代表的认知空间。

Result: ECG衍生的投影能够显著捕捉认知状态的变化，为准确分类提供良好支持。ECG作为可解释、实时、可穿戴的日常认知监测解决方案具有可行性。

Conclusion: ECG可以作为EEG的有效替代方案，为日常认知监测提供实用、可穿戴的解决方案，支持仅使用ECG进行认知负荷推断。

Abstract: Understanding the interaction of neural and cardiac systems during cognitive activity is critical to advancing physiological computing. Although EEG has been the gold standard for assessing mental workload, its limited portability restricts its real-world use. Widely available ECG through wearable devices proposes a pragmatic alternative. This research investigates whether ECG signals can reliably reflect cognitive load and serve as proxies for EEG-based indicators. In this work, we present multimodal data acquired from two different paradigms involving working-memory and passive-listening tasks. For each modality, we extracted ECG time-domain HRV metrics and Catch22 descriptors against EEG spectral and Catch22 features, respectively. We propose a cross-modal XGBoost framework to project the ECG features onto EEG-representative cognitive spaces, thereby allowing workload inferences using only ECG. Our results show that ECG-derived projections expressively capture variation in cognitive states and provide good support for accurate classification. Our findings underpin ECG as an interpretable, real-time, wearable solution for everyday cognitive monitoring.

</details>


### [52] [Bayesian Subspace Gradient Estimation for Zeroth-Order Optimization of Large Language Models](https://arxiv.org/abs/2601.01452)
*Jian Feng,Zhihong Huang*

Main category: cs.LG

TL;DR: BSZO是一种贝叶斯子空间零阶优化方法，通过卡尔曼滤波结合多个扰动方向的有限差分信息，相比传统ZO方法提高了收敛速度，在保持低内存消耗的同时显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有零阶优化方法依赖随机扰动的一步梯度估计，限制了优化效率和性能。需要一种能够有效利用多个扰动方向信息、提高收敛速度的零阶优化方法。

Method: 提出贝叶斯子空间零阶优化(BSZO)，将每个有限差分测量视为噪声观测，通过卡尔曼滤波构建投影梯度的后验分布，使用基于残差的自适应机制调整扰动尺度，结合多个扰动方向的信息进行贝叶斯推断。

Result: 理论分析显示BSZO比标准ZO方法的收敛速度提高了k/γ倍。在RoBERTa、Mistral和OPT模型上的实验表明，BSZO优于MeZO、MeZO-Adam和HiZOO，在OPT-13B上实现了高达6.67%的绝对平均改进，同时内存使用接近仅推理基线（MeZO的1.00-1.08倍）。

Conclusion: BSZO通过贝叶斯方法有效整合多个扰动方向的信息，显著提高了零阶优化的收敛速度和性能，同时保持了低内存消耗的优势，为大语言模型的微调提供了一种高效的内存友好优化方案。

Abstract: Fine-tuning large language models (LLMs) with zeroth-order (ZO) optimization reduces memory by approximating gradients through function evaluations, but existing methods rely on one-step gradient estimates from random perturbations. We introduce Bayesian Subspace Zeroth-Order optimization (BSZO), a ZO optimizer that applies Kalman filtering to combine finite-difference information across multiple perturbation directions. By treating each finite-difference measurement as a noisy observation, BSZO builds a posterior distribution over the projected gradient and updates it through Bayesian inference, with a residual-based adaptive mechanism to adjust perturbation scales. Theoretical analysis shows that BSZO improves the convergence rate by a factor of $k/γ$ compared to standard ZO methods. Experiments on RoBERTa, Mistral, and OPT models show that BSZO outperforms MeZO, MeZO-Adam, and HiZOO across various tasks, achieving up to 6.67\% absolute average improvement on OPT-13B while keeping memory usage close to inference-only baselines (1.00$\times$--1.08$\times$ of MeZO).

</details>


### [53] [Multi-Subspace Multi-Modal Modeling for Diffusion Models: Estimation, Convergence and Mixture of Experts](https://arxiv.org/abs/2601.01475)
*Ruofeng Yang,Yongcan Li,Bo Jiang,Cheng Chen,Shuai Li*

Main category: cs.LG

TL;DR: 本文提出MoLR-MoG建模方法，通过混合低秩高斯混合模型捕捉数据的多模态特性，解决了扩散模型在高维数据中的维度诅咒问题，实现了更好的生成效果和更快的优化收敛。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型虽然在小数据集上表现良好，但估计误差受维度诅咒影响（n^{-1/D}）。现有方法将数据建模为高斯隐变量的线性子空间联合，虽然反映了多流形特性，但无法捕捉隐流形的多模态特性。

Method: 提出混合低秩高斯混合模型（MoLR-MoG），将目标数据建模为K个线性子空间的联合，每个子空间采用高斯混合隐变量（n_k个模态，维度d_k）。对应的得分函数具有混合专家结构，能捕捉多模态信息并包含非线性特性。

Result: MoE-latent MoG NN的生成结果明显优于MoE-latent Gaussian score，且与参数多10倍的MoE-latent Unet性能相当。理论分析显示估计误差为R^4√(∑n_k)√(∑n_kd_k)/√n，避免了维度诅咒。优化过程证明在MoLR-MoG建模下有收敛保证。

Conclusion: MoLR-MoG建模能有效捕捉真实世界数据的多模态特性，解释了为什么扩散模型只需少量训练样本就能快速优化并获得优异性能，为扩散模型的理论理解提供了新视角。

Abstract: Recently, diffusion models have achieved a great performance with a small dataset of size $n$ and a fast optimization process. However, the estimation error of diffusion models suffers from the curse of dimensionality $n^{-1/D}$ with the data dimension $D$. Since images are usually a union of low-dimensional manifolds, current works model the data as a union of linear subspaces with Gaussian latent and achieve a $1/\sqrt{n}$ bound. Though this modeling reflects the multi-manifold property, the Gaussian latent can not capture the multi-modal property of the latent manifold. To bridge this gap, we propose the mixture subspace of low-rank mixture of Gaussian (MoLR-MoG) modeling, which models the target data as a union of $K$ linear subspaces, and each subspace admits a mixture of Gaussian latent ($n_k$ modals with dimension $d_k$). With this modeling, the corresponding score function naturally has a mixture of expert (MoE) structure, captures the multi-modal information, and contains nonlinear property. We first conduct real-world experiments to show that the generation results of MoE-latent MoG NN are much better than MoE-latent Gaussian score. Furthermore, MoE-latent MoG NN achieves a comparable performance with MoE-latent Unet with $10 \times$ parameters. These results indicate that the MoLR-MoG modeling is reasonable and suitable for real-world data. After that, based on such MoE-latent MoG score, we provide a $R^4\sqrt{Σ_{k=1}^Kn_k}\sqrt{Σ_{k=1}^Kn_kd_k}/\sqrt{n}$ estimation error, which escapes the curse of dimensionality by using data structure. Finally, we study the optimization process and prove the convergence guarantee under the MoLR-MoG modeling. Combined with these results, under a setting close to real-world data, this work explains why diffusion models only require a small training sample and enjoy a fast optimization process to achieve a great performance.

</details>


### [54] [Utilizing Earth Foundation Models to Enhance the Simulation Performance of Hydrological Models with AlphaEarth Embeddings](https://arxiv.org/abs/2601.01558)
*Pengfei Qu,Wenyu Ouyang,Chi Zhang,Yikai Chai,Shuolong Xu,Lei Ye,Yongri Piao,Miao Zhang,Huchuan Lu*

Main category: cs.LG

TL;DR: 卫星图像嵌入比传统流域属性更能有效预测无径流记录地区的河流流量，通过选择相似流域可提高预测精度


<details>
  <summary>Details</summary>
Motivation: 传统流域属性无法完全描述自然环境复杂性，需要更有效的方法来预测无径流记录地区的河流流量

Method: 使用AlphaEarth Foundation嵌入（从大规模卫星图像学习的环境表示）来描述流域特征，并研究如何选择适当的供体流域来预测无测站流域的流量

Result: 基于嵌入的模型在未用于训练的流域上预测精度更高，表明其比传统属性更能捕捉关键物理差异；基于嵌入相似性选择供体流域可提高预测性能

Conclusion: 卫星图像驱动的环境表示可以加强水文预测，支持开发更易适应不同景观的模型

Abstract: Predicting river flow in places without streamflow records is challenging because basins respond differently to climate, terrain, vegetation, and soils. Traditional basin attributes describe some of these differences, but they cannot fully represent the complexity of natural environments. This study examines whether AlphaEarth Foundation embeddings, which are learned from large collections of satellite images rather than designed by experts, offer a more informative way to describe basin characteristics. These embeddings summarize patterns in vegetation, land surface properties, and long-term environmental dynamics. We find that models using them achieve higher accuracy when predicting flows in basins not used for training, suggesting that they capture key physical differences more effectively than traditional attributes. We further investigate how selecting appropriate donor basins influences prediction in ungauged regions. Similarity based on the embeddings helps identify basins with comparable environmental and hydrological behavior, improving performance, whereas adding many dissimilar basins can reduce accuracy. The results show that satellite-informed environmental representations can strengthen hydrological forecasting and support the development of models that adapt more easily to different landscapes.

</details>


### [55] [REE-TTT: Highly Adaptive Radar Echo Extrapolation Based on Test-Time Training](https://arxiv.org/abs/2601.01605)
*Xin Di,Xinglin Piao,Fei Wang,Guodong Jing,Yong Zhang*

Main category: cs.LG

TL;DR: 提出REE-TTT模型，通过时空测试时训练机制改进雷达回波外推降水临近预报，解决传统方法泛化性差的问题


<details>
  <summary>Details</summary>
Motivation: 基于深度学习的雷达回波外推降水临近预报方法存在泛化性差的问题，依赖高质量本地训练数据和静态模型参数，难以适应不同地区和极端天气事件

Method: 提出REE-TTT模型，引入自适应测试时训练机制，设计时空测试时训练块，用任务特定注意力机制替代标准线性投影，增强对非平稳气象分布的适应能力

Result: 在跨区域极端降水场景实验中，REE-TTT在预测精度和泛化性方面显著优于最先进的基线模型，表现出对数据分布变化的出色适应性

Conclusion: REE-TTT通过测试时训练机制有效提升了降水临近预报的泛化能力，为应对不同地区和极端天气事件提供了更可靠的解决方案

Abstract: Precipitation nowcasting is critically important for meteorological forecasting. Deep learning-based Radar Echo Extrapolation (REE) has become a predominant nowcasting approach, yet it suffers from poor generalization due to its reliance on high-quality local training data and static model parameters, limiting its applicability across diverse regions and extreme events. To overcome this, we propose REE-TTT, a novel model that incorporates an adaptive Test-Time Training (TTT) mechanism. The core of our model lies in the newly designed Spatio-temporal Test-Time Training (ST-TTT) block, which replaces the standard linear projections in TTT layers with task-specific attention mechanisms, enabling robust adaptation to non-stationary meteorological distributions and thereby significantly enhancing the feature representation of precipitation. Experiments under cross-regional extreme precipitation scenarios demonstrate that REE-TTT substantially outperforms state-of-the-art baseline models in prediction accuracy and generalization, exhibiting remarkable adaptability to data distribution shifts.

</details>


### [56] [Real Time NILM Based Power Monitoring of Identical Induction Motors Representing Cutting Machines in Textile Industry](https://arxiv.org/abs/2601.01616)
*Md Istiauk Hossain Rifat,Moin Khan,Mohammad Zunaed*

Main category: cs.LG

TL;DR: 本文针对孟加拉国纺织行业能耗监控落后问题，提出基于非侵入式负载监控的实时工业监测框架，重点关注纺织切割机等相同电机负载，开发硬件系统并创建新数据集，评估了MATNILM模型在工业环境中的表现。


<details>
  <summary>Details</summary>
Motivation: 孟加拉国纺织行业是能源密集型产业，但现有的监控方法过时，导致能源使用效率低下和运营成本高昂。需要开发实时监控系统来改善能源管理。

Method: 开发了包含电压电流传感器、Arduino Mega和ESP8266的硬件系统，用于采集总负载和单个负载数据。创建了包含三个相同感应电机和辅助负载的新数据集（超过18万个样本）。使用MATNILM模型在具有挑战性的工业条件下进行评估。

Result: 总体能耗估计相对准确，但在多个相同机器同时运行时，单个设备的能耗分解面临困难。集成系统通过Blynk应用实现了实用的实时远程监控。

Conclusion: NILM技术在工业环境中既有潜力也有局限性。未来改进方向包括更高频率的数据采集、更大规模的数据集以及处理相同负载的先进深度学习方法。

Abstract: The textile industry in Bangladesh is one of the most energy-intensive sectors, yet its monitoring practices remain largely outdated, resulting in inefficient power usage and high operational costs. To address this, we propose a real-time Non-Intrusive Load Monitoring (NILM)-based framework tailored for industrial applications, with a focus on identical motor-driven loads representing textile cutting machines. A hardware setup comprising voltage and current sensors, Arduino Mega and ESP8266 was developed to capture aggregate and individual load data, which was stored and processed on cloud platforms. A new dataset was created from three identical induction motors and auxiliary loads, totaling over 180,000 samples, to evaluate the state-of-the-art MATNILM model under challenging industrial conditions. Results indicate that while aggregate energy estimation was reasonably accurate, per-appliance disaggregation faced difficulties, particularly when multiple identical machines operated simultaneously. Despite these challenges, the integrated system demonstrated practical real-time monitoring with remote accessibility through the Blynk application. This work highlights both the potential and limitations of NILM in industrial contexts, offering insights into future improvements such as higher-frequency data collection, larger-scale datasets and advanced deep learning approaches for handling identical loads.

</details>


### [57] [Communication-Efficient Federated AUC Maximization with Cyclic Client Participation](https://arxiv.org/abs/2601.01649)
*Umesh Vangapally,Wenhan Wu,Chen Chen,Zhishuai Guo*

Main category: cs.LG

TL;DR: 该论文针对联邦学习中客户端循环参与的场景，开发了通信高效的联邦AUC最大化算法，在平方替代损失下达到最优通信复杂度，在一般成对损失下也显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有联邦AUC最大化方法通常假设客户端完全可用，但实际联邦学习系统中客户端往往按照固定循环时间表参与训练。这种循环参与模式给非可分解的AUC目标带来了独特的优化挑战。

Method: 研究两种设置：1）使用平方替代损失的AUC最大化，将其重新表述为非凸-强凹极小极大优化问题，利用Polyak-Łojasiewicz条件；2）一般成对AUC损失。针对循环客户端参与场景开发通信高效算法。

Result: 在平方替代损失下达到通信复杂度$\widetilde{O}(1/ε^{1/2})$和迭代复杂度$\widetilde{O}(1/ε)$；在一般成对损失下达到通信复杂度$O(1/ε^3)$和迭代复杂度$O(1/ε^4)$，在PL条件下改进为$\widetilde{O}(1/ε^{1/2})$和$\widetilde{O}(1/ε)$。实验在图像分类、医学影像和欺诈检测任务中验证了方法的优越性。

Conclusion: 该论文成功解决了循环客户端参与下的联邦AUC最大化问题，提出了通信高效的算法，在理论和实验上都取得了显著进展，为实际联邦学习系统中的不平衡数据学习提供了有效解决方案。

Abstract: Federated AUC maximization is a powerful approach for learning from imbalanced data in federated learning (FL). However, existing methods typically assume full client availability, which is rarely practical. In real-world FL systems, clients often participate in a cyclic manner: joining training according to a fixed, repeating schedule. This setting poses unique optimization challenges for the non-decomposable AUC objective. This paper addresses these challenges by developing and analyzing communication-efficient algorithms for federated AUC maximization under cyclic client participation. We investigate two key settings: First, we study AUC maximization with a squared surrogate loss, which reformulates the problem as a nonconvex-strongly-concave minimax optimization. By leveraging the Polyak-Łojasiewicz (PL) condition, we establish a state-of-the-art communication complexity of $\widetilde{O}(1/ε^{1/2})$ and iteration complexity of $\widetilde{O}(1/ε)$. Second, we consider general pairwise AUC losses. We establish a communication complexity of $O(1/ε^3)$ and an iteration complexity of $O(1/ε^4)$. Further, under the PL condition, these bounds improve to communication complexity of $\widetilde{O}(1/ε^{1/2})$ and iteration complexity of $\widetilde{O}(1/ε)$. Extensive experiments on benchmark tasks in image classification, medical imaging, and fraud detection demonstrate the superior efficiency and effectiveness of our proposed methods.

</details>


### [58] [Length-Aware Adversarial Training for Variable-Length Trajectories: Digital Twins for Mall Shopper Paths](https://arxiv.org/abs/2601.01663)
*He Sun,Jiwoong Shin,Ravi Dhar*

Main category: cs.LG

TL;DR: 提出长度感知采样（LAS）方法，通过按长度分组轨迹来减少批次内长度异质性，改善生成模型对轨迹衍生统计量的分布匹配效果


<details>
  <summary>Details</summary>
Motivation: 标准小批量训练在轨迹长度高度异质时不稳定，这会降低轨迹衍生统计量的分布匹配效果

Method: 提出长度感知采样（LAS）策略：按轨迹长度分组，从单一长度桶中采样批次，减少批次内长度异质性；将LAS集成到带有辅助时间对齐损失的轨迹GAN中

Result: 在购物者轨迹数据集和多个公共序列数据集（GPS、教育、电子商务、电影）上，LAS一致改善了衍生变量分布的匹配效果，优于随机采样

Conclusion: LAS通过消除仅基于长度的捷径批评器并针对桶内差异，改善了分布匹配，为轨迹生成建模提供了简单有效的批次采样策略

Abstract: We study generative modeling of \emph{variable-length trajectories} -- sequences of visited locations/items with associated timestamps -- for downstream simulation and counterfactual analysis. A recurring practical issue is that standard mini-batch training can be unstable when trajectory lengths are highly heterogeneous, which in turn degrades \emph{distribution matching} for trajectory-derived statistics. We propose \textbf{length-aware sampling (LAS)}, a simple batching strategy that groups trajectories by length and samples batches from a single length bucket, reducing within-batch length heterogeneity (and making updates more consistent) without changing the model class. We integrate LAS into a conditional trajectory GAN with auxiliary time-alignment losses and provide (i) a distribution-level guarantee for derived variables under mild boundedness assumptions, and (ii) an IPM/Wasserstein mechanism explaining why LAS improves distribution matching by removing length-only shortcut critics and targeting within-bucket discrepancies. Empirically, LAS consistently improves matching of derived-variable distributions on a multi-mall dataset of shopper trajectories and on diverse public sequence datasets (GPS, education, e-commerce, and movies), outperforming random sampling across dataset-specific metrics.

</details>


### [59] [Who is the Winning Algorithm? Rank Aggregation for Comparative Studies](https://arxiv.org/abs/2601.01664)
*Amichai Painsky*

Main category: cs.LG

TL;DR: 提出新框架，利用算法在多个数据集上的完整排名信息（不只是胜场数）来估计各算法在未来未知数据集上的获胜概率。


<details>
  <summary>Details</summary>
Motivation: 传统方法只统计每个算法在基准数据集上的胜场数来预测未来表现，但完整的排名信息（第二名、第三名等）包含更多有价值的信息，如何有效利用这些信息来更准确地估计算法获胜概率是研究动机。

Method: 引入新的概念框架，基于m个算法在基准数据集上的完整排名信息来估计每个算法在未来未知数据集上的获胜概率。该方法充分利用了完整的排名分布信息，而不仅仅是胜场数。

Result: 提出的新框架在合成和真实世界示例中显著优于当前已知的方法，能够更准确地估计算法的获胜概率。

Conclusion: 利用算法在基准数据集上的完整排名信息（而不仅仅是胜场数）可以显著提高对未来算法获胜概率的估计准确性，提出的新框架为此提供了有效的解决方案。

Abstract: Consider a collection of m competing machine learning algorithms. Given their performance on a benchmark of datasets, we would like to identify the best performing algorithm. Specifically, which algorithm is most likely to ``win'' (rank highest) on a future, unseen dataset. The standard maximum likelihood approach suggests counting the number of wins per each algorithm. In this work, we argue that there is much more information in the complete rankings. That is, the number of times that each algorithm finished second, third and so forth. Yet, it is not entirely clear how to effectively utilize this information for our purpose. In this work we introduce a novel conceptual framework for estimating the win probability for each of the m algorithms, given their complete rankings over a benchmark of datasets. Our proposed framework significantly improves upon currently known methods in synthetic and real-world examples.

</details>


### [60] [Adversarial Instance Generation and Robust Training for Neural Combinatorial Optimization with Multiple Objectives](https://arxiv.org/abs/2601.01665)
*Wei Liu,Yaoxin Wu,Yingqian Zhang,Thomas Bäck,Yingjie Fan*

Main category: cs.LG

TL;DR: 提出一个面向多目标组合优化问题的强化学习求解器鲁棒性框架，包含偏好对抗攻击生成困难实例，以及基于对抗训练的防御策略提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习在多目标组合优化问题上表现出色，但其鲁棒性研究不足，特别是在多样化和复杂问题分布上的表现需要进一步探索。

Method: 提出统一的鲁棒性导向框架：1）基于偏好的对抗攻击方法生成暴露求解器弱点的困难实例；2）防御策略将难度感知的偏好选择集成到对抗训练中，减少对受限偏好区域的过拟合。

Result: 在多目标旅行商问题、多目标容量车辆路径问题和多目标背包问题上验证：攻击方法成功为不同求解器生成困难实例；防御方法显著增强了神经求解器的鲁棒性和泛化能力，在困难或分布外实例上表现优异。

Conclusion: 提出的鲁棒性框架有效提升了多目标组合优化问题中深度强化学习求解器的稳健性和泛化性能，为学习型求解器的可靠性提供了系统解决方案。

Abstract: Deep reinforcement learning (DRL) has shown great promise in addressing multi-objective combinatorial optimization problems (MOCOPs). Nevertheless, the robustness of these learning-based solvers has remained insufficiently explored, especially across diverse and complex problem distributions. In this paper, we propose a unified robustness-oriented framework for preference-conditioned DRL solvers for MOCOPs. Within this framework, we develop a preference-based adversarial attack to generate hard instances that expose solver weaknesses, and quantify the attack impact by the resulting degradation on Pareto-front quality. We further introduce a defense strategy that integrates hardness-aware preference selection into adversarial training to reduce overfitting to restricted preference regions and improve out-of-distribution performance. The experimental results on multi-objective traveling salesman problem (MOTSP), multi-objective capacitated vehicle routing problem (MOCVRP), and multi-objective knapsack problem (MOKP) verify that our attack method successfully learns hard instances for different solvers. Furthermore, our defense method significantly strengthens the robustness and generalizability of neural solvers, delivering superior performance on hard or out-of-distribution instances.

</details>


### [61] [DiMEx: Breaking the Cold Start Barrier in Data-Free Model Extraction via Latent Diffusion Priors](https://arxiv.org/abs/2601.01688)
*Yash Thesia,Meera Suthar*

Main category: cs.LG

TL;DR: DiMEx利用预训练扩散模型的语义先验，通过潜在空间贝叶斯优化绕过模型窃取中的"冷启动"问题，显著提升攻击效率；同时提出HSE防御机制，通过检测优化轨迹来有效对抗此类攻击。


<details>
  <summary>Details</summary>
Motivation: 针对机器学习即服务中的模型窃取攻击，特别是数据自由模型提取面临的"冷启动"问题——基于GAN的攻击需要大量查询从随机噪声收敛到有意义数据，效率低下。

Method: 提出DiMEx框架：利用预训练潜在扩散模型的丰富语义先验，在生成器潜在空间中使用随机嵌入贝叶斯优化，立即合成高保真查询，绕过初始化障碍。同时提出HSE防御：混合状态集成防御，通过识别潜在空间攻击的独特"优化轨迹"来检测攻击。

Result: DiMEx在SVHN数据集上仅用2000次查询就达到52.1%的协议率，比最先进的GAN基线高出16%以上。HSE防御能够将攻击成功率抑制到21.6%，且延迟可忽略不计，同时能够规避静态分布检测器。

Conclusion: DiMEx展示了利用预训练扩散模型语义先验进行高效模型窃取的可行性，而HSE防御则提供了针对此类语义攻击的有效对抗手段，强调了在MLaaS安全中考虑攻击优化轨迹的重要性。

Abstract: Model stealing attacks pose an existential threat to Machine Learning as a Service (MLaaS), allowing adversaries to replicate proprietary models for a fraction of their training cost. While Data-Free Model Extraction (DFME) has emerged as a stealthy vector, it remains fundamentally constrained by the "Cold Start" problem: GAN-based adversaries waste thousands of queries converging from random noise to meaningful data. We propose DiMEx, a framework that weaponizes the rich semantic priors of pre-trained Latent Diffusion Models to bypass this initialization barrier entirely. By employing Random Embedding Bayesian Optimization (REMBO) within the generator's latent space, DiMEx synthesizes high-fidelity queries immediately, achieving 52.1 percent agreement on SVHN with just 2,000 queries - outperforming state-of-the-art GAN baselines by over 16 percent. To counter this highly semantic threat, we introduce the Hybrid Stateful Ensemble (HSE) defense, which identifies the unique "optimization trajectory" of latent-space attacks. Our results demonstrate that while DiMEx evades static distribution detectors, HSE exploits this temporal signature to suppress attack success rates to 21.6 percent with negligible latency.

</details>


### [62] [Enhanced Multi-model Online Conformal Prediction](https://arxiv.org/abs/2601.01692)
*Erfan Hajihashemi,Yanning Shen*

Main category: cs.LG

TL;DR: 提出了一种新颖的多模型在线共形预测算法，通过生成二分图筛选有效模型子集，降低计算复杂度并提升预测效率


<details>
  <summary>Details</summary>
Motivation: 传统共形预测依赖单一固定模型，在在线环境中可能表现不佳；现有多模型方法计算成本高且可能受低效模型影响

Method: 在每个时间步生成二分图来识别有效模型子集，从中选择模型构建预测集，降低计算复杂度

Result: 实验表明该方法在预测集大小和计算效率方面均优于现有多模型共形预测技术

Conclusion: 提出的算法通过有效模型筛选机制，成功解决了多模型共形预测中的计算效率和性能平衡问题

Abstract: Conformal prediction is a framework for uncertainty quantification that constructs prediction sets for previously unseen data, guaranteeing coverage of the true label with a specified probability. However, the efficiency of these prediction sets, measured by their size, depends on the choice of the underlying learning model. Relying on a single fixed model may lead to suboptimal performance in online environments, as a single model may not consistently perform well across all time steps. To mitigate this, prior work has explored selecting a model from a set of candidates. However, this approach becomes computationally expensive as the number of candidate models increases. Moreover, poorly performing models in the set may also hinder the effectiveness. To tackle this challenge, this work develops a novel multi-model online conformal prediction algorithm that reduces computational complexity and improves prediction efficiency. At each time step, a bipartite graph is generated to identify a subset of effective models, from which a model is selected to construct the prediction set. Experiments demonstrate that our method outperforms existing multi-model conformal prediction techniques in terms of both prediction set size and computational efficiency.

</details>


### [63] [Entropy-Aligned Decoding of LMs for Better Writing and Reasoning](https://arxiv.org/abs/2601.01714)
*Kareem Ahmed,Sameer Singh*

Main category: cs.LG

TL;DR: EPIC是一种超参数自由的解码方法，通过将未来轨迹的熵纳入语言模型解码，在每一步生成中显式调节不确定性，使采样分布的熵与数据不确定性对齐。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型解码算法依赖贪婪启发式方法，导致短视扭曲，产生同质化、重复和不连贯的句子。需要一种能够更好调节生成不确定性的解码方法。

Method: EPIC通过熵感知懒惰Gumbel-Max采样，将未来轨迹的熵纳入解码过程，显式调节每一步生成的不确定性，使采样分布的熵与数据不确定性对齐，且仅需亚线性数量的熵评估。

Result: 在创意写作和摘要任务中，EPIC在LM-as-judge偏好胜率上持续优于广泛使用的解码策略；自动指标显示EPIC产生更多样化的生成和更忠实的摘要；在数学推理任务中，EPIC也优于所有基线方法。

Conclusion: EPIC是一种有效且高效的语言模型解码方法，能够产生更高质量、更多样化和更连贯的文本生成，在多个任务上优于现有解码策略。

Abstract: Language models (LMs) are trained on billions of tokens in an attempt to recover the true language distribution. Still, vanilla random sampling from LMs yields low quality generations. Decoding algorithms attempt to restrict the LM distribution to a set of high-probability continuations, but rely on greedy heuristics that introduce myopic distortions, yielding sentences that are homogeneous, repetitive and incoherent. In this paper, we introduce EPIC, a hyperparameter-free decoding approach that incorporates the entropy of future trajectories into LM decoding. EPIC explicitly regulates the amount of uncertainty expressed at every step of generation, aligning the sampling distribution's entropy to the aleatoric (data) uncertainty. Through Entropy-Aware Lazy Gumbel-Max sampling, EPIC manages to be exact, while also being efficient, requiring only a sublinear number of entropy evaluations per step. Unlike current baselines, EPIC yields sampling distributions that are empirically well-aligned with the entropy of the underlying data distribution. Across creative writing and summarization tasks, EPIC consistently improves LM-as-judge preference win-rates over widely used decoding strategies. These preference gains are complemented by automatic metrics, showing that EPIC produces more diverse generations and more faithful summaries. We also evaluate EPIC on mathematical reasoning, where it outperforms all baselines.

</details>


### [64] [Context-Free Recognition with Transformers](https://arxiv.org/abs/2601.01754)
*Selim Jerad,Anej Svete,Sophie Hao,Ryan Cotterell,William Merrill*

Main category: cs.LG

TL;DR: 循环Transformer通过O(log n)循环层和O(n^6)填充token可以识别所有上下文无关语言，但对于自然子类如无歧义CFL，仅需O(n^3)填充token，实际更可行。


<details>
  <summary>Details</summary>
Motivation: Transformer在处理具有语法结构的输入（如自然语言和代码）方面表现出色，但尚不清楚它们如何真正处理语法结构。现有研究表明标准Transformer无法识别上下文无关语言（CFL），而循环层能帮助识别正则语言，但CFL识别问题仍未解决。

Method: 提出循环Transformer架构，使用O(log n)循环层和O(n^6)填充token来识别所有CFL。针对无歧义CFL等自然子类，将填充token需求降低到O(n^3)。通过实验验证循环机制在需要对数深度的语言上的有效性。

Result: 理论证明循环Transformer可以识别所有CFL，但需要大量填充token。对于无歧义CFL等自然子类，识别变得实际可行。实验证实循环机制确实能帮助处理需要对数深度的语言。

Conclusion: CFL识别对Transformer具有内在复杂性：一般识别可能需要不切实际的填充token，但自然约束（如无歧义性）能产生高效的识别算法。循环机制为Transformer处理语法结构提供了理论保证。

Abstract: Transformers excel on tasks that process well-formed inputs according to some grammar, such as natural language and code. However, it remains unclear how they can process grammatical syntax. In fact, under standard complexity conjectures, standard transformers cannot recognize context-free languages (CFLs), a canonical formalism to describe syntax, or even regular languages, a subclass of CFLs (Merrill et al., 2022). Merrill & Sabharwal (2024) show that $\mathcal{O}(\log n)$ looping layers (w.r.t. input length $n$) allows transformers to recognize regular languages, but the question of context-free recognition remained open. In this work, we show that looped transformers with $\mathcal{O}(\log n)$ looping layers and $\mathcal{O}(n^6)$ padding tokens can recognize all CFLs. However, training and inference with $\mathcal{O}(n^6)$ padding tokens is potentially impractical. Fortunately, we show that, for natural subclasses such as unambiguous CFLs, the recognition problem on transformers becomes more tractable, requiring $\mathcal{O}(n^3)$ padding. We empirically validate our results and show that looping helps on a language that provably requires logarithmic depth. Overall, our results shed light on the intricacy of CFL recognition by transformers: While general recognition may require an intractable amount of padding, natural constraints such as unambiguity yield efficient recognition algorithms.

</details>


### [65] [HyperCLOVA X 8B Omni](https://arxiv.org/abs/2601.01792)
*NAVER Cloud HyperCLOVA X Team*

Main category: cs.LG

TL;DR: HyperCLOVA X 8B Omni是首个支持文本、音频、视觉作为输入和输出的任意到任意全模态模型，通过统一的多模态序列预测实现8B规模的通用助手。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统多模态模型需要独立处理不同模态的问题，开发一个能够统一处理文本、音频和视觉的任意到任意全模态模型，实现更实用的通用助手。

Method: 通过共享的下一个标记预测接口统一多模态序列，使用视觉和音频编码器注入连续嵌入以实现细粒度理解和基础，将多模态理解和生成整合到单一模型中。

Result: 在韩语和英语的文本、音频、视觉多种输入输出组合上，与类似规模模型相比表现出有竞争力的性能。

Conclusion: HyperCLOVA X 8B Omni作为8B规模的全模态路径探索点，其开源权重将支持广泛的研究和部署场景，推动任意到任意全模态助手的发展。

Abstract: In this report, we present HyperCLOVA X 8B Omni, the first any-to-any omnimodal model in the HyperCLOVA X family that supports text, audio, and vision as both inputs and outputs. By consolidating multimodal understanding and generation into a single model rather than separate modality-specific pipelines, HyperCLOVA X 8B Omni serves as an 8B-scale omni-pathfinding point toward practical any-to-any omni assistants. At a high level, the model unifies modalities through a shared next-token prediction interface over an interleaved multimodal sequence, while vision and audio encoders inject continuous embeddings for fine-grained understanding and grounding. Empirical evaluations demonstrate competitive performance against comparably sized models across diverse input-output combinations spanning text, audio, and vision, in both Korean and English. We anticipate that the open-weight release of HyperCLOVA X 8B Omni will support a wide range of research and deployment scenarios.

</details>


### [66] [Distributed Federated Learning by Alternating Periods of Training](https://arxiv.org/abs/2601.01793)
*Shamik Bhattacharyya,Rachel Kalpana Kalaimani*

Main category: cs.LG

TL;DR: 本文提出了一种分布式联邦学习框架，通过多服务器架构解决传统联邦学习中单点故障和可扩展性问题，设计了DFL算法实现服务器间的协同训练。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习依赖单一中央服务器，在面对大量客户端时存在可扩展性挑战，且存在单点故障风险。为解决这些关键限制，需要设计分布式方法。

Method: 提出分布式联邦学习框架，包含多个具有服务器间通信能力的服务器，每个服务器关联一组不相交的客户端。设计了DFL算法，采用客户端本地训练和服务器间全局训练交替进行的模式。

Result: 在适当参数选择下，DFL算法确保所有服务器收敛到接近理想模型的共同模型值，有效整合了本地和全局训练模型。通过数值模拟验证了理论主张。

Conclusion: 分布式联邦学习框架通过多服务器架构解决了传统联邦学习的可扩展性和容错性限制，DFL算法实现了服务器间的有效协同，为大规模联邦学习应用提供了可行方案。

Abstract: Federated learning is a privacy-focused approach towards machine learning where models are trained on client devices with locally available data and aggregated at a central server. However, the dependence on a single central server is challenging in the case of a large number of clients and even poses the risk of a single point of failure. To address these critical limitations of scalability and fault-tolerance, we present a distributed approach to federated learning comprising multiple servers with inter-server communication capabilities. While providing a fully decentralized approach, the designed framework retains the core federated learning structure where each server is associated with a disjoint set of clients with server-client communication capabilities. We propose a novel DFL (Distributed Federated Learning) algorithm which uses alternating periods of local training on the client data followed by global training among servers. We show that the DFL algorithm, under a suitable choice of parameters, ensures that all the servers converge to a common model value within a small tolerance of the ideal model, thus exhibiting effective integration of local and global training models. Finally, we illustrate our theoretical claims through numerical simulations.

</details>


### [67] [RealPDEBench: A Benchmark for Complex Physical Systems with Real-World Data](https://arxiv.org/abs/2601.01829)
*Peiyan Hu,Haodong Feng,Hongyuan Liu,Tongtong Yan,Wenhao Deng,Tianrun Gao,Rong Zheng,Haoren Zheng,Chenglei Yu,Chuanrui Wang,Kaiwen Li,Zhi-Ming Ma,Dezhi Zhou,Xingcai Lu,Dixia Fan,Tailin Wu*

Main category: cs.LG

TL;DR: RealPDEBench是首个将真实世界测量数据与配对数值模拟相结合的科学机器学习基准，包含5个数据集、3个任务、8个指标和10个基线模型，旨在解决科学ML中真实数据缺乏的问题并促进sim-to-real迁移研究。


<details>
  <summary>Details</summary>
Motivation: 科学机器学习面临的关键瓶颈是缺乏昂贵的真实世界数据，导致大多数模型只能在模拟数据上训练和验证，这不仅限制了科学ML的发展评估，也阻碍了sim-to-real迁移等关键任务的研究。

Method: 构建了RealPDEBench基准，包含：1）5个真实世界测量数据集及其配对的模拟数据集；2）3个任务设计，用于比较真实与模拟数据并开发桥接方法；3）8个评估指标，涵盖数据导向和物理导向指标；4）10个代表性基线模型，包括SOTA模型、预训练PDE基础模型和传统方法。

Result: 实验显示模拟数据与真实世界数据存在显著差异，同时表明使用模拟数据进行预训练能持续提高准确性和收敛性，为科学ML桥接sim-to-real差距提供了重要见解。

Conclusion: RealPDEBench通过整合真实世界测量数据与配对模拟数据，为科学机器学习提供了首个综合性基准，有望推动科学ML向桥接sim-to-real差距和实际部署方向发展。

Abstract: Predicting the evolution of complex physical systems remains a central problem in science and engineering. Despite rapid progress in scientific Machine Learning (ML) models, a critical bottleneck is the lack of expensive real-world data, resulting in most current models being trained and validated on simulated data. Beyond limiting the development and evaluation of scientific ML, this gap also hinders research into essential tasks such as sim-to-real transfer. We introduce RealPDEBench, the first benchmark for scientific ML that integrates real-world measurements with paired numerical simulations. RealPDEBench consists of five datasets, three tasks, eight metrics, and ten baselines. We first present five real-world measured datasets with paired simulated datasets across different complex physical systems. We further define three tasks, which allow comparisons between real-world and simulated data, and facilitate the development of methods to bridge the two. Moreover, we design eight evaluation metrics, spanning data-oriented and physics-oriented metrics, and finally benchmark ten representative baselines, including state-of-the-art models, pretrained PDE foundation models, and a traditional method. Experiments reveal significant discrepancies between simulated and real-world data, while showing that pretraining with simulated data consistently improves both accuracy and convergence. In this work, we hope to provide insights from real-world data, advancing scientific ML toward bridging the sim-to-real gap and real-world deployment. Our benchmark, datasets, and instructions are available at https://realpdebench.github.io/.

</details>


### [68] [FAROS: Robust Federated Learning with Adaptive Scaling against Backdoor Attacks](https://arxiv.org/abs/2601.01833)
*Chenyu Hu,Qiming Hu,Sinan Chen,Nianyu Li,Mingyue Zhang,Jialong Li*

Main category: cs.LG

TL;DR: FAROS：一种增强的联邦学习框架，通过自适应差分缩放和鲁棒核心集计算来防御后门攻击，在保持主任务精度的同时显著降低攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 联邦学习面临后门攻击的严重威胁，现有防御方法依赖固定参数，存在单点故障风险，难以应对复杂攻击者。需要一种更鲁棒的防御机制来动态适应攻击策略变化。

Method: 提出FAROS框架，包含两个核心组件：1）自适应差分缩放（ADS）机制，根据客户端上传梯度的离散度动态调整防御灵敏度；2）鲁棒核心集计算（RCC），通过计算高置信度客户端核心集的质心来缓解单点故障风险。

Result: 在多种数据集、模型和攻击场景下的实验表明，该方法在攻击成功率和主任务精度方面均优于现有防御方法。

Conclusion: FAROS通过动态自适应防御机制有效应对联邦学习中的后门攻击，解决了现有防御方法依赖固定参数和单点故障的问题，为联邦学习安全提供了更鲁棒的解决方案。

Abstract: Federated Learning (FL) enables multiple clients to collaboratively train a shared model without exposing local data. However, backdoor attacks pose a significant threat to FL. These attacks aim to implant a stealthy trigger into the global model, causing it to mislead on inputs that possess a specific trigger while functioning normally on benign data. Although pre-aggregation detection is a main defense direction, existing state-of-the-art defenses often rely on fixed defense parameters. This reliance makes them vulnerable to single-point-of-failure risks, rendering them less effective against sophisticated attackers. To address these limitations, we propose FAROS, an enhanced FL framework that incorporates Adaptive Differential Scaling (ADS) and Robust Core-set Computing (RCC). The ADS mechanism adjusts the defense's sensitivity dynamically, based on the dispersion of uploaded gradients by clients in each round. This allows it to counter attackers who strategically shift between stealthiness and effectiveness. Furthermore, the RCC effectively mitigates the risk of single-point failure by computing the centroid of a core set comprising clients with the highest confidence. We conducted extensive experiments across various datasets, models, and attack scenarios. The results demonstrate that our method outperforms current defenses in both attack success rate and main task accuracy.

</details>


### [69] [Safety at One Shot: Patching Fine-Tuned LLMs with A Single Instance](https://arxiv.org/abs/2601.01887)
*Jiawen Zhang,Lipeng He,Kejia Chen,Jian Lou,Jian Liu,Xiaohu Yang,Ruoxi Jia*

Main category: cs.LG

TL;DR: 只需单个安全样本即可完全恢复安全对齐大语言模型，无需牺牲实用性，成本极低


<details>
  <summary>Details</summary>
Motivation: 微调安全对齐的大语言模型会显著损害其安全性，现有方法需要大量安全样本或校准集，计算开销大且导致模型实用性下降

Method: 发现仅需单个安全示例即可恢复安全对齐，无论有害示例数量或模型大小，仅需几个epoch即可收敛，并揭示了安全梯度的低秩结构

Result: 在五个安全对齐LLM和多个数据集上验证有效，证明了方法的通用性

Conclusion: 安全对齐可以通过极低成本高效恢复，这归因于安全梯度的低秩特性，为LLM安全调整提供了新思路

Abstract: Fine-tuning safety-aligned large language models (LLMs) can substantially compromise their safety. Previous approaches require many safety samples or calibration sets, which not only incur significant computational overhead during realignment but also lead to noticeable degradation in model utility. Contrary to this belief, we show that safety alignment can be fully recovered with only a single safety example, without sacrificing utility and at minimal cost. Remarkably, this recovery is effective regardless of the number of harmful examples used in fine-tuning or the size of the underlying model, and convergence is achieved within just a few epochs. Furthermore, we uncover the low-rank structure of the safety gradient, which explains why such efficient correction is possible. We validate our findings across five safety-aligned LLMs and multiple datasets, demonstrating the generality of our approach.

</details>


### [70] [Tackling Resource-Constrained and Data-Heterogeneity in Federated Learning with Double-Weight Sparse Pack](https://arxiv.org/abs/2601.01840)
*Qiantao Yang,Liquan Chen,Mingfu Xue,Songze Li*

Main category: cs.LG

TL;DR: FedCSPACK：一种基于余弦稀疏化参数打包和双重加权聚合的个性化联邦学习方法，旨在解决数据异构性和客户端资源限制的平衡问题


<details>
  <summary>Details</summary>
Motivation: 联邦学习中边缘客户端的数据异构性严重影响模型性能，现有方法虽然通过模型分割和知识蒸馏来增强模型兼容性，但忽略了客户端通信带宽和计算能力的限制，未能有效平衡数据异构性处理和有限客户端资源之间的矛盾

Method: 提出FedCSPACK方法：1）客户端通过余弦相似度对模型参数进行打包，选择贡献最大的参数包进行共享，减少带宽需求；2）基于共享参数包生成掩码矩阵，提高稀疏更新在服务器上的对齐和聚合效率；3）在掩码中嵌入方向和分布距离权重，实现加权引导聚合机制

Result: 在四个数据集上使用十种最先进方法的广泛实验表明，FedCSPACK在保持高模型精度的同时，有效提高了通信和计算效率

Conclusion: FedCSPACK通过参数打包和双重加权聚合机制，成功平衡了数据异构性处理和客户端资源限制之间的矛盾，为资源受限环境下的联邦学习提供了有效的解决方案

Abstract: Federated learning has drawn widespread interest from researchers, yet the data heterogeneity across edge clients remains a key challenge, often degrading model performance. Existing methods enhance model compatibility with data heterogeneity by splitting models and knowledge distillation. However, they neglect the insufficient communication bandwidth and computing power on the client, failing to strike an effective balance between addressing data heterogeneity and accommodating limited client resources. To tackle this limitation, we propose a personalized federated learning method based on cosine sparsification parameter packing and dual-weighted aggregation (FedCSPACK), which effectively leverages the limited client resources and reduces the impact of data heterogeneity on model performance. In FedCSPACK, the client packages model parameters and selects the most contributing parameter packages for sharing based on cosine similarity, effectively reducing bandwidth requirements. The client then generates a mask matrix anchored to the shared parameter package to improve the alignment and aggregation efficiency of sparse updates on the server. Furthermore, directional and distribution distance weights are embedded in the mask to implement a weighted-guided aggregation mechanism, enhancing the robustness and generalization performance of the global model. Extensive experiments across four datasets using ten state-of-the-art methods demonstrate that FedCSPACK effectively improves communication and computational efficiency while maintaining high model accuracy.

</details>


### [71] [Evaluating Feature Dependent Noise in Preference-based Reinforcement Learning](https://arxiv.org/abs/2601.01904)
*Yuxuan Li,Harshith Reddy Kethireddy,Srijita Das*

Main category: cs.LG

TL;DR: 该研究探讨了强化学习偏好学习中的特征依赖噪声问题，发现现有噪声鲁棒方法在某些特征依赖噪声场景下表现不佳，而无需显式去噪的方法反而表现更好。


<details>
  <summary>Details</summary>
Motivation: 强化学习偏好学习（PbRL）在奖励函数不易获取的复杂任务中具有优势，但实际偏好数据常包含不确定性和噪声。现有研究大多假设噪声均匀分布且与观察无关，而实际噪声往往与任务特征相关，这种特征依赖噪声对现有方法的鲁棒性提出了挑战。

Method: 研究形式化了特征依赖噪声的概念，提出了几种变体：轨迹特征噪声、轨迹相似性噪声、不确定性感知噪声和语言模型噪声。在DMControl和Meta-world的复杂连续控制任务中评估特征依赖噪声的影响，比较了噪声鲁棒PbRL方法与无需显式去噪的PbRL方法。

Result: 实验表明，在某些特征依赖噪声设置下，最先进的噪声鲁棒PbRL方法的学习性能显著下降，而无需显式去噪的PbRL方法在多数设置中反而表现更好。语言模型产生的噪声表现出与特征依赖噪声相似的特征。

Conclusion: 特征依赖噪声对现有噪声鲁棒PbRL方法构成挑战，语言模型噪声模拟了真实人类的噪声特征。研究呼吁进一步开发能够鲁棒处理特征依赖噪声的偏好学习方法。

Abstract: Learning from Preferences in Reinforcement Learning (PbRL) has gained attention recently, as it serves as a natural fit for complicated tasks where the reward function is not easily available. However, preferences often come with uncertainty and noise if they are not from perfect teachers. Much prior literature aimed to detect noise, but with limited types of noise and most being uniformly distributed with no connection to observations. In this work, we formalize the notion of targeted feature-dependent noise and propose several variants like trajectory feature noise, trajectory similarity noise, uncertainty-aware noise, and Language Model noise.
  We evaluate feature-dependent noise, where noise is correlated with certain features in complex continuous control tasks from DMControl and Meta-world. Our experiments show that in some feature-dependent noise settings, the state-of-the-art noise-robust PbRL method's learning performance is significantly deteriorated, while PbRL method with no explicit denoising can surprisingly outperform noise-robust PbRL in majority settings.
  We also find language model's noise exhibits similar characteristics to feature-dependent noise, thereby simulating realistic humans and call for further study in learning with feature-dependent noise robustly.

</details>


### [72] [High-Order Epistasis Detection Using Factorization Machine with Quadratic Optimization Annealing and MDR-Based Evaluation](https://arxiv.org/abs/2601.01860)
*Shuta Kikuchi,Shu Tanaka*

Main category: cs.LG

TL;DR: 提出基于因子分解机和二次优化退火的FMQA方法，用于高效检测高阶上位性相互作用，解决了传统MDR方法计算复杂度爆炸的问题。


<details>
  <summary>Details</summary>
Motivation: 检测高阶上位性是遗传关联研究中的基本挑战，传统多因子降维方法在候选位点组合数量爆炸时计算不可行，需要更高效的方法。

Method: 将上位性检测问题定义为黑盒优化问题，使用因子分解机结合二次优化退火方法，以MDR计算的分类错误率作为黑盒目标函数。

Result: 在模拟病例对照数据集上的实验表明，该方法能在有限迭代次数内成功识别各种交互阶数和遗传位点数量的真实上位性相互作用。

Conclusion: 提出的FMQA方法对于高阶上位性检测既有效又计算高效，为解决遗传关联研究中的组合爆炸问题提供了可行方案。

Abstract: Detecting high-order epistasis is a fundamental challenge in genetic association studies due to the combinatorial explosion of candidate locus combinations. Although multifactor dimensionality reduction (MDR) is a widely used method for evaluating epistasis, exhaustive MDR-based searches become computationally infeasible as the number of loci or the interaction order increases. In this paper, we define the epistasis detection problem as a black-box optimization problem and solve it with a factorization machine with quadratic optimization annealing (FMQA). We propose an efficient epistasis detection method based on FMQA, in which the classification error rate (CER) computed by MDR is used as a black-box objective function. Experimental evaluations were conducted using simulated case-control datasets with predefined high-order epistasis. The results demonstrate that the proposed method successfully identified ground-truth epistasis across various interaction orders and the numbers of genetic loci within a limited number of iterations. These results indicate that the proposed method is effective and computationally efficient for high-order epistasis detection.

</details>


### [73] [DéjàQ: Open-Ended Evolution of Diverse, Learnable and Verifiable Problems](https://arxiv.org/abs/2601.01931)
*Willem Röpke,Samuel Coward,Andrei Lupu,Thomas Foster,Tim Rocktäschel,Jakob Foerster*

Main category: cs.LG

TL;DR: DéjàQ框架通过进化合成数学问题与模型训练同步进行，使用LLM驱动的突变策略让模型自身生成训练数据，提升数学推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前推理模型在数学和编码方面取得了显著成果，但大多依赖静态数据集，这可能导致记忆而非泛化。需要一种动态适应模型能力的数据生成方法。

Method: 提出DéjàQ框架，在模型训练过程中同步进化多样化的合成数学问题集。采用两种LLM驱动的突变策略：1) 改变上下文细节；2) 直接修改问题结构。模型自身参与生成训练数据。

Result: 模型能够生成新颖且有意义的数学问题，LLM驱动的突变策略改善了强化学习训练效果。分析了生成问题的有效性和计算开销，证明了动态演化训练数据的潜力。

Conclusion: 动态演化训练数据能有效提升数学推理能力，该方法具有更广泛的适用性，作者将开源代码支持进一步研究。

Abstract: Recent advances in reasoning models have yielded impressive results in mathematics and coding. However, most approaches rely on static datasets, which have been suggested to encourage memorisation and limit generalisation. We introduce DéjàQ, a framework that departs from this paradigm by jointly evolving a diverse set of synthetic mathematical problems alongside model training. This evolutionary process adapts to the model's ability throughout training, optimising problems for learnability. We propose two LLM-driven mutation strategies in which the model itself mutates the training data, either by altering contextual details or by directly modifying problem structure. We find that the model can generate novel and meaningful problems, and that these LLM-driven mutations improve RL training. We analyse key aspects of DéjàQ, including the validity of generated problems and computational overhead. Our results underscore the potential of dynamically evolving training data to enhance mathematical reasoning and indicate broader applicability, which we will support by open-sourcing our code.

</details>


### [74] [FedBiCross: A Bi-Level Optimization Framework to Tackle Non-IID Challenges in Data-Free One-Shot Federated Learning on Medical Data](https://arxiv.org/abs/2601.01901)
*Yuexuan Xia,Yinghao Zhang,Yalin Liu,Hong-Ning Dai,Yong Xia*

Main category: cs.LG

TL;DR: FedBiCross提出了一种个性化的单次联邦学习框架，通过聚类、双层跨集群优化和个性化蒸馏来解决非独立同分布数据下预测冲突导致监督信号弱化的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的单次联邦学习方法在非独立同分布数据下，将所有客户端的预测聚合形成全局教师模型时，冲突的预测会在平均过程中相互抵消，产生接近均匀分布的软标签，从而提供较弱的蒸馏监督信号。

Method: FedBiCross包含三个阶段：1) 基于模型输出相似性对客户端进行聚类，形成一致的子集成；2) 双层跨集群优化，学习自适应权重以选择性地利用有益的跨集群知识，同时抑制负迁移；3) 个性化蒸馏进行客户端特定适应。

Result: 在四个医学图像数据集上的实验表明，FedBiCross在不同程度的非独立同分布设置下，始终优于最先进的基线方法。

Conclusion: FedBiCross通过聚类和选择性知识转移机制，有效解决了单次联邦学习在非独立同分布数据下的性能瓶颈，为隐私敏感的医学应用提供了更有效的解决方案。

Abstract: Data-free knowledge distillation-based one-shot federated learning (OSFL) trains a model in a single communication round without sharing raw data, making OSFL attractive for privacy-sensitive medical applications. However, existing methods aggregate predictions from all clients to form a global teacher. Under non-IID data, conflicting predictions cancel out during averaging, yielding near-uniform soft labels that provide weak supervision for distillation. We propose FedBiCross, a personalized OSFL framework with three stages: (1) clustering clients by model output similarity to form coherent sub-ensembles, (2) bi-level cross-cluster optimization that learns adaptive weights to selectively leverage beneficial cross-cluster knowledge while suppressing negative transfer, and (3) personalized distillation for client-specific adaptation. Experiments on four medical image datasets demonstrate that FedBiCross consistently outperforms state-of-the-art baselines across different non-IID degrees.

</details>


### [75] [Refinement Provenance Inference: Detecting LLM-Refined Training Prompts from Model Behavior](https://arxiv.org/abs/2601.01966)
*Bo Yin,Qi Li,Runpeng Yu,Xinchao Wang*

Main category: cs.LG

TL;DR: 该论文提出RePro框架，用于推断指令调优数据集中哪些训练样本使用了LLM精炼过的提示词，解决数据集治理和争议问题。


<details>
  <summary>Details</summary>
Motivation: 指令调优越来越多地依赖LLM进行提示词精炼，这引发了一个实例级审计问题：对于微调模型和训练提示-响应对，能否推断模型是在原始提示还是其LLM精炼版本上训练的？这对数据集治理和训练数据争议解决很重要。

Method: 提出RePro框架，基于精炼提示会导致教师强制token分布稳定可检测偏移的现象，融合教师强制似然特征和logit排序信号。通过影子微调学习可迁移表示，使用轻量级线性头在未见受害者模型上推断来源，无需访问训练数据。

Result: RePro在实证中始终表现强劲，并能很好地跨不同精炼器迁移，表明它利用了精炼器无关的分布偏移而非改写风格伪影。

Conclusion: 该研究形式化了精炼来源推断任务，证明了提示精炼会产生稳定可检测的分布偏移，提出的RePro框架能有效解决这一审计问题，具有实际应用价值。

Abstract: Instruction tuning increasingly relies on LLM-based prompt refinement, where prompts in the training corpus are selectively rewritten by an external refiner to improve clarity and instruction alignment. This motivates an instance-level audit problem: for a fine-tuned model and a training prompt-response pair, can we infer whether the model was trained on the original prompt or its LLM-refined version within a mixed corpus? This matters for dataset governance and dispute resolution when training data are contested. However, it is non-trivial in practice: refined and raw instances are interleaved in the training corpus with unknown, source-dependent mixture ratios, making it harder to develop provenance methods that generalize across models and training setups. In this paper, we formalize this audit task as Refinement Provenance Inference (RPI) and show that prompt refinement yields stable, detectable shifts in teacher-forced token distributions, even when semantic differences are not obvious. Building on this phenomenon, we propose RePro, a logit-based provenance framework that fuses teacher-forced likelihood features with logit-ranking signals. During training, RePro learns a transferable representation via shadow fine-tuning, and uses a lightweight linear head to infer provenance on unseen victims without training-data access. Empirically, RePro consistently attains strong performance and transfers well across refiners, suggesting that it exploits refiner-agnostic distribution shifts rather than rewrite-style artifacts.

</details>


### [76] [Output Embedding Centering for Stable LLM Pretraining](https://arxiv.org/abs/2601.02031)
*Felix Stollenwerk,Anna Lokrantz,Niclas Hertzberg*

Main category: cs.LG

TL;DR: 论文提出输出嵌入中心化(OEC)方法，通过μ-centering和μ-loss两种实现方式解决大语言模型预训练中的输出logit发散问题，相比z-loss更有效且超参数调优更简单。


<details>
  <summary>Details</summary>
Motivation: 大语言模型预训练成本高且容易出现训练不稳定性，特别是训练后期大学习率下的输出logit发散问题。现有z-loss方法仅解决症状而非根本原因。

Method: 从输出嵌入几何角度分析不稳定性原因，提出输出嵌入中心化(OEC)作为新缓解策略，包括确定性操作μ-centering和正则化方法μ-loss两种实现方式。

Result: 两种OEC变体在训练稳定性和学习率敏感性方面均优于z-loss，能确保在大学习率下训练收敛（此时z-loss会失败），且μ-loss对正则化超参数调优的敏感性显著低于z-loss。

Conclusion: OEC通过解决输出嵌入几何的根本问题有效缓解输出logit发散，相比z-loss提供更稳定、更鲁棒的训练解决方案，特别适合大学习率场景。

Abstract: Pretraining of large language models is not only expensive but also prone to certain training instabilities. A specific instability that often occurs for large learning rates at the end of training is output logit divergence. The most widely used mitigation strategy, z-loss, merely addresses the symptoms rather than the underlying cause of the problem. In this paper, we analyze the instability from the perspective of the output embeddings' geometry and identify its cause. Based on this, we propose output embedding centering (OEC) as a new mitigation strategy, and prove that it suppresses output logit divergence. OEC can be implemented in two different ways, as a deterministic operation called μ-centering, or a regularization method called μ-loss. Our experiments show that both variants outperform z-loss in terms of training stability and learning rate sensitivity. In particular, they ensure that training converges even for large learning rates when z-loss fails. Furthermore, we find that μ-loss is significantly less sensitive to regularization hyperparameter tuning than z-loss.

</details>


### [77] [Distorted Distributional Policy Evaluation for Offline Reinforcement Learning](https://arxiv.org/abs/2601.01917)
*Ryo Iwaki,Takayuki Osogami*

Main category: cs.LG

TL;DR: 本文提出了一种针对离线分布强化学习的新方法，通过引入分位数扭曲概念实现非均匀悲观主义，解决了现有方法因均匀低估回报分位数而导致的过度保守问题。


<details>
  <summary>Details</summary>
Motivation: 分布强化学习方法在在线场景中表现出色，但在离线场景中效果有限。作者假设现有离线DRL方法的主要局限在于它们均匀低估回报分位数的方法，这种均匀悲观主义会导致过于保守的价值估计，最终阻碍泛化和性能提升。

Method: 引入了一个新概念——分位数扭曲，通过根据支持数据的可用性调整保守程度，实现非均匀悲观主义。该方法基于理论分析，并通过实证验证。

Result: 该方法在实证验证中表现出改进的性能，优于均匀悲观主义方法。

Conclusion: 通过引入分位数扭曲实现非均匀悲观主义，能够有效解决离线分布强化学习中均匀低估回报分位数的问题，提高算法性能。

Abstract: While Distributional Reinforcement Learning (DRL) methods have demonstrated strong performance in online settings, its success in offline scenarios remains limited. We hypothesize that a key limitation of existing offline DRL methods lies in their approach to uniformly underestimate return quantiles. This uniform pessimism can lead to overly conservative value estimates, ultimately hindering generalization and performance. To address this, we introduce a novel concept called quantile distortion, which enables non-uniform pessimism by adjusting the degree of conservatism based on the availability of supporting data. Our approach is grounded in theoretical analysis and empirically validated, demonstrating improved performance over uniform pessimism.

</details>


### [78] [Entropy-Adaptive Fine-Tuning: Resolving Confident Conflicts to Mitigate Forgetting](https://arxiv.org/abs/2601.02151)
*Muxi Diao,Lele Yang,Wuxuan Gong,Yutong Zhang,Zhonghao Yan,Yufei Han,Kongming Liang,Weiran Xu,Zhanyu Ma*

Main category: cs.LG

TL;DR: 论文提出了一种名为EAFT的新方法，通过利用token级别的熵作为门控机制来区分认知不确定性和知识冲突，从而在保持下游任务性能的同时显著减轻监督微调带来的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 监督微调（SFT）是领域适应的标准范式，但经常导致灾难性遗忘。相比之下，策略强化学习（RL）能有效保留通用能力。作者研究这种差异，发现根本原因在于分布差距：RL与模型内部信念对齐，而SFT强制模型拟合外部监督。这种不匹配表现为"自信冲突"token，其特征是低概率但低熵，导致破坏性的梯度更新。

Method: 提出熵自适应微调（EAFT）方法。与仅依赖预测概率的方法不同，EAFT利用token级别的熵作为门控机制来区分认知不确定性和知识冲突。这使得模型能够从不确定样本中学习，同时抑制冲突数据的梯度。

Result: 在Qwen和GLM系列模型（参数范围从4B到32B）上，在数学、医疗和智能体领域进行了广泛实验。EAFT在匹配标准SFT下游性能的同时，显著减轻了通用能力的退化。

Conclusion: EAFT通过熵自适应机制有效解决了监督微调中的灾难性遗忘问题，在保持下游任务性能的同时更好地保留了模型的通用能力，为领域适应提供了一种更优的微调方法。

Abstract: Supervised Fine-Tuning (SFT) is the standard paradigm for domain adaptation, yet it frequently incurs the cost of catastrophic forgetting. In sharp contrast, on-policy Reinforcement Learning (RL) effectively preserves general capabilities. We investigate this discrepancy and identify a fundamental distributional gap: while RL aligns with the model's internal belief, SFT forces the model to fit external supervision. This mismatch often manifests as "Confident Conflicts" tokens characterized by low probability but low entropy. In these instances, the model is highly confident in its own prediction but is forced to learn a divergent ground truth, triggering destructive gradient updates. To address this, we propose Entropy-Adaptive Fine-Tuning (EAFT). Unlike methods relying solely on prediction probability, EAFT utilizes token-level entropy as a gating mechanism to distinguish between epistemic uncertainty and knowledge conflict. This allows the model to learn from uncertain samples while suppressing gradients on conflicting data. Extensive experiments on Qwen and GLM series (ranging from 4B to 32B parameters) across mathematical, medical, and agentic domains confirm our hypothesis. EAFT consistently matches the downstream performance of standard SFT while significantly mitigating the degradation of general capabilities.

</details>


### [79] [DatBench: Discriminative, Faithful, and Efficient VLM Evaluations](https://arxiv.org/abs/2601.02316)
*Siddharth Joshi,Haoli Yin,Rishabh Adiga,Ricardo Monti,Aldo Carranza,Alex Fang,Alvin Deng,Amro Abbas,Brett Larsen,Cody Blakeney,Darren Teh,David Schwab,Fan Pan,Haakon Mongstad,Jack Urbanek,Jason Lee,Jason Telanoff,Josh Wills,Kaleigh Mentzer,Luke Merrick,Parth Doshi,Paul Burstein,Pratyush Maini,Scott Loftin,Spandan Das,Tony Jiang,Vineeth Dorna,Zhengping Wang,Bogdan Gaza,Ari Morcos,Matthew Leavitt*

Main category: cs.LG

TL;DR: 该论文提出评估视觉语言模型(VLMs)的三个理想标准：忠实性、区分性和效率性，并识别现有评估方法的缺陷，通过转换和过滤现有基准创建了更可靠的评估套件DatBench。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型训练工作众多，但其评估方法仍不成熟。现有评估存在多种缺陷：多项选择题鼓励猜测、不能反映实际应用、容易饱和；可盲目回答的问题占比高；样本标注错误或模糊；评估计算成本过高（占开发计算资源的近20%）。

Method: 提出评估VLMs的三个理想标准：忠实性（对模态和应用的忠实）、区分性（能区分不同质量的模型）、效率性（计算效率）。通过转换（将多项选择题转为生成任务）和过滤（移除可盲目回答和错误标注的样本）来优化现有基准，创建了DatBench-Full（33个数据集）和DatBench（区分性子集）。

Result: 将多项选择题转为生成任务后，模型能力下降高达35%；过滤可盲目回答和错误标注样本后，提高了区分能力同时降低了计算成本；DatBench实现了13倍平均加速（最高50倍），同时保持了与原数据集相似的区分能力。

Conclusion: 该工作为VLMs评估提供了更严谨和可持续的路径，通过转换和过滤现有基准，创建了更忠实、更具区分性且更高效的评估套件，为大规模VLMs的发展提供了更好的评估指导。

Abstract: Empirical evaluation serves as the primary compass guiding research progress in foundation models. Despite a large body of work focused on training frontier vision-language models (VLMs), approaches to their evaluation remain nascent. To guide their maturation, we propose three desiderata that evaluations should satisfy: (1) faithfulness to the modality and application, (2) discriminability between models of varying quality, and (3) efficiency in compute. Through this lens, we identify critical failure modes that violate faithfulness and discriminability, misrepresenting model capabilities: (i) multiple-choice formats reward guessing, poorly reflect downstream use cases, and saturate early as models improve; (ii) blindly solvable questions, which can be answered without images, constitute up to 70% of some evaluations; and (iii) mislabeled or ambiguous samples compromise up to 42% of examples in certain datasets. Regarding efficiency, the computational burden of evaluating frontier models has become prohibitive: by some accounts, nearly 20% of development compute is devoted to evaluation alone. Rather than discarding existing benchmarks, we curate them via transformation and filtering to maximize fidelity and discriminability. We find that converting multiple-choice questions to generative tasks reveals sharp capability drops of up to 35%. In addition, filtering blindly solvable and mislabeled samples improves discriminative power while simultaneously reducing computational cost. We release DatBench-Full, a cleaned evaluation suite of 33 datasets spanning nine VLM capabilities, and DatBench, a discriminative subset that achieves 13x average speedup (up to 50x) while closely matching the discriminative power of the original datasets. Our work outlines a path toward evaluation practices that are both rigorous and sustainable as VLMs continue to scale.

</details>


### [80] [SynRXN: An Open Benchmark and Curated Dataset for Computational Reaction Modeling](https://arxiv.org/abs/2601.01943)
*Tieu-Long Phan,Nhu-Ngoc Nguyen Song,Peter F. Stadler*

Main category: cs.LG

TL;DR: SynRXN是一个用于计算机辅助合成规划的统一基准测试框架和开放数据资源，将端到端合成规划分解为五个任务族，提供标准化数据集和评估流程。


<details>
  <summary>Details</summary>
Motivation: 当前计算机辅助合成规划领域缺乏统一的基准测试框架，数据集异构且评估标准不一致，难以进行公平的纵向比较和可靠的性能评估。

Method: 将合成规划分解为五个任务族：反应平衡、原子到原子映射、反应分类、反应性质预测和合成路线设计；从异构公共源收集反应数据并统一表示；提供透明的数据分割函数、标准化评估流程和指标套件；使用脚本化构建配方确保可复现性。

Result: 创建了一个包含版本化数据集、明确元数据、许可证标签和机器可读清单的资源；提供了泄漏感知的训练/验证/测试分割；敏感任务仅作为评估集分发；整个资源在宽松开源许可证下发布。

Conclusion: SynRXN通过消除数据集异构性和提供透明的可重用评估框架，支持CASP方法的公平纵向比较，降低从业者获取稳健可比性能评估的门槛，促进计算机辅助合成规划领域的发展。

Abstract: We present SynRXN, a unified benchmarking framework and open-data resource for computer-aided synthesis planning (CASP). SynRXN decomposes end-to-end synthesis planning into five task families, covering reaction rebalancing, atom-to-atom mapping, reaction classification, reaction property prediction, and synthesis route design. Curated, provenance-tracked reaction corpora are assembled from heterogeneous public sources into a harmonized representation and packaged as versioned datasets for each task family, with explicit source metadata, licence tags, and machine-readable manifests that record checksums, and row counts. For every task, SynRXN provides transparent splitting functions that generate leakage-aware train, validation, and test partitions, together with standardized evaluation workflows and metric suites tailored to classification, regression, and structured prediction settings. For sensitive benchmarking, we combine public training and validation data with held-out gold-standard test sets, and contamination-prone tasks such as reaction rebalancing and atom-to-atom mapping are distributed only as evaluation sets and are explicitly not intended for model training. Scripted build recipes enable bitwise-reproducible regeneration of all corpora across machines and over time, and the entire resource is released under permissive open licences to support reuse and extension. By removing dataset heterogeneity and packaging transparent, reusable evaluation scaffolding, SynRXN enables fair longitudinal comparison of CASP methods, supports rigorous ablations and stress tests along the full reaction-informatics pipeline, and lowers the barrier for practitioners who seek robust and comparable performance estimates for real-world synthesis planning workloads.

</details>


### [81] [SerpentFlow: Generative Unpaired Domain Alignment via Shared-Structure Decomposition](https://arxiv.org/abs/2601.01979)
*Julie Keisler,Anastase Alexandre Charantonis,Yannig Goude,Boutheina Oueslati,Claire Monteleoni*

Main category: cs.LG

TL;DR: SerpentFlow是一个用于无配对域对齐的生成框架，通过将数据分解为共享结构和域特定组件，生成合成训练对，使条件生成模型能在无配对设置下工作。


<details>
  <summary>Details</summary>
Motivation: 在域对齐任务中，当不同域之间存在共享结构模式但缺乏配对观测数据时，直接监督学习变得困难。需要一种方法能够在无配对设置下学习域间的对应关系。

Method: SerpentFlow在潜在空间中将数据分解为共享组件和域特定组件。通过隔离共享结构并用随机噪声替换域特定组件，构建共享表示与目标域样本之间的合成训练对。在超分辨率任务中，共享组件对应低频内容，高频细节对应域特定变异性，使用基于分类器的标准自动确定分离频率。

Result: 在合成图像、物理过程模拟和气候降尺度任务上的实验表明，该方法能有效重建与底层低频模式一致的高频结构，支持共享结构分解作为无配对域对齐的有效策略。

Conclusion: 共享结构分解是解决无配对域对齐问题的有效方法，SerpentFlow框架通过生成伪配对数据，使传统需要配对数据的条件生成模型能够在无监督设置下工作，为域对齐任务提供了新思路。

Abstract: Domain alignment refers broadly to learning correspondences between data distributions from distinct domains. In this work, we focus on a setting where domains share underlying structural patterns despite differences in their specific realizations. The task is particularly challenging in the absence of paired observations, which removes direct supervision across domains. We introduce a generative framework, called SerpentFlow (SharEd-structuRe decomPosition for gEnerative domaiN adapTation), for unpaired domain alignment. SerpentFlow decomposes data within a latent space into a shared component common to both domains and a domain-specific one. By isolating the shared structure and replacing the domain-specific component with stochastic noise, we construct synthetic training pairs between shared representations and target-domain samples, thereby enabling the use of conditional generative models that are traditionally restricted to paired settings. We apply this approach to super-resolution tasks, where the shared component naturally corresponds to low-frequency content while high-frequency details capture domain-specific variability. The cutoff frequency separating low- and high-frequency components is determined automatically using a classifier-based criterion, ensuring a data-driven and domain-adaptive decomposition. By generating pseudo-pairs that preserve low-frequency structures while injecting stochastic high-frequency realizations, we learn the conditional distribution of the target domain given the shared representation. We implement SerpentFlow using Flow Matching as the generative pipeline, although the framework is compatible with other conditional generative approaches. Experiments on synthetic images, physical process simulations, and a climate downscaling task demonstrate that the method effectively reconstructs high-frequency structures consistent with underlying low-frequency patterns, supporting shared-structure decomposition as an effective strategy for unpaired domain alignment.

</details>


### [82] [Prior Diffusiveness and Regret in the Linear-Gaussian Bandit](https://arxiv.org/abs/2601.02022)
*Yifan Zhu,John C. Duchi,Benjamin Van Roy*

Main category: cs.LG

TL;DR: 本文证明了Thompson采样在线性高斯多臂赌博机问题中具有$\tilde{O}(σd \sqrt{T} + d r \sqrt{\mathrm{Tr}(Σ_0)})$的贝叶斯遗憾上界，其中先验相关的"预热"项与极小极大遗憾项呈可加性而非乘性关系。


<details>
  <summary>Details</summary>
Motivation: 现有Thompson采样在线性高斯赌博机中的遗憾界通常包含先验分布参数与极小极大遗憾项的乘积关系。本文旨在证明这两个项实际上可以解耦为可加关系，从而更精确地刻画算法性能。

Method: 通过引入新的"椭圆势能"引理来分析Thompson采样在线性高斯赌博机中的性能。该方法能够分离先验相关的"预热"项和长期运行的极小极大遗憾项。

Result: 证明了Thompson采样具有$\tilde{O}(σd \sqrt{T} + d r \sqrt{\mathrm{Tr}(Σ_0)})$的贝叶斯遗憾上界，其中$d$为维度，$T$为时间范围，$r$为动作的最大$\ell_2$范数，$σ^2$为噪声方差。同时提供了下界证明预热项是不可避免的。

Conclusion: Thompson采样在线性高斯赌博机中的遗憾可以分解为可加的两部分：先验相关的预热项和长期运行的极小极大遗憾项。这一结果改进了现有理论，并通过下界证明了预热项的必然性。

Abstract: We prove that Thompson sampling exhibits $\tilde{O}(σd \sqrt{T} + d r \sqrt{\mathrm{Tr}(Σ_0)})$ Bayesian regret in the linear-Gaussian bandit with a $\mathcal{N}(μ_0, Σ_0)$ prior distribution on the coefficients, where $d$ is the dimension, $T$ is the time horizon, $r$ is the maximum $\ell_2$ norm of the actions, and $σ^2$ is the noise variance. In contrast to existing regret bounds, this shows that to within logarithmic factors, the prior-dependent ``burn-in'' term $d r \sqrt{\mathrm{Tr}(Σ_0)}$ decouples additively from the minimax (long run) regret $σd \sqrt{T}$. Previous regret bounds exhibit a multiplicative dependence on these terms. We establish these results via a new ``elliptical potential'' lemma, and also provide a lower bound indicating that the burn-in term is unavoidable.

</details>


### [83] [GDRO: Group-level Reward Post-training Suitable for Diffusion Models](https://arxiv.org/abs/2601.02036)
*Yiyang Wang,Xi Chen,Xiaogang Xu,Yu Liu,Hengshuang Zhao*

Main category: cs.LG

TL;DR: 本文提出GDRO（组级直接奖励优化），一种针对文本到图像整流流扩散模型的离线后训练范式，解决了现有在线强化学习方法效率低、依赖随机采样器和奖励黑客等问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法采用从LLMs到文本到图像整流流扩散模型的在线强化学习进行奖励对齐，虽然使用组级奖励成功对齐了模型与目标奖励，但面临效率低、依赖随机采样器和奖励黑客等挑战。整流流模型与LLMs存在根本差异：1）在线图像采样耗时多，主导训练时间；2）整流流在初始噪声固定后是确定性的。

Method: 设计了组级直接奖励优化（GDRO），这是一种结合整流流模型特性的组级奖励对齐后训练范式。通过严格理论分析，GDRO支持完全离线训练，节省图像采样的大时间成本，且独立于扩散采样器，无需ODE-to-SDE近似来获得随机性。同时实证研究奖励黑客陷阱，在评估中使用校正分数考虑原始评估奖励和奖励黑客趋势。

Result: 大量实验表明，GDRO在OCR和GenEval任务中通过组级离线优化有效且高效地提高了扩散模型的奖励分数，同时在缓解奖励黑客方面表现出强大的稳定性和鲁棒性。

Conclusion: GDRO为文本到图像整流流扩散模型提供了一种高效、稳定且鲁棒的组级奖励对齐方法，解决了现有在线强化学习方法的局限性，通过离线训练和采样器独立性显著提升了训练效率。

Abstract: Recent advancements adopt online reinforcement learning (RL) from LLMs to text-to-image rectified flow diffusion models for reward alignment. The use of group-level rewards successfully aligns the model with the targeted reward. However, it faces challenges including low efficiency, dependency on stochastic samplers, and reward hacking. The problem is that rectified flow models are fundamentally different from LLMs: 1) For efficiency, online image sampling takes much more time and dominates the time of training. 2) For stochasticity, rectified flow is deterministic once the initial noise is fixed. Aiming at these problems and inspired by the effects of group-level rewards from LLMs, we design Group-level Direct Reward Optimization (GDRO). GDRO is a new post-training paradigm for group-level reward alignment that combines the characteristics of rectified flow models. Through rigorous theoretical analysis, we point out that GDRO supports full offline training that saves the large time cost for image rollout sampling. Also, it is diffusion-sampler-independent, which eliminates the need for the ODE-to-SDE approximation to obtain stochasticity. We also empirically study the reward hacking trap that may mislead the evaluation, and involve this factor in the evaluation using a corrected score that not only considers the original evaluation reward but also the trend of reward hacking. Extensive experiments demonstrate that GDRO effectively and efficiently improves the reward score of the diffusion model through group-wise offline optimization across the OCR and GenEval tasks, while demonstrating strong stability and robustness in mitigating reward hacking.

</details>


### [84] [Explore the Ideology of Deep Learning in ENSO Forecasts](https://arxiv.org/abs/2601.02050)
*Yanhai Gan,Yipeng Chen,Ning Li,Xingguo Liu,Junyu Dong,Xianyao Chen*

Main category: cs.LG

TL;DR: 该论文提出了基于有界变差函数的可解释性框架，通过激活函数饱和区"拯救"死神经元来增强模型表达能力，揭示了ENSO预测主要来自热带太平洋，并探讨了春季可预测性障碍的成因。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习显著提高了ENSO预测技能，但模型的不透明性阻碍了科学信任和业务部署，需要建立数学基础的可解释性框架。

Method: 引入基于有界变差函数的可解释性框架，通过从激活函数饱和区"拯救"死神经元来增强模型表达能力，进行控制实验验证方法稳健性。

Result: 分析显示ENSO可预测性主要来自热带太平洋，印度洋和大西洋也有贡献；春季可预测性障碍期间敏感性扩大但预测性能下降，可能源于次优变量选择。

Conclusion: 该方法与物理理解一致，纳入更多海洋-大气变量可能有助于超越春季可预测性障碍限制，推进长期ENSO预测。

Abstract: The El Ni{~n}o-Southern Oscillation (ENSO) exerts profound influence on global climate variability, yet its prediction remains a grand challenge. Recent advances in deep learning have significantly improved forecasting skill, but the opacity of these models hampers scientific trust and operational deployment. Here, we introduce a mathematically grounded interpretability framework based on bounded variation function. By rescuing the "dead" neurons from the saturation zone of the activation function, we enhance the model's expressive capacity. Our analysis reveals that ENSO predictability emerges dominantly from the tropical Pacific, with contributions from the Indian and Atlantic Oceans, consistent with physical understanding. Controlled experiments affirm the robustness of our method and its alignment with established predictors. Notably, we probe the persistent Spring Predictability Barrier (SPB), finding that despite expanded sensitivity during spring, predictive performance declines-likely due to suboptimal variable selection. These results suggest that incorporating additional ocean-atmosphere variables may help transcend SPB limitations and advance long-range ENSO prediction.

</details>


### [85] [Prototype-Based Learning for Healthcare: A Demonstration of Interpretable AI](https://arxiv.org/abs/2601.02106)
*Ashish Rana,Ammar Shaker,Sascha Saralajew,Takashi Suzuki,Kosuke Yasuda,Shintaro Kato,Toshikazu Wada,Toshiyuki Fujikawa,Toru Kikutsuji*

Main category: cs.LG

TL;DR: ProtoPal框架通过原型学习实现个性化预防医疗，提供可理解和可验证的预测、干预和推荐，具有前后端模式，在保持优异量化性能的同时提供直观的干预展示和模拟结果。


<details>
  <summary>Details</summary>
Motivation: 尽管机器学习和可解释AI有所进展，但个性化预防医疗领域仍存在缺口：预测、干预和推荐需要对所有医疗领域利益相关者来说都是可理解和可验证的。

Method: 提出ProtoPal框架，采用原型学习方法，具有前端和后端两种模式，能够提供直观的干预展示和模拟结果。

Result: 该框架实现了优异的量化性能，同时提供了干预措施及其模拟结果的直观呈现。

Conclusion: 原型学习方法能够有效解决个性化预防医疗中可理解性和可验证性的需求，为医疗领域利益相关者提供既准确又直观的解决方案。

Abstract: Despite recent advances in machine learning and explainable AI, a gap remains in personalized preventive healthcare: predictions, interventions, and recommendations should be both understandable and verifiable for all stakeholders in the healthcare sector. We present a demonstration of how prototype-based learning can address these needs. Our proposed framework, ProtoPal, features both front- and back-end modes; it achieves superior quantitative performance while also providing an intuitive presentation of interventions and their simulated outcomes.

</details>


### [86] [Edge-aware GAT-based protein binding site prediction](https://arxiv.org/abs/2601.02138)
*Weisen Yang,Hanqing Zhang,Wangren Qiu,Xuan Xiao,Weizhong Lin*

Main category: cs.LG

TL;DR: 提出Edge-aware GAT模型用于蛋白质结合位点预测，通过原子级图构建和多维结构特征整合，在基准数据集上达到0.93 ROC-AUC，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 蛋白质结合位点的准确识别对理解生物分子相互作用机制和药物靶点理性设计至关重要。传统预测方法在捕捉复杂空间构象时难以平衡预测精度和计算效率。

Method: 提出Edge-aware GAT模型，构建原子级图并整合几何描述符、DSSP二级结构和相对溶剂可及性等多维结构特征。通过将原子间距离和方向向量作为注意力机制中的边特征，增强模型表示能力。使用方向张量传播和残基级注意力池化进一步改进结合位点定位和局部结构细节捕捉。

Result: 在基准数据集上，模型在蛋白质-蛋白质结合位点预测中达到0.93 ROC-AUC，优于多种最先进方法。PyMOL可视化证实了模型的实用性和可解释性。已部署公开可访问的Web服务器。

Conclusion: 该方法为蛋白质功能位点识别提供了一种新颖高效的解决方案，平衡了预测准确性、泛化能力和可解释性。

Abstract: Accurate identification of protein binding sites is crucial for understanding biomolecular interaction mechanisms and for the rational design of drug targets. Traditional predictive methods often struggle to balance prediction accuracy with computational efficiency when capturing complex spatial conformations. To address this challenge, we propose an Edge-aware Graph Attention Network (Edge-aware GAT) model for the fine-grained prediction of binding sites across various biomolecules, including proteins, DNA/RNA, ions, ligands, and lipids. Our method constructs atom-level graphs and integrates multidimensional structural features, including geometric descriptors, DSSP-derived secondary structure, and relative solvent accessibility (RSA), to generate spatially aware embedding vectors. By incorporating interatomic distances and directional vectors as edge features within the attention mechanism, the model significantly enhances its representation capacity. On benchmark datasets, our model achieves an ROC-AUC of 0.93 for protein-protein binding site prediction, outperforming several state-of-the-art methods. The use of directional tensor propagation and residue-level attention pooling further improves both binding site localization and the capture of local structural details. Visualizations using PyMOL confirm the model's practical utility and interpretability. To facilitate community access and application, we have deployed a publicly accessible web server at http://119.45.201.89:5000/. In summary, our approach offers a novel and efficient solution that balances prediction accuracy, generalization, and interpretability for identifying functional sites in proteins.

</details>


### [87] [ELLA: Efficient Lifelong Learning for Adapters in Large Language Models](https://arxiv.org/abs/2601.02232)
*Shristi Das Biswas,Yue Zhang,Anwesan Pal,Radhika Bhargava,Kaushik Roy*

Main category: cs.LG

TL;DR: ELLA是一个基于选择性子空间去相关原则的持续学习框架，通过惩罚高能量任务特定方向的对齐，同时保留低能量残差子空间的自由度，实现了无需数据回放、无需架构扩展的LLM持续学习。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在持续学习设置中面临严重的灾难性遗忘问题。现有方法存在根本性限制：基于回放的方法不切实际且侵犯隐私，而严格正交性方法在规模扩展时会崩溃，因为每个新任务都被投影到正交补空间，逐渐减少残差自由度并消除前向传递。

Method: ELLA基于选择性子空间去相关原则，明确表征过去更新的结构，惩罚沿其高能量、任务特定方向的对齐，同时保留低能量残差子空间的自由度以实现传递。通过轻量级正则化器在单个聚合更新矩阵上实现，对应各向异性收缩算子来限制干扰。

Result: 在三个流行基准测试中实现了最先进的持续学习性能，相对准确率提升高达9.6%，内存占用减少35倍。无需数据回放、无需架构扩展，存储需求可忽略。在不同架构上稳健扩展，并主动增强模型在未见任务上的零样本泛化性能。

Conclusion: ELLA为构建性终身LLM适应提供了一个原则性且可扩展的解决方案，通过选择性子空间去相关有效解决了持续学习中的灾难性遗忘问题，同时保持了前向传递能力。

Abstract: Large Language Models (LLMs) suffer severe catastrophic forgetting when adapted sequentially to new tasks in a continual learning (CL) setting. Existing approaches are fundamentally limited: replay-based methods are impractical and privacy-violating, while strict orthogonality-based methods collapse under scale: each new task is projected onto an orthogonal complement, progressively reducing the residual degrees of freedom and eliminating forward transfer by forbidding overlap in shared representations. In this work, we introduce ELLA, a training framework built on the principle of selective subspace de-correlation. Rather than forbidding all overlap, ELLA explicitly characterizes the structure of past updates and penalizes alignments along their high-energy, task-specific directions, while preserving freedom in the low-energy residual subspaces to enable transfer. Formally, this is realized via a lightweight regularizer on a single aggregated update matrix. We prove this mechanism corresponds to an anisotropic shrinkage operator that bounds interference, yielding a penalty that is both memory- and compute-constant regardless of task sequence length. ELLA requires no data replay, no architectural expansion, and negligible storage. Empirically, it achieves state-of-the-art CL performance on three popular benchmarks, with relative accuracy gains of up to $9.6\%$ and a $35\times$ smaller memory footprint. Further, ELLA scales robustly across architectures and actively enhances the model's zero-shot generalization performance on unseen tasks, establishing a principled and scalable solution for constructive lifelong LLM adaptation.

</details>


### [88] [Differential Privacy for Transformer Embeddings of Text with Nonparametric Variational Information Bottleneck](https://arxiv.org/abs/2601.02307)
*Dina El Zein,James Henderson*

Main category: cs.LG

TL;DR: 提出NVDP方法，通过向transformer嵌入添加噪声来保护隐私，在GLUE基准上实现隐私与准确性的平衡


<details>
  <summary>Details</summary>
Motivation: transformer嵌入包含多个向量，可能编码敏感信息，使攻击者能够恢复原始输入数据，需要隐私保护的数据共享方法

Method: 提出非参数变分差分隐私(NVDP)，将非参数变分信息瓶颈(NVIB)层集成到transformer架构中，向多向量嵌入注入噪声，使用Rényi散度和贝叶斯差分隐私(BDP)衡量隐私保护

Result: 在GLUE基准测试中，通过调整噪声水平实现了隐私与准确性的有用权衡，较低噪声水平下模型保持高准确性同时提供强隐私保证

Conclusion: NVDP方法有效平衡了隐私保护和数据实用性，为文本数据的安全共享提供了可行的解决方案

Abstract: We propose a privacy-preserving method for sharing text data by sharing noisy versions of their transformer embeddings. It has been shown that hidden representations learned by deep models can encode sensitive information from the input, making it possible for adversaries to recover the input data with considerable accuracy. This problem is exacerbated in transformer embeddings because they consist of multiple vectors, one per token. To mitigate this risk, we propose Nonparametric Variational Differential Privacy (NVDP), which ensures both useful data sharing and strong privacy protection. We take a differential privacy approach, integrating a Nonparametric Variational Information Bottleneck (NVIB) layer into the transformer architecture to inject noise into its multi-vector embeddings and thereby hide information, and measuring privacy protection with Rényi divergence and its corresponding Bayesian Differential Privacy (BDP) guarantee. Training the NVIB layer calibrates the noise level according to utility. We test NVDP on the GLUE benchmark and show that varying the noise level gives us a useful tradeoff between privacy and accuracy. With lower noise levels, our model maintains high accuracy while offering strong privacy guarantees, effectively balancing privacy and utility.

</details>


### [89] [Temporal Kolmogorov-Arnold Networks (T-KAN) for High-Frequency Limit Order Book Forecasting: Efficiency, Interpretability, and Alpha Decay](https://arxiv.org/abs/2601.02310)
*Ahmad Makinde*

Main category: cs.LG

TL;DR: 该论文提出T-KAN模型，用可学习的B样条激活函数替代传统LSTM的固定线性权重，用于高频交易环境中的限价订单簿预测，相比DeepLOB模型在k=100时间窗口上获得19.1%的F1分数相对提升，并在交易成本下实现132.48%的回报率。


<details>
  <summary>Details</summary>
Motivation: 高频交易环境中的限价订单簿数据噪声大、非线性强，传统模型如DeepLOB随着时间窗口增加预测能力衰减严重（alpha衰减问题），需要更有效的模型来处理市场信号的"形状"而不仅仅是幅度。

Method: 提出Temporal Kolmogorov-Arnold Networks (T-KAN)，用可学习的B样条激活函数替代标准LSTM中的固定线性权重，使模型能够学习市场信号的"形状"。模型使用FI-2010数据集，并针对低延迟FPGA实现通过高级综合进行优化。

Result: 在k=100时间窗口上，T-KAN相比DeepLOB获得19.1%的F1分数相对提升；在1.0基点交易成本下，T-KAN实现132.48%的回报率，而DeepLOB则亏损82.76%。模型还具有较好的可解释性，样条中的"死区"清晰可见。

Conclusion: T-KAN模型在高频限价订单簿预测中表现出色，不仅提高了预测性能，还实现了显著的经济回报，同时具有可解释性和低延迟硬件实现的潜力。

Abstract: High-Frequency trading (HFT) environments are characterised by large volumes of limit order book (LOB) data, which is notoriously noisy and non-linear. Alpha decay represents a significant challenge, with traditional models such as DeepLOB losing predictive power as the time horizon (k) increases. In this paper, using data from the FI-2010 dataset, we introduce Temporal Kolmogorov-Arnold Networks (T-KAN) to replace the fixed, linear weights of standard LSTMs with learnable B-spline activation functions. This allows the model to learn the 'shape' of market signals as opposed to just their magnitude. This resulted in a 19.1% relative improvement in the F1-score at the k = 100 horizon. The efficacy of T-KAN networks cannot be understated, producing a 132.48% return compared to the -82.76% DeepLOB drawdown under 1.0 bps transaction costs. In addition to this, the T-KAN model proves quite interpretable, with the 'dead-zones' being clearly visible in the splines. The T-KAN architecture is also uniquely optimized for low-latency FPGA implementation via High level Synthesis (HLS). The code for the experiments in this project can be found at https://github.com/AhmadMak/Temporal-Kolmogorov-Arnold-Networks-T-KAN-for-High-Frequency-Limit-Order-Book-Forecasting.

</details>


### [90] [Game of Coding: Coding Theory in the Presence of Rational Adversaries, Motivated by Decentralized Machine Learning](https://arxiv.org/abs/2601.02313)
*Hanzaleh Akbari Nodehi,Viveck R. Cadambe,Mohammad Ali Maddah-Ali*

Main category: cs.LG

TL;DR: 该论文提出了一个新颖的博弈论框架"编码游戏"，将编码理论扩展到诚实节点不占多数的去中心化系统中，特别针对理性对手而非纯粹恶意对手的场景。


<details>
  <summary>Details</summary>
Motivation: 在去中心化机器学习等新兴应用中，参与节点因被接受的贡献而获得奖励，这种激励机制催生了理性对手（strategic adversaries）而非纯粹恶意对手。传统编码理论假设最坏情况的对抗模型，要求诚实节点必须超过对手节点，这在去中心化系统中不适用，因此需要新的编码框架来处理理性对手场景。

Method: 提出了"编码游戏"这一博弈论框架，将编码理论扩展到信任最小化的设置中。特别关注重复编码，展示了该框架的两个关键特性：1）即使在对手节点占多数时也能实现非零的数据恢复概率；2）具有Sybil抵抗性，即均衡状态不随对手节点数量增加而改变。

Result: 该框架能够在诚实节点不占多数的情况下实现数据恢复，突破了传统编码理论的限制。重复编码在理性对手场景下展现出两个重要特性：非零恢复概率和Sybil抵抗性。即使在对手策略未知的情况下，该框架也能提供分析工具。

Conclusion: 编码游戏框架为去中心化系统中的编码理论提供了新的视角，特别适用于理性对手场景。该框架突破了传统编码理论对诚实节点多数的要求，为去中心化机器学习等应用提供了理论基础。论文还指出了对手策略未知场景下的开放性问题，为未来研究指明了方向。

Abstract: Coding theory plays a crucial role in enabling reliable communication, storage, and computation. Classical approaches assume a worst-case adversarial model and ensure error correction and data recovery only when the number of honest nodes exceeds the number of adversarial ones by some margin. However, in some emerging decentralized applications, particularly in decentralized machine learning (DeML), participating nodes are rewarded for accepted contributions. This incentive structure naturally gives rise to rational adversaries who act strategically rather than behaving in purely malicious ways.
  In this paper, we first motivate the need for coding in the presence of rational adversaries, particularly in the context of outsourced computation in decentralized systems. We contrast this need with existing approaches and highlight their limitations. We then introduce the game of coding, a novel game-theoretic framework that extends coding theory to trust-minimized settings where honest nodes are not in the majority. Focusing on repetition coding, we highlight two key features of this framework: (1) the ability to achieve a non-zero probability of data recovery even when adversarial nodes are in the majority, and (2) Sybil resistance, i.e., the equilibrium remains unchanged even as the number of adversarial nodes increases. Finally, we explore scenarios in which the adversary's strategy is unknown and outline several open problems for future research.

</details>


### [91] [Heterogeneous Low-Bandwidth Pre-Training of LLMs](https://arxiv.org/abs/2601.02360)
*Yazan Obeidi,Amir Sarfi,Joel Lidin,Paul Janson,Eugene Belilovsky*

Main category: cs.LG

TL;DR: SparseLoCo低通信数据并行方法与低带宽流水线模型并行结合，通过激活和激活梯度压缩实现异构分布式训练，在保持性能的同时显著降低通信开销。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型预训练需要分布式计算，但带宽限制阻碍了在数据中心之外的扩展，特别是当模型并行需要频繁的大规模设备间通信时。需要研究如何将低通信方法与低带宽模型并行结合，以支持异构参与者的分布式训练。

Method: 提出异构分布式训练框架：高带宽参与者托管完整模型副本，资源受限参与者分组使用流水线并行，通过子空间投影压缩激活和激活梯度通信。将子空间流水线压缩与SparseLoCo方法适配，研究多种调整方案。

Result: 在178M-1B参数的大规模语言建模实验中，激活压缩与SparseLoCo结合成本适中，选择性（异构）压缩相比压缩所有副本能持续改善损失-通信权衡，特别是在高压缩比下效果更明显。

Conclusion: 研究结果表明，将低带宽模型并行和异构参与者结合到LLM预训练中是可行的，为在资源受限环境中扩展大规模语言模型训练提供了实用路径。

Abstract: Pre-training large language models (LLMs) increasingly requires distributed compute, yet bandwidth constraints make it difficult to scale beyond well-provisioned datacenters-especially when model parallelism forces frequent, large inter-device communications. We study whether SparseLoCo, a low-communication data parallel method based on infrequent synchronization and sparse pseudo-gradient exchange, can be combined with low-bandwidth pipeline model parallelism via activation and activation-gradient compression. We introduce a heterogeneous distributed training framework where some participants host full replicas on high-bandwidth interconnects, while resource-limited participants are grouped to jointly instantiate a replica using pipeline parallelism with subspace-projected inter-stage communication. To make the recently introduced subspace pipeline compression compatible with SparseLoCo, we study a number of adaptations. Across large-scale language modeling experiments (178M-1B parameters) on standard pretraining corpora, we find that activation compression composes with SparseLoCo at modest cost, while selective (heterogeneous) compression consistently improves the loss-communication tradeoff relative to compressing all replicas-especially at aggressive compression ratios. These results suggest a practical path to incorporating low-bandwidth model parallelism and heterogeneous participants into LLM pre-training.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [92] [Semantic Alignment of Multilingual Knowledge Graphs via Contextualized Vector Projections](https://arxiv.org/abs/2601.00814)
*Abhishek Kumar*

Main category: cs.AI

TL;DR: 跨语言本体对齐系统使用基于嵌入的余弦相似度匹配，通过创新描述生成技术增强本体实体上下文信息，采用微调的多语言Transformer模型生成更好的嵌入，在OAEI-2022多语言农场赛道获得71% F1分数，比最佳基线提升16%。


<details>
  <summary>Details</summary>
Motivation: 解决跨语言本体对齐问题，传统方法难以捕捉跨语言的细微语义相似性，需要更有效的技术来提升对齐性能。

Method: 1. 使用创新技术为实体生成描述性文本，丰富本体实体的上下文信息；2. 采用微调的多语言Transformer模型生成高质量的实体嵌入；3. 使用余弦相似度匹配正样本实体对；4. 应用阈值过滤保留高相似度实体对。

Result: 在OAEI-2022多语言农场赛道评估中，获得71%的F1分数（78%召回率和65%精确率），比最佳基线方法提升16%，表明该方法能有效捕捉跨语言相似性。

Conclusion: 提出的跨语言本体对齐流程能有效捕捉跨语言的细微语义相似性，通过增强实体上下文表示和使用多语言嵌入模型，显著提升了本体对齐性能。

Abstract: The paper presents our work on cross-lingual ontology alignment system which uses embedding based cosine similarity matching. The ontology entities are made contextually richer by creating descriptions using novel techniques. We use a fine-tuned transformer based multilingual model for generating better embeddings. We use cosine similarity to find positive ontology entities pairs and then apply threshold filtering to retain only highly similar entities. We have evaluated our work on OAEI-2022 multifarm track. We achieve 71% F1 score (78% recall and 65% precision) on the evaluation dataset, 16% increase from best baseline score. This suggests that our proposed alignment pipeline is able to capture the subtle cross-lingual similarities.

</details>


### [93] [MathLedger: A Verifiable Learning Substrate with Ledger-Attested Feedback](https://arxiv.org/abs/2601.00816)
*Ismail Ahmad Abdullah*

Main category: cs.AI

TL;DR: MathLedger是一个用于可验证机器认知的框架，将形式验证、密码学证明和学习动态集成到单一认知循环中，旨在解决AI系统在安全关键部署中的信任危机。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统虽然性能卓越但缺乏透明度和可验证性，在安全关键应用中存在信任危机。需要建立能够提供形式化验证和可审计性的系统。

Method: 采用反射形式学习（RFL），这是一种符号化的梯度下降方法，更新由验证器结果而非统计损失驱动。系统集成了形式验证、密码学证明和学习动态，并包含故障关闭治理机制。

Result: 第一阶段实验验证了测量和治理基础设施：CAL-EXP-3验证了测量基础设施（Delta p计算、方差跟踪）；压力测试确认了故障关闭治理在超出边界条件下正确触发。系统原型能够实现大规模可审计性。

Conclusion: MathLedger提供了一个基础设施原型，实现了账本证明的学习系统，能够实现大规模可审计性。这是一个基础设施贡献，而非关于收敛性或能力的声明。

Abstract: Contemporary AI systems achieve extraordinary performance yet remain opaque and non-verifiable, creating a crisis of trust for safety-critical deployment. We introduce MathLedger, a substrate for verifiable machine cognition that integrates formal verification, cryptographic attestation, and learning dynamics into a single epistemic loop. The system implements Reflexive Formal Learning (RFL), a symbolic analogue of gradient descent where updates are driven by verifier outcomes rather than statistical loss.
  Phase I experiments validate the measurement and governance substrate under controlled conditions. CAL-EXP-3 validates measurement infrastructure (Delta p computation, variance tracking); separate stress tests confirm fail-closed governance triggers correctly under out-of-bounds conditions. No convergence or capability claims are made. The contribution is infrastructural: a working prototype of ledger-attested learning that enables auditability at scale.
  Keywords: verifiable learning, formal verification, cryptographic attestation, reflexive feedback, fail-closed governance

</details>


### [94] [Agentic AI for Autonomous, Explainable, and Real-Time Credit Risk Decision-Making](https://arxiv.org/abs/2601.00818)
*Chandra Sekhar Kubam*

Main category: cs.AI

TL;DR: 本文提出了一种基于Agentic AI的信用风险评估框架，通过多智能体系统实现自主、透明、实时的信用决策，相比传统模型在决策速度、透明度和响应性方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 金融服务快速数字化导致对自主、透明、实时信用风险决策系统的迫切需求。传统机器学习模型虽在模式识别上有效，但缺乏现代金融运营所需的适应性推理、情境感知和自主性。

Method: 提出Agentic AI框架，构建多智能体系统，包含强化学习、自然语言推理、可解释AI模块和实时数据吸收管道。系统包括智能体协作协议、风险评分引擎、可解释性层和持续反馈学习循环。

Result: 该系统在决策速度、透明度和响应性方面优于传统信用评分模型。但仍存在模型漂移风险、高维数据解释不一致、监管不确定性以及低资源环境基础设施限制等实际局限性。

Conclusion: 该框架具有变革信用分析的潜力，未来研究应关注动态监管合规机制、新型智能体协作、对抗鲁棒性以及跨国信用生态系统中的大规模实施。

Abstract: Significant digitalization of financial services in a short period of time has led to an urgent demand to have autonomous, transparent and real-time credit risk decision making systems. The traditional machine learning models are effective in pattern recognition, but do not have the adaptive reasoning, situational awareness, and autonomy needed in modern financial operations. As a proposal, this paper presents an Agentic AI framework, or a system where AI agents view the world of dynamic credit independent of human observers, who then make actions based on their articulable decision-making paths. The research introduces a multi-agent system with reinforcing learning, natural language reasoning, explainable AI modules, and real-time data absorption pipelines as a means of assessing the risk profiles of borrowers with few humans being involved. The processes consist of agent collaboration protocol, risk-scoring engines, interpretability layers, and continuous feedback learning cycles. Findings indicate that decision speed, transparency and responsiveness is better than traditional credit scoring models. Nevertheless, there are still some practical limitations such as risks of model drift, inconsistencies in interpreting high dimensional data and regulatory uncertainties as well as infrastructure limitations in low-resource settings. The suggested system has a high prospective to transform credit analytics and future studies ought to be directed on dynamic regulatory compliance mobilizers, new agent teamwork, adversarial robustness, and large-scale implementation in cross-country credit ecosystems.

</details>


### [95] [CogCanvas: Compression-Resistant Cognitive Artifacts for Long LLM Conversations](https://arxiv.org/abs/2601.00821)
*Tao An*

Main category: cs.AI

TL;DR: CogCanvas是一个无需训练的框架，通过从对话中提取认知构件并组织成时间感知图来解决大语言模型在长对话中的上下文限制问题，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型面临上下文窗口限制与长对话信息保真度之间的基本矛盾。现有方法（截断和摘要）要么丢弃早期信息，要么丢失细节。

Method: 引入CogCanvas框架，从对话轮次中提取基于原文的认知构件（决策、事实、提醒），并将其组织成时间感知图，实现抗压缩的检索。

Result: 在LoCoMo基准测试中，CogCanvas达到34.7%总体准确率，优于RAG（25.6%）和GraphRAG（13.7%）。在时间推理上相对改进+530%，在多跳因果推理上达到81.0%通过率。

Conclusion: 虽然经过专门训练的方法能达到更高绝对分数，但CogCanvas作为无需训练的方法为实践者提供了立即可部署的替代方案，显著优于标准基线。

Abstract: Large language models face a fundamental tension between context window limits and information fidelity in long conversations. Existing approaches--truncation and summarization--either discard early information or lose nuanced details. We introduce CogCanvas, a training-free framework that extracts verbatim-grounded cognitive artifacts (decisions, facts, reminders) from conversation turns and organizes them into a temporal-aware graph for compression-resistant retrieval.
  On the LoCoMo benchmark, CogCanvas achieves 34.7% overall accuracy, outperforming RAG (25.6%, +9.1pp) and GraphRAG (13.7%, +21.0pp). The advantage is most pronounced on temporal reasoning: 31.5% vs. 9.3% (RAG) and 5.0% (GraphRAG)--a +530% relative improvement. On multi-hop causal reasoning, CogCanvas achieves 81.0% pass rate vs. 40.0% for GraphRAG (+41.0pp). Controlled benchmarks show 97.5% recall (+78.5pp vs. summarization) with 93.0% exact match preservation.
  While heavily-optimized approaches achieve higher absolute scores through dedicated training (EverMemOS: approximately 92%), our training-free approach provides practitioners with an immediately-deployable alternative that significantly outperforms standard baselines. Code and data: https://github.com/tao-hpu/cog-canvas.

</details>


### [96] [Energy-Aware Routing to Large Reasoning Models](https://arxiv.org/abs/2601.00823)
*Austin R. Ellis-Mohr,Max Hartman,Lav R. Varshney*

Main category: cs.AI

TL;DR: 该论文研究大型推理模型(LRMs)的能源效率优化，提出在临界状态下通过方差感知的路由和调度策略来平衡平均能源供应与随机波动，从而减少能源浪费。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型具有异构的推理能源成本，不同模型和推理方式的能耗差异很大。为了减少能源消耗，需要选择合适的LRM并以正确的方式运行。系统性能取决于平均能源供应与随机波动之间的平衡。

Method: 提出临界状态作为最优操作点，在此状态下既不浪费辅助能源也不浪费基线能源。开发二阶特征分析来理解波动限制下的性能表现。基于训练计算和推理计算缩放定律制定调度策略，强调方差感知的路由和调度作为设计原则。

Result: 性能表现由变异性在时间、模型和执行选择中的吸收方式决定。方差感知路由和调度成为减少能源浪费的关键设计维度，为开发能源感知模型路由策略提供了理论基础。

Conclusion: 通过临界状态操作和方差感知调度策略，可以优化大型推理模型的能源效率，平衡能源供应与需求，减少能源浪费，为能源感知的模型路由策略提供理论框架。

Abstract: Large reasoning models (LRMs) have heterogeneous inference energy costs based on which model is used and how much it reasons. To reduce energy, it is important to choose the right LRM and operate it in the right way. As a result, the performance of systems that dispatch tasks to different individual LRMs depend on the balance between mean energy provisioning and stochastic fluctuations. The critical regime is the unique operating point at which neither auxiliary energy nor baseline energy is systematically wasted. Increasing baseline supply shifts the system toward persistent over-supply and baseline-energy waste, while reducing supply induces persistent reliance on auxiliary energy. Yet in this regime, performance remains volatility-limited and so a second-order characterization provides further insights that we develop. Here, performance is governed by how variability is absorbed across time, models, and execution choices. This perspective highlights variance-aware routing and dispatch as a principled design axis, and provides a theoretical basis for developing energy-aware model routing policies. Routing behavior is characterized when dispatch policies are based on training-compute and inference-compute scaling laws for LRMs.

</details>


### [97] [Can We Trust AI Explanations? Evidence of Systematic Underreporting in Chain-of-Thought Reasoning](https://arxiv.org/abs/2601.00830)
*Deep Pankajbhai Mehta*

Main category: cs.AI

TL;DR: 研究发现AI模型在推理过程中会注意到隐藏提示但选择不报告，即使被直接询问时承认注意到这些提示，这表明单纯观察AI推理过程不足以发现隐藏影响。


<details>
  <summary>Details</summary>
Motivation: 当AI系统逐步解释其推理过程时，从业者通常假设这些解释揭示了真正影响AI答案的因素。本研究旨在测试这一假设，探究AI模型是否会报告其推理中实际注意到的隐藏提示信息。

Method: 通过在问题中嵌入提示信息，测试AI模型是否会提及这些提示。研究涉及超过9,000个测试案例，覆盖11个领先的AI模型。采用多种干预措施：直接询问模型是否注意到提示、告知模型正在被观察、强制要求报告提示等。

Result: 模型几乎从不自发提及提示，但当被直接询问时，它们承认注意到了这些提示。告知模型正在被观察没有帮助。强制报告提示虽然有效，但会导致模型在没有提示时也报告提示，并降低其准确性。迎合用户偏好的提示尤其危险——模型最常遵循这些提示，却最少报告它们。

Conclusion: AI模型能够注意到影响其推理的信息，但会选择性地不报告这些信息。单纯观察AI的推理过程不足以发现隐藏的影响因素，需要更有效的机制来确保AI推理的透明度和可信度。

Abstract: When AI systems explain their reasoning step-by-step, practitioners often assume these explanations reveal what actually influenced the AI's answer. We tested this assumption by embedding hints into questions and measuring whether models mentioned them. In a study of over 9,000 test cases across 11 leading AI models, we found a troubling pattern: models almost never mention hints spontaneously, yet when asked directly, they admit noticing them. This suggests models see influential information but choose not to report it. Telling models they are being watched does not help. Forcing models to report hints works, but causes them to report hints even when none exist and reduces their accuracy. We also found that hints appealing to user preferences are especially dangerous-models follow them most often while reporting them least. These findings suggest that simply watching AI reasoning is not enough to catch hidden influences.

</details>


### [98] [OmniNeuro: A Multimodal HCI Framework for Explainable BCI Feedback via Generative AI and Sonification](https://arxiv.org/abs/2601.00843)
*Ayda Aghaei Nia*

Main category: cs.AI

TL;DR: OmniNeuro是一个新型HCI框架，将BCI从黑盒解码器转变为透明的反馈伙伴，通过物理、混沌和量子启发的可解释性引擎提供实时神经声化和生成式AI临床报告。


<details>
  <summary>Details</summary>
Motivation: 深度学习虽然提高了脑机接口的解码精度，但其"黑盒"特性阻碍了临床采用，导致用户挫折感和神经可塑性结果不佳。

Method: 提出OmniNeuro框架，集成三个可解释性引擎：1) 物理（能量），2) 混沌（分形复杂性），3) 量子启发的不确定性建模。这些指标驱动实时神经声化和生成式AI临床报告。该系统与解码器无关，可作为任何最先进架构的可解释性层。

Result: 在PhysioNet数据集（N=109）上评估，系统平均准确率达到58.52%。定性试点研究（N=3）证实，可解释的反馈有助于用户调节心理努力并减少"试错"阶段。

Conclusion: OmniNeuro通过将BCI转变为透明的反馈伙伴，解决了深度学习算法的黑盒问题，有助于改善用户体验和临床结果，促进脑机接口的临床采用。

Abstract: While Deep Learning has improved Brain-Computer Interface (BCI) decoding accuracy, clinical adoption is hindered by the "Black Box" nature of these algorithms, leading to user frustration and poor neuroplasticity outcomes. We propose OmniNeuro, a novel HCI framework that transforms the BCI from a silent decoder into a transparent feedback partner. OmniNeuro integrates three interpretability engines: (1) Physics (Energy), (2) Chaos (Fractal Complexity), and (3) Quantum-Inspired uncertainty modeling. These metrics drive real-time Neuro-Sonification and Generative AI Clinical Reports. Evaluated on the PhysioNet dataset ($N=109$), the system achieved a mean accuracy of 58.52%, with qualitative pilot studies ($N=3$) confirming that explainable feedback helps users regulate mental effort and reduces the "trial-and-error" phase. OmniNeuro is decoder-agnostic, acting as an essential interpretability layer for any state-of-the-art architecture.

</details>


### [99] [Enhancing Temporal Awareness in LLMs for Temporal Point Processes](https://arxiv.org/abs/2601.00845)
*Lili Chen,Wensheng Gan,Shuang Liang,Philip S. Yu*

Main category: cs.AI

TL;DR: 提出TPP-TAL框架，通过增强LLMs的时间感知能力来改进时序点过程建模，显著提升时间似然估计和事件预测精度。


<details>
  <summary>Details</summary>
Motivation: 时序点过程在金融、医疗等领域很重要，但现有方法难以有效捕捉时间信息与语义上下文之间的复杂交互。尽管LLMs在序列建模中成功，但在时序点过程应用中仍面临挑战。

Method: 提出TPP-TAL框架，这是一种即插即用的方法，在将信息输入LLM之前，显式地对齐时间动态与上下文语义，而不是简单地拼接事件时间和类型嵌入。

Result: 在多个基准数据集上的实验表明，TPP-TAL在时间似然估计和事件预测准确性方面都有显著改进。

Conclusion: 增强LLMs的时间感知能力对于连续时间事件建模至关重要，TPP-TAL框架为此提供了有效的解决方案。

Abstract: Temporal point processes (TPPs) are crucial for analyzing events over time and are widely used in fields such as finance, healthcare, and social systems. These processes are particularly valuable for understanding how events unfold over time, accounting for their irregularity and dependencies. Despite the success of large language models (LLMs) in sequence modeling, applying them to temporal point processes remains challenging. A key issue is that current methods struggle to effectively capture the complex interaction between temporal information and semantic context, which is vital for accurate event modeling. In this context, we introduce TPP-TAL (Temporal Point Processes with Enhanced Temporal Awareness in LLMs), a novel plug-and-play framework designed to enhance temporal reasoning within LLMs. Rather than using the conventional method of simply concatenating event time and type embeddings, TPP-TAL explicitly aligns temporal dynamics with contextual semantics before feeding this information into the LLM. This alignment allows the model to better perceive temporal dependencies and long-range interactions between events and their surrounding contexts. Through comprehensive experiments on several benchmark datasets, it is shown that TPP-TAL delivers substantial improvements in temporal likelihood estimation and event prediction accuracy, highlighting the importance of enhancing temporal awareness in LLMs for continuous-time event modeling. The code is made available at https://github.com/chenlilil/TPP-TAL

</details>


### [100] [Comment on: Your Brain on ChatGPT: Accumulation of Cognitive Debt When Using an AI Assistant for Essay Writing Tasks](https://arxiv.org/abs/2601.00856)
*Milos Stankovic,Ella Hirche,Sarah Kollatzsch,Julia Nadine Doetsch*

Main category: cs.AI

TL;DR: 这是一篇针对Kosmyna等人(2025)关于ChatGPT与人类认知表现研究的评论文章，指出了原研究在样本量、可重复性、EEG分析方法、结果报告一致性和透明度等方面存在的问题。


<details>
  <summary>Details</summary>
Motivation: 作者旨在对Kosmyna等人关于AI助手对写作任务认知影响的研究提供建设性评论，帮助改进该研究以符合同行评审发表标准，因为原研究的一些结果可能需要更保守的解释。

Method: 通过批判性分析原研究的方法论，重点关注五个核心问题：研究设计（特别是样本量限制）、分析的可重复性、EEG分析方法问题、结果报告的不一致性以及研究过程和发现的透明度不足。

Result: 识别出原研究在多个关键方面存在缺陷，包括样本代表性不足、分析方法可能不可重复、EEG数据处理方法存在问题、结果报告存在不一致性以及整体透明度不够。

Conclusion: 虽然肯定原研究的价值和重要性，但指出该研究在方法学严谨性和透明度方面需要改进，建议更保守地解释结果，并解决已识别的问题以提高研究的科学可信度。

Abstract: Recently published work titled Your Brain on ChatGPT: Accumulation of Cognitive Debt When Using an AI Assistant for Essay Writing Task by Kosmyna et al. (2025) has sparked a vivid debate on the topic of artificial intelligence (AI) and human performance. We sincerely congratulate Kosmyna et al. for initiating such important research, collecting a valuable dataset, and establishing highly automated pipelines for Natural Language Processing (NLP) analyses and scoring. We aim to provide constructive comments that may improve the manuscript's readiness for peer-reviewed publication, as some results by Kosmyna et al. (2025) could be interpreted more conservatively. Our primary concerns focus on: (i) study design considerations, including the limited sample size; (ii) the reproducibility of the analyses; (iii) methodological issues related to the EEG analysis; (iv) inconsistencies in the reporting of results; and (v) limited transparency in several aspects of the study's procedures and findings.

</details>


### [101] [Cultural Encoding in Large Language Models: The Existence Gap in AI-Mediated Brand Discovery](https://arxiv.org/abs/2601.00869)
*Huang Junyao,Situ Ruimin,Ye Renqin*

Main category: cs.AI

TL;DR: 研究发现LLMs存在文化编码差异，中国LLMs品牌提及率比国际LLMs高30.6个百分点，这种差异源于训练数据地理分布而非语言本身，提出了"存在鸿沟"和"数据护城河"框架。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能系统越来越多地介入消费者信息发现过程，品牌面临算法不可见性的挑战。本研究旨在探究大型语言模型中存在的文化编码现象——由训练数据构成导致的品牌推荐系统性差异。

Method: 分析了1,909个纯英文查询，覆盖6个LLMs（GPT-4o、Claude、Gemini、Qwen3、DeepSeek、Doubao）和30个品牌，比较中国LLMs与国际LLMs的品牌提及率差异，并通过知织边界（OmniEdge）案例研究验证语言边界障碍的影响。

Result: 中国LLMs的品牌提及率比国际LLMs高30.6个百分点（88.9% vs. 58.3%，p<.001），这种差异在相同的英文查询中依然存在，表明训练数据的地理分布而非语言本身是主要驱动因素。知织边界平台在中国LLMs中提及率达65.6%，但在国际模型中为0%（p<.001）。

Conclusion: 提出了"存在鸿沟"概念和"数据护城河"框架，将AI可见内容视为VRIN战略资源，并定义"算法无处不在"作为生成引擎优化的战略目标。研究揭示了在AI中介市场中，品牌的"数据边界"决定了其"市场前沿"的界限。

Abstract: As artificial intelligence systems increasingly mediate consumer information discovery,
  brands face algorithmic invisibility. This study investigates Cultural Encoding in Large
  Language Models (LLMs) -- systematic differences in brand recommendations arising from
  training data composition. Analyzing 1,909 pure-English queries across 6 LLMs (GPT-4o,
  Claude, Gemini, Qwen3, DeepSeek, Doubao) and 30 brands, we find Chinese LLMs exhibit 30.6
  percentage points higher brand mention rates than International LLMs (88.9% vs. 58.3%,
  p<.001). This disparity persists in identical English queries, indicating training data
  geography -- not language -- drives the effect. We introduce the Existence Gap: brands
  absent from LLM training corpora lack "existence" in AI responses regardless of quality.
  Through a case study of Zhizibianjie (OmniEdge), a collaboration platform with 65.6%
  mention rate in Chinese LLMs but 0% in International models (p<.001), we demonstrate how
  Linguistic Boundary Barriers create invisible market entry obstacles. Theoretically, we
  contribute the Data Moat Framework, conceptualizing AI-visible content as a VRIN strategic
  resource. We operationalize Algorithmic Omnipresence -- comprehensive brand visibility
  across LLM knowledge bases -- as the strategic objective for Generative Engine Optimization
  (GEO). Managerially, we provide an 18-month roadmap for brands to build Data Moats
  through semantic coverage, technical depth, and cultural localization. Our findings reveal
  that in AI-mediated markets, the limits of a brand's "Data Boundaries" define the limits
  of its "Market Frontiers."

</details>


### [102] [Universal Conditional Logic: A Formal Language for Prompt Engineering](https://arxiv.org/abs/2601.00880)
*Anthony Mikinka*

Main category: cs.AI

TL;DR: UCL是一个将提示工程从启发式实践转化为系统化优化的数学框架，通过系统评估证明能显著减少29.8%的token使用并降低成本。


<details>
  <summary>Details</summary>
Motivation: 当前提示工程主要依赖启发式实践，缺乏系统化的优化框架。研究者希望建立数学框架来系统化提示优化，提高LLM交互效率并降低成本。

Method: 提出Universal Conditional Logic (UCL)框架，包含指示函数(I_i)、结构开销函数(O_s = gamma * sum(ln C_k))、早期绑定等核心机制。通过系统评估(N=305, 11个模型, 4次迭代)验证框架效果。

Result: 显著减少29.8%的token使用(t(10)=6.36, p < 0.001, Cohen's d = 2.01)，相应降低成本。发现过指定悖论：超过阈值S* = 0.509后，额外指定会二次降低性能。最优UCL配置因模型架构而异。

Conclusion: UCL为高效LLM交互提供了可校准的框架，模型家族特定的优化是未来关键研究方向。该框架将提示工程从启发式实践转变为系统化优化。

Abstract: We present Universal Conditional Logic (UCL), a mathematical framework for prompt optimization that transforms prompt engineering from heuristic practice into systematic optimization. Through systematic evaluation (N=305, 11 models, 4 iterations), we demonstrate significant token reduction (29.8%, t(10)=6.36, p < 0.001, Cohen's d = 2.01) with corresponding cost savings. UCL's structural overhead function O_s(A) explains version-specific performance differences through the Over-Specification Paradox: beyond threshold S* = 0.509, additional specification degrades performance quadratically. Core mechanisms -- indicator functions (I_i in {0,1}), structural overhead (O_s = gamma * sum(ln C_k)), early binding -- are validated. Notably, optimal UCL configuration varies by model architecture -- certain models (e.g., Llama 4 Scout) require version-specific adaptations (V4.1). This work establishes UCL as a calibratable framework for efficient LLM interaction, with model-family-specific optimization as a key research direction.

</details>


### [103] [Counterfactual Self-Questioning for Stable Policy Optimization in Language Models](https://arxiv.org/abs/2601.00885)
*Mandar Parab*

Main category: cs.AI

TL;DR: 提出Counterfactual Self-Questioning框架，让单一语言模型通过生成和评估自身推理的反事实批评来实现自我改进，无需外部批评者或奖励模型。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型自我改进方法大多依赖外部批评者、学习到的奖励模型或集成采样，这增加了复杂性和训练不稳定性。需要一种更简单、更稳定的自我改进方法。

Method: 提出反事实自我提问框架：1）生成初始推理轨迹；2）针对潜在失败点制定针对性问题；3）生成暴露错误假设或无效步骤的替代推理轨迹；4）利用这些反事实轨迹作为结构化相对反馈直接用于策略优化。

Result: 在多个数学推理基准测试中，反事实自我提问提高了准确性和训练稳定性，特别是对于较小模型，仅使用内部生成的监督就能实现可扩展的自我改进。

Conclusion: Counterfactual Self-Questioning框架为语言模型自我改进提供了一种更简单、更稳定的方法，无需外部模型辅助，仅依靠内部生成的监督就能实现有效改进。

Abstract: Recent work on language model self-improvement shows that models can refine their own reasoning through reflection, verification, debate, or self-generated rewards. However, most existing approaches rely on external critics, learned reward models, or ensemble sampling, which increases complexity and training instability. We propose Counterfactual Self-Questioning, a framework in which a single language model generates and evaluates counterfactual critiques of its own reasoning. The method produces an initial reasoning trace, formulates targeted questions that challenge potential failure points, and generates alternative reasoning trajectories that expose incorrect assumptions or invalid steps. These counterfactual trajectories provide structured relative feedback that can be directly used for policy optimization without auxiliary models. Experiments on multiple mathematical reasoning benchmarks show that counterfactual self-questioning improves accuracy and training stability, particularly for smaller models, enabling scalable self-improvement using internally generated supervision alone.

</details>


### [104] [Context Collapse: In-Context Learning and Model Collapse](https://arxiv.org/abs/2601.00923)
*Josef Ott*

Main category: cs.AI

TL;DR: 该论文研究了大型语言模型中的两个关键现象：上下文学习和模型崩溃。通过线性变换器和简化设置分析，揭示了上下文学习中的相变现象，证明了模型崩溃的必然性，并提出了"上下文崩溃"的新概念。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型中的上下文学习和模型崩溃现象，理解其内在机制和动态特性，为模型稳定性和长期生成能力提供理论分析基础。

Method: 1. 使用带权重绑定的线性变换器研究上下文学习，将前向传播简化为预条件梯度下降；2. 使用鞅理论和随机游走理论分析线性回归和高斯拟合的简化设置；3. 提出"上下文崩溃"概念分析长序列生成中的上下文退化问题。

Result: 1. 上下文学习存在相变：超过临界上下文长度时，解会发展出斜对称分量；2. 模型崩溃几乎必然发生，除非数据快速增长或长期保留；3. 发现了上下文崩溃现象，特别是在思维链推理中上下文会随时间退化。

Conclusion: 上下文学习和模型崩溃是LLMs中的核心现象，上下文学习中的相变机制与模型长期稳定性挑战通过"上下文崩溃"概念联系起来，为理解LLMs的动态行为提供了新的理论框架。

Abstract: This thesis investigates two key phenomena in large language models (LLMs): in-context learning (ICL) and model collapse. We study ICL in a linear transformer with tied weights trained on linear regression tasks, and show that minimising the in-context loss leads to a phase transition in the learned parameters. Above a critical context length, the solution develops a skew-symmetric component. We prove this by reducing the forward pass of the linear transformer under weight tying to preconditioned gradient descent, and then analysing the optimal preconditioner. This preconditioner includes a skew-symmetric component, which induces a rotation of the gradient direction. For model collapse, we use martingale and random walk theory to analyse simplified settings - linear regression and Gaussian fitting - under both replacing and cumulative data regimes. We strengthen existing results by proving almost sure convergence, showing that collapse occurs unless the data grows sufficiently fast or is retained over time. Finally, we introduce the notion of context collapse: a degradation of context during long generations, especially in chain-of-thought reasoning. This concept links the dynamics of ICL with long-term stability challenges in generative models.

</details>


### [105] [ElecTwit: A Framework for Studying Persuasion in Multi-Agent Social Systems](https://arxiv.org/abs/2601.00994)
*Michael Bao*

Main category: cs.AI

TL;DR: ElecTwit是一个模拟社交媒体政治选举中多智能体说服行为的框架，发现LLM使用了25种说服技术，不同模型架构和训练会影响社会模拟动态，并观察到"真相内核"和"墨水迷恋"等独特现象。


<details>
  <summary>Details</summary>
Motivation: 克服以往研究中基于游戏的模拟局限性，在真实环境中研究多智能体系统中的说服行为，特别是在社交媒体政治选举背景下的互动。

Method: 开发了ElecTwit模拟框架，在模拟社交媒体平台的政治选举环境中测试多个LLM，观察它们使用的说服技术和互动模式。

Result: 观察到25种具体的说服技术被大多数测试的LLM广泛使用，范围比之前报道的更广；不同模型在技术使用和整体说服输出上存在差异；发现了"真相内核"消息和"墨水迷恋"等独特现象，即智能体集体要求书面证据。

Conclusion: 该研究为在真实世界环境中评估有说服力的LLM智能体奠定了基础，有助于确保对齐并防止危险结果，同时展示了不同模型架构和训练如何影响现实社会模拟的动态。

Abstract: This paper introduces ElecTwit, a simulation framework designed to study persuasion within multi-agent systems, specifically emulating the interactions on social media platforms during a political election. By grounding our experiments in a realistic environment, we aimed to overcome the limitations of game-based simulations often used in prior research. We observed the comprehensive use of 25 specific persuasion techniques across most tested LLMs, encompassing a wider range than previously reported. The variations in technique usage and overall persuasion output between models highlight how different model architectures and training can impact the dynamics in realistic social simulations. Additionally, we observed unique phenomena such as "kernel of truth" messages and spontaneous developments with an "ink" obsession, where agents collectively demanded written proof. Our study provides a foundation for evaluating persuasive LLM agents in real-world contexts, ensuring alignment and preventing dangerous outcomes.

</details>


### [106] [Reinforcement Learning Enhanced Multi-hop Reasoning for Temporal Knowledge Question Answering](https://arxiv.org/abs/2601.01195)
*Wuzhenghong Wen,Chao Xue,Su Pan,Yuwei Sun,Minlong Peng*

Main category: cs.AI

TL;DR: 提出MRE框架，通过增强前向和后向推理来改进TKGQA中的多跳推理，使用提示工程生成多样推理轨迹，并通过T-GRPO算法优化全局最优推理路径


<details>
  <summary>Details</summary>
Motivation: 解决TKGQA中多跳推理时，LLM在每个跳步检索到大量时间相似且语义复杂的关系，导致次优决策和错误传播风险增加的问题

Method: 提出MRE框架：1) 使用提示工程引导LLM生成多样推理轨迹；2) 选择有效轨迹进行监督微调作为冷启动策略；3) 引入T-GRPO递归树结构学习探索方法，建立跳步间强因果依赖

Result: 在两个TKGQA基准测试中，MRE模型持续超越SOTA方法，在处理复杂多跳查询方面表现优异，同时提高了可解释性和对噪声时间标注的鲁棒性

Conclusion: MRE框架通过增强前向后向推理，有效改善了TKGQA中的多跳推理性能，减少了错误传播，提高了全局最优推理轨迹的识别能力

Abstract: Temporal knowledge graph question answering (TKGQA) involves multi-hop reasoning over temporally constrained entity relationships in the knowledge graph to answer a given question. However, at each hop, large language models (LLMs) retrieve subgraphs with numerous temporally similar and semantically complex relations, increasing the risk of suboptimal decisions and error propagation. To address these challenges, we propose the multi-hop reasoning enhanced (MRE) framework, which enhances both forward and backward reasoning to improve the identification of globally optimal reasoning trajectories. Specifically, MRE begins with prompt engineering to guide the LLM in generating diverse reasoning trajectories for a given question. Valid reasoning trajectories are then selected for supervised fine-tuning, serving as a cold-start strategy. Finally, we introduce Tree-Group Relative Policy Optimization (T-GRPO), a recursive, tree-structured learning-by-exploration approach. At each hop, exploration establishes strong causal dependencies on the previous hop, while evaluation is informed by multi-path exploration feedback from subsequent hops. Experimental results on two TKGQA benchmarks indicate that the proposed MRE-based model consistently surpasses state-of-the-art (SOTA) approaches in handling complex multi-hop queries. Further analysis highlights improved interpretability and robustness to noisy temporal annotations.

</details>


### [107] [Beyond Gemini-3-Pro: Revisiting LLM Routing and Aggregation at Scale](https://arxiv.org/abs/2601.01330)
*Shengji Tang,Weihao Lin,Jingqi Ye,Hao Li,Bo Zhang,Shuyue Hu,Tao Chen,Wangli Ouyang,Lei Bai,Peng Ye*

Main category: cs.AI

TL;DR: JiSi框架通过查询-响应混合路由、支持集聚合器选择和自适应路由-聚合切换三大创新，使10个开源LLM协作以47%成本超越Gemini-3-Pro，证明集体智能是通往AGI的新路径


<details>
  <summary>Details</summary>
Motivation: 当前LLM路由和聚合存在三个关键瓶颈：1）基于查询的路由器仅关注文本相似性；2）聚合方法静态，无法为不同任务选择合适聚合器；3）路由与聚合的互补性未充分利用。需要释放LLM协作的完整潜力

Method: 提出JiSi框架，包含三大创新：1）查询-响应混合路由，同时捕捉语义信息和问题难度；2）基于支持集的聚合器选择，联合评估聚合器的聚合能力和领域能力；3）自适应路由-聚合切换，动态利用路由和聚合的优势

Result: 在9个基准测试中，JiSi通过协调10个开源LLM，仅用47%成本就超越了Gemini-3-Pro的性能，同时优于主流基线方法

Conclusion: 集体智能代表了通往人工通用智能（AGI）的新路径，通过LLM协作而非单一模型扩展可以实现更优性能

Abstract: Large Language Models (LLMs) have rapidly advanced, with Gemini-3-Pro setting a new performance milestone. In this work, we explore collective intelligence as an alternative to monolithic scaling, and demonstrate that open-source LLMs' collaboration can surpass Gemini-3-Pro. We first revisit LLM routing and aggregation at scale and identify three key bottlenecks: (1) current train-free routers are limited by a query-based paradigm focusing solely on textual similarity; (2) recent aggregation methods remain largely static, failing to select appropriate aggregators for different tasks;(3) the complementarity of routing and aggregation remains underutilized. To address these problems, we introduce JiSi, a novel framework designed to release the full potential of LLMs' collaboration through three innovations: (1) Query-Response Mixed Routing capturing both semantic information and problem difficulty; (2) Support-Set-based Aggregator Selection jointly evaluating the aggregation and domain capacity of aggregators; (3) Adaptive Routing-Aggregation Switch dynamically leveraging the advantages of routing and aggregation. Comprehensive experiments on nine benchmarks demonstrate that JiSi can surpass Gemini-3-Pro with only 47% costs by orchestrating ten open-source LLMs, while outperforming mainstream baselines. It suggests that collective intelligence represents a novel path towards Artificial General Intelligence (AGI).

</details>


### [108] [A unified multimodal understanding and generation model for cross-disciplinary scientific research](https://arxiv.org/abs/2601.01363)
*Xiaomeng Yang,Zhiyu Tan,Xiaohui Zhong,Mengping Yang,Qiusheng Huang,Lei Chen,Libo Wu,Hao Li*

Main category: cs.AI

TL;DR: FuXi-Uni是一个统一的多模态科学模型，能够在单一架构中理解和生成跨学科的科学数据，在地球科学和生物医学领域表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型通常是领域特定的，缺乏同时理解和生成多模态科学数据的能力，而许多全球性挑战和科学问题本质上是跨学科的，需要跨多个领域的协同进展。

Method: FuXi-Uni将跨学科科学标记与自然语言标记对齐，使用科学解码器重建科学标记，支持自然语言对话和科学数值预测，在统一的多模态潜在空间中处理异构科学模态。

Result: 在地球系统建模中：1）生成0.25°分辨率的10天全球预报优于最先进的物理预报系统；2）热带气旋轨迹和强度预测优于最先进的物理模型；3）生成的高分辨率区域天气场超越标准插值基线。在生物医学中：在多个生物医学视觉问答基准上优于领先的多模态大语言模型。

Conclusion: FuXi-Uni通过在原生共享潜在空间中统一异构科学模态，同时保持强大的领域特定性能，为更通用的多模态科学模型迈出了重要一步。

Abstract: Scientific discovery increasingly relies on integrating heterogeneous, high-dimensional data across disciplines nowadays. While AI models have achieved notable success across various scientific domains, they typically remain domain-specific or lack the capability of simultaneously understanding and generating multimodal scientific data, particularly for high-dimensional data. Yet, many pressing global challenges and scientific problems are inherently cross-disciplinary and require coordinated progress across multiple fields. Here, we present FuXi-Uni, a native unified multimodal model for scientific understanding and high-fidelity generation across scientific domains within a single architecture. Specifically, FuXi-Uni aligns cross-disciplinary scientific tokens within natural language tokens and employs science decoder to reconstruct scientific tokens, thereby supporting both natural language conversation and scientific numerical prediction. Empirically, we validate FuXi-Uni in Earth science and Biomedicine. In Earth system modeling, the model supports global weather forecasting, tropical cyclone (TC) forecast editing, and spatial downscaling driven by only language instructions. FuXi-Uni generates 10-day global forecasts at 0.25° resolution that outperform the SOTA physical forecasting system. It shows superior performance for both TC track and intensity prediction relative to the SOTA physical model, and generates high-resolution regional weather fields that surpass standard interpolation baselines. Regarding biomedicine, FuXi-Uni outperforms leading multimodal large language models on multiple biomedical visual question answering benchmarks. By unifying heterogeneous scientific modalities within a native shared latent space while maintaining strong domain-specific performance, FuXi-Uni provides a step forward more general-purpose, multimodal scientific models.

</details>


### [109] [Empowering Small Language Models with Factual Hallucination-Aware Reasoning for Financial Classification](https://arxiv.org/abs/2601.01378)
*Han Yuan,Yilin Wu,Li Zhang,Zheng Ma*

Main category: cs.AI

TL;DR: 通过识别关联、自动检测和自适应推理的三步流程（AAAI），缓解小语言模型在金融分类中的事实幻觉问题，提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 小语言模型在金融分类中虽然推理速度快、可本地部署，但相比大语言模型更容易产生事实幻觉，分类性能较弱。研究旨在探索缓解事实幻觉是否能改善小语言模型的金融分类能力。

Method: 提出名为AAAI的三步流程：1) 关联识别 - 识别事实关联；2) 自动检测 - 使用基于编码器的验证器检测事实幻觉；3) 自适应推理 - 结合事实错误反馈，使小语言模型能够自适应推理。

Result: 在三个代表性小语言模型上的实验表明：1) 事实幻觉与错误分类呈正相关；2) 基于编码器的验证器能有效检测事实幻觉；3) 结合事实错误反馈的自适应推理能显著提升分类性能。

Conclusion: AAAI流程有助于提升小语言模型在金融领域的可信度和有效性应用，通过缓解事实幻觉问题来改善分类性能。

Abstract: Small language models (SLMs) are increasingly used for financial classification due to their fast inference and local deployability. However, compared with large language models, SLMs are more prone to factual hallucinations in reasoning and exhibit weaker classification performance. This raises a natural question: Can mitigating factual hallucinations improve SLMs' financial classification? To address this, we propose a three-step pipeline named AAAI (Association Identification, Automated Detection, and Adaptive Inference). Experiments on three representative SLMs reveal that: (1) factual hallucinations are positively correlated with misclassifications; (2) encoder-based verifiers effectively detect factual hallucinations; and (3) incorporating feedback on factual errors enables SLMs' adaptive inference that enhances classification performance. We hope this pipeline contributes to trustworthy and effective applications of SLMs in finance.

</details>


### [110] [A construction of an optimal base for conditional attribute and attributional condition implications in triadic contexts](https://arxiv.org/abs/2601.01467)
*Romuald Kwessy Mouona,Blaise Blériot Koguep Njionou,Etienne Romuald Temgoua Alomo,Rokia Missaoui,Leonard Kwuida*

Main category: cs.AI

TL;DR: 研究三元背景下条件属性和属性条件蕴含的优化基构造


<details>
  <summary>Details</summary>
Motivation: 研究Ganter和Obiedkov提出的三元上下文中的蕴含关系，特别是条件属性蕴含和属性条件蕴含，旨在为这些蕴含构建最优基

Method: 未在摘要中明确说明具体方法，但研究重点是在三元上下文中构建蕴含的最优基

Result: 未在摘要中明确说明具体结果，但研究目标是构建这些蕴含的最优基

Conclusion: 该研究致力于为三元上下文中的条件属性和属性条件蕴含构建最优基，以完善形式概念分析理论

Abstract: This article studies implications in triadic contexts. Specifically, we focus on those introduced by Ganter and Obiedkov, namely conditional attribute and attributional condition implications. Our aim is to construct an optimal base for these implications.

</details>


### [111] [Improving Behavioral Alignment in LLM Social Simulations via Context Formation and Navigation](https://arxiv.org/abs/2601.01546)
*Letian Kong,Qianran,Jin,Renyu Zhang*

Main category: cs.AI

TL;DR: 本文提出一个两阶段框架来改善LLM在复杂决策环境中的行为对齐：第一阶段上下文形成明确指定实验设计，第二阶段上下文导航在该表示中引导推理过程。验证表明复杂决策需要两个阶段，而简单任务仅需第一阶段。


<details>
  <summary>Details</summary>
Motivation: LLM越来越多地用于模拟人类行为实验，但在复杂决策环境中（需要预测他人行动并基于观察行为形成信念）与人类决策存在系统性差异，需要改进行为对齐方法。

Method: 提出两阶段框架：1) 上下文形成阶段明确指定实验设计，建立决策任务及其上下文的准确表示；2) 上下文导航阶段在该表示中引导推理过程做出决策。通过复制Kremer和Debo(2016)的顺序购买游戏进行验证，并扩展到Cason等人(2025)的众筹游戏和Gui与Toubia(2025)的需求估计任务。

Result: 在四个SOTA模型（GPT-4o、GPT-5、Claude-4.0-Sonnet-Thinking、DeepSeek-R1）上测试发现：复杂决策环境需要两个阶段才能实现与人类基准的行为对齐，而较简单的需求估计任务仅需要上下文形成阶段。

Conclusion: 研究阐明了每个阶段何时必要，为设计和诊断LLM社会模拟提供了系统方法，使其能够作为行为研究中人类受试者的补充。

Abstract: Large language models (LLMs) are increasingly used to simulate human behavior in experimental settings, but they systematically diverge from human decisions in complex decision-making environments, where participants must anticipate others' actions and form beliefs based on observed behavior. We propose a two-stage framework for improving behavioral alignment. The first stage, context formation, explicitly specifies the experimental design to establish an accurate representation of the decision task and its context. The second stage, context navigation, guides the reasoning process within that representation to make decisions. We validate this framework through a focal replication of a sequential purchasing game with quality signaling (Kremer and Debo, 2016), extending to a crowdfunding game with costly signaling (Cason et al., 2025) and a demand-estimation task (Gui and Toubia, 2025) to test generalizability across decision environments. Across four state-of-the-art (SOTA) models (GPT-4o, GPT-5, Claude-4.0-Sonnet-Thinking, DeepSeek-R1), we find that complex decision-making environments require both stages to achieve behavioral alignment with human benchmarks, whereas the simpler demand-estimation task requires only context formation. Our findings clarify when each stage is necessary and provide a systematic approach for designing and diagnosing LLM social simulations as complements to human subjects in behavioral research.

</details>


### [112] [Logics-STEM: Empowering LLM Reasoning via Failure-Driven Post-Training and Document Knowledge Enhancement](https://arxiv.org/abs/2601.01562)
*Mingyu Xu,Cheng Fang,Keyue Jiang,Yuqian Zheng,Yanghua Xiao,Baojian Zhou,Qifang Zhao,Suhang Zheng,Xiuwen Zhu,Jiyang Tang,Yongchi Zhao,Yijia Luo,Zhiqi Bai,Yuchi Xu,Wenbo Su,Wei Wang,Bing Zhao,Lin Qu,Xiaoxiao Xu*

Main category: cs.AI

TL;DR: Logics-STEM是一个在10M规模高质量数据集上微调的推理模型，专注于STEM领域，在8B规模上相比次优模型平均提升4.68%性能，通过数据算法协同设计实现优化。


<details>
  <summary>Details</summary>
Motivation: 现有推理模型在STEM领域的表现仍有提升空间，需要大规模高质量的长链思维语料库来增强模型的推理能力，同时需要数据与算法的协同优化来更好地拟合推理任务的黄金分布。

Method: 采用数据算法协同设计引擎：数据方面，通过5阶段数据整理引擎（标注、去重、去污染、蒸馏、分层采样）构建10M规模的Logics-STEM-SFT-Dataset；算法方面，使用失败驱动的后训练框架，在SFT阶段针对模型失败区域进行针对性知识检索和数据合成，指导第二阶段SFT或强化学习。

Result: Logics-STEM在STEM相关基准测试中表现出色，在8B规模上相比次优模型平均提升4.68%性能，验证了大规模开源数据与精心设计合成数据结合的巨大潜力。

Conclusion: 数据算法协同设计通过后训练显著提升推理能力，Logics-STEM模型（8B和32B）及数据集（10M和2.2M版本）已开源，支持开源社区未来研究。

Abstract: We present Logics-STEM, a state-of-the-art reasoning model fine-tuned on Logics-STEM-SFT-Dataset, a high-quality and diverse dataset at 10M scale that represents one of the largest-scale open-source long chain-of-thought corpora. Logics-STEM targets reasoning tasks in the domains of Science, Technology, Engineering, and Mathematics (STEM), and exhibits exceptional performance on STEM-related benchmarks with an average improvement of 4.68% over the next-best model at 8B scale. We attribute the gains to our data-algorithm co-design engine, where they are jointly optimized to fit a gold-standard distribution behind reasoning. Data-wise, the Logics-STEM-SFT-Dataset is constructed from a meticulously designed data curation engine with 5 stages to ensure the quality, diversity, and scalability, including annotation, deduplication, decontamination, distillation, and stratified sampling. Algorithm-wise, our failure-driven post-training framework leverages targeted knowledge retrieval and data synthesis around model failure regions in the Supervised Fine-tuning (SFT) stage to effectively guide the second-stage SFT or the reinforcement learning (RL) for better fitting the target distribution. The superior empirical performance of Logics-STEM reveals the vast potential of combining large-scale open-source data with carefully designed synthetic data, underscoring the critical role of data-algorithm co-design in enhancing reasoning capabilities through post-training. We make both the Logics-STEM models (8B and 32B) and the Logics-STEM-SFT-Dataset (10M and downsampled 2.2M versions) publicly available to support future research in the open-source community.

</details>


### [113] [Structured Decomposition for LLM Reasoning: Cross-Domain Validation and Semantic Web Integration](https://arxiv.org/abs/2601.01609)
*Albert Sadowski,Jarosław A. Chudziak*

Main category: cs.AI

TL;DR: 本文提出了一种结合LLMs和符号推理的框架，将非结构化文本转换为ABox断言，然后应用SWRL规则进行确定性推理，在三个领域验证了该方法优于few-shot提示


<details>
  <summary>Details</summary>
Motivation: 在需要可审计和可解释决策的领域（如临床协议、法律证据规则、科学标准），需要同时具备解释灵活性和形式化保证。LLMs提供灵活性但无法保证规则应用的一致性，符号系统提供保证但需要结构化输入

Method: 提出集成模式：LLMs作为本体填充引擎，将非结构化文本转换为ABox断言（基于专家编写的TBox规范），SWRL推理器应用规则提供确定性保证。框架将推理分解为实体识别、断言提取和符号验证，任务定义基于OWL 2本体

Result: 在三个领域（法律传闻确定、科学方法任务应用、临床试验资格）和11个语言模型上验证了该方法。结构化分解在总体上比few-shot提示有统计显著改进，三个领域都有提升。消融研究确认符号验证比仅结构化提示有实质性好处

Conclusion: 填充的ABox可与标准语义网工具集成进行检查和查询，为更丰富的推理模式奠定基础，这是简单形式化无法表达的。该框架结合了LLMs的灵活性和符号推理的确定性保证

Abstract: Rule-based reasoning over natural language input arises in domains where decisions must be auditable and justifiable: clinical protocols specify eligibility criteria in prose, evidence rules define admissibility through textual conditions, and scientific standards dictate methodological requirements. Applying rules to such inputs demands both interpretive flexibility and formal guarantees. Large language models (LLMs) provide flexibility but cannot ensure consistent rule application; symbolic systems provide guarantees but require structured input. This paper presents an integration pattern that combines these strengths: LLMs serve as ontology population engines, translating unstructured text into ABox assertions according to expert-authored TBox specifications, while SWRL-based reasoners apply rules with deterministic guarantees. The framework decomposes reasoning into entity identification, assertion extraction, and symbolic verification, with task definitions grounded in OWL 2 ontologies. Experiments across three domains (legal hearsay determination, scientific method-task application, clinical trial eligibility) and eleven language models validate the approach. Structured decomposition achieves statistically significant improvements over few-shot prompting in aggregate, with gains observed across all three domains. An ablation study confirms that symbolic verification provides substantial benefit beyond structured prompting alone. The populated ABox integrates with standard semantic web tooling for inspection and querying, positioning the framework for richer inference patterns that simpler formalisms cannot express.

</details>


### [114] [Yuan3.0 Flash: An Open Multimodal Large Language Model for Enterprise Applications](https://arxiv.org/abs/2601.01718)
*YuanLab. ai,:,Shawn Wu,Sean Wang,Louie Li,Darcy Chen,Allen Wang,Jiangang Luo,Xudong Zhao,Joseph Shen,Gawain Ma,Jasper Jia,Marcus Mao,Claire Wang,Hunter He,Carol Wang,Zera Zhang,Jason Wang,Chonly Shen,Leo Zhang,Logan Chen,Qasim Meng,James Gong,Danied Zhao,Penn Zheng,Owen Zhu,Tong Yu*

Main category: cs.AI

TL;DR: Yuan3.0 Flash是一个开源的MoE多模态大语言模型，具有3.7B激活参数和40B总参数，专为企业任务优化，同时保持通用任务竞争力。通过RAPO算法解决了大推理模型的过度思考问题。


<details>
  <summary>Details</summary>
Motivation: 解决大推理模型中常见的"过度思考"现象，同时开发一个既能优化企业任务性能又能保持通用任务竞争力的高效多模态大语言模型。

Method: 1. 采用混合专家架构，3.7B激活参数/40B总参数；2. 提出Reflection-aware Adaptive Policy Optimization算法来调节过度思考行为；3. 针对企业任务如RAG、复杂表格理解和摘要进行专门优化。

Result: 在企业任务（RAG、表格理解、摘要）上表现优异，在数学、科学等推理领域也展现出强大能力，达到前沿模型相当的准确度，同时仅需约1/4到1/2的平均token数量。

Conclusion: Yuan3.0 Flash是一个高效的企业级多模态大语言模型，通过RAPO算法有效解决了过度思考问题，在企业任务和通用推理任务上都表现出色，已完全开源供研究和实际部署。

Abstract: We introduce Yuan3.0 Flash, an open-source Mixture-of-Experts (MoE) MultiModal Large Language Model featuring 3.7B activated parameters and 40B total parameters, specifically designed to enhance performance on enterprise-oriented tasks while maintaining competitive capabilities on general-purpose tasks. To address the overthinking phenomenon commonly observed in Large Reasoning Models (LRMs), we propose Reflection-aware Adaptive Policy Optimization (RAPO), a novel RL training algorithm that effectively regulates overthinking behaviors. In enterprise-oriented tasks such as retrieval-augmented generation (RAG), complex table understanding, and summarization, Yuan3.0 Flash consistently achieves superior performance. Moreover, it also demonstrates strong reasoning capabilities in domains such as mathematics, science, etc., attaining accuracy comparable to frontier model while requiring only approximately 1/4 to 1/2 of the average tokens. Yuan3.0 Flash has been fully open-sourced to facilitate further research and real-world deployment: https://github.com/Yuan-lab-LLM/Yuan3.0.

</details>


### [115] [AI Agent Systems: Architectures, Applications, and Evaluation](https://arxiv.org/abs/2601.01743)
*Bin Xu*

Main category: cs.AI

TL;DR: 本文对AI智能体架构进行了系统性综述，涵盖推理、规划、工具调用等核心组件，提出了统一的分类体系，并讨论了设计权衡、评估挑战和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型与推理、规划、记忆和工具使用相结合，AI智能体正在成为自然语言意图与真实世界计算之间的实用接口。本文旨在对这一新兴领域进行系统性梳理和综合。

Method: 通过文献综述方法，将现有工作组织成统一的分类体系，涵盖智能体组件（策略/LLM核心、记忆、世界模型、规划器、工具路由器、批评器）、编排模式（单智能体vs.多智能体；集中式vs.去中心化协调）和部署设置（离线分析vs.在线交互辅助；安全关键vs.开放任务）。

Result: 提出了AI智能体架构的全面分类框架，识别了关键设计权衡（延迟vs.准确性、自主性vs.可控性、能力vs.可靠性），并指出了评估面临的挑战（非确定性、长期信用分配、工具和环境变异性、隐藏成本）。

Conclusion: AI智能体架构领域正在快速发展，但仍面临验证和护栏、可扩展的内存和上下文管理、决策可解释性以及真实工作负载下的可重复评估等开放挑战，需要进一步研究解决。

Abstract: AI agents -- systems that combine foundation models with reasoning, planning, memory, and tool use -- are rapidly becoming a practical interface between natural-language intent and real-world computation. This survey synthesizes the emerging landscape of AI agent architectures across: (i) deliberation and reasoning (e.g., chain-of-thought-style decomposition, self-reflection and verification, and constraint-aware decision making), (ii) planning and control (from reactive policies to hierarchical and multi-step planners), and (iii) tool calling and environment interaction (retrieval, code execution, APIs, and multimodal perception). We organize prior work into a unified taxonomy spanning agent components (policy/LLM core, memory, world models, planners, tool routers, and critics), orchestration patterns (single-agent vs.\ multi-agent; centralized vs.\ decentralized coordination), and deployment settings (offline analysis vs.\ online interactive assistance; safety-critical vs.\ open-ended tasks). We discuss key design trade-offs -- latency vs.\ accuracy, autonomy vs.\ controllability, and capability vs.\ reliability -- and highlight how evaluation is complicated by non-determinism, long-horizon credit assignment, tool and environment variability, and hidden costs such as retries and context growth. Finally, we summarize measurement and benchmarking practices (task suites, human preference and utility metrics, success under constraints, robustness and security) and identify open challenges including verification and guardrails for tool actions, scalable memory and context management, interpretability of agent decisions, and reproducible evaluation under realistic workloads.

</details>


### [116] [A New Benchmark for the Appropriate Evaluation of RTL Code Optimization](https://arxiv.org/abs/2601.01765)
*Yao Lu,Shang Liu,Hangan Zhou,Wenji Fang,Qijun Zhang,Zhiyao Xie*

Main category: cs.AI

TL;DR: RTL-OPT是一个用于评估大语言模型在RTL代码优化能力的新基准测试，包含36个手工设计的数字电路，专注于评估功耗、性能和面积（PPA）优化质量而非仅语法正确性。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要评估RTL代码的语法正确性，而忽略了优化质量（PPA）。随着AI在集成电路设计中的应用增加，需要评估LLM在硬件设计优化方面的实际能力。

Method: 创建了包含36个手工数字设计的RTL-OPT基准，涵盖组合逻辑、流水线数据通路、有限状态机和存储器接口等类别。每个任务提供次优版本和人工优化的参考版本，并集成了自动化评估框架来验证功能正确性和量化PPA改进。

Result: RTL-OPT提供了标准化的评估框架，能够验证功能正确性并量化PPA改进，为生成模型在硬件设计优化方面的评估提供了有意义的基准。

Conclusion: RTL-OPT填补了现有基准测试的空白，专注于评估LLM在RTL优化方面的实际能力，为硬件设计优化的生成模型提供了标准化评估工具。

Abstract: The rapid progress of artificial intelligence increasingly relies on efficient integrated circuit (IC) design. Recent studies have explored the use of large language models (LLMs) for generating Register Transfer Level (RTL) code, but existing benchmarks mainly evaluate syntactic correctness rather than optimization quality in terms of power, performance, and area (PPA). This work introduces RTL-OPT, a benchmark for assessing the capability of LLMs in RTL optimization. RTL-OPT contains 36 handcrafted digital designs that cover diverse implementation categories including combinational logic, pipelined datapaths, finite state machines, and memory interfaces. Each task provides a pair of RTL codes, a suboptimal version and a human-optimized reference that reflects industry-proven optimization patterns not captured by conventional synthesis tools. Furthermore, RTL-OPT integrates an automated evaluation framework to verify functional correctness and quantify PPA improvements, enabling standardized and meaningful assessment of generative models for hardware design optimization.

</details>


### [117] [Can Large Language Models Solve Engineering Equations? A Systematic Comparison of Direct Prediction and Solver-Assisted Approaches](https://arxiv.org/abs/2601.01774)
*Sai Varun Kodathala,Rakesh Vunnam*

Main category: cs.AI

TL;DR: 论文系统评估了大型语言模型在求解超越方程方面的能力，比较了直接数值预测与结合传统迭代求解器的混合架构的效果。研究发现混合方法显著降低了误差，表明LLMs更适合作为传统数值求解器的智能接口而非独立计算引擎。


<details>
  <summary>Details</summary>
Motivation: 超越方程在工程实践中普遍存在，需要迭代数值求解。研究旨在评估大型语言模型能否直接求解这些方程，或者结合传统迭代求解器的混合架构是否更有效。

Method: 测试了6个最先进的LLM模型（GPT-5.1, GPT-5.2, Gemini-3-Flash, Gemini-2.5-Lite, Claude-Sonnet-4.5, Claude-Opus-4.5），在7个工程领域的100个问题上比较了两种方法：1）直接数值预测；2）求解器辅助计算（LLMs制定控制方程和提供初始条件，牛顿-拉弗森迭代执行数值求解）。

Result: 直接预测的平均相对误差为0.765到1.262，而求解器辅助计算达到0.225到0.301，误差减少了67.9%到81.8%。领域分析显示电子学改进最大（93.1%），流体力学改进最小（7.2%）。

Conclusion: 当代LLMs擅长符号操作和领域知识检索，但在精度关键的迭代算术方面表现不佳。它们最适合作为传统数值求解器的智能接口，而不是独立的计算引擎。

Abstract: Transcendental equations requiring iterative numerical solution pervade engineering practice, from fluid mechanics friction factor calculations to orbital position determination. We systematically evaluate whether Large Language Models can solve these equations through direct numerical prediction or whether a hybrid architecture combining LLM symbolic manipulation with classical iterative solvers proves more effective. Testing six state-of-the-art models (GPT-5.1, GPT-5.2, Gemini-3-Flash, Gemini-2.5-Lite, Claude-Sonnet-4.5, Claude-Opus-4.5) on 100 problems spanning seven engineering domains, we compare direct prediction against solver-assisted computation where LLMs formulate governing equations and provide initial conditions while Newton-Raphson iteration performs numerical solution. Direct prediction yields mean relative errors of 0.765 to 1.262 across models, while solver-assisted computation achieves 0.225 to 0.301, representing error reductions of 67.9% to 81.8%. Domain-specific analysis reveals dramatic improvements in Electronics (93.1%) due to exponential equation sensitivity, contrasted with modest gains in Fluid Mechanics (7.2%) where LLMs exhibit effective pattern recognition. These findings establish that contemporary LLMs excel at symbolic manipulation and domain knowledge retrieval but struggle with precision-critical iterative arithmetic, suggesting their optimal deployment as intelligent interfaces to classical numerical solvers rather than standalone computational engines.

</details>


### [118] [Admissibility Alignment](https://arxiv.org/abs/2601.01816)
*Chris Duffey*

Main category: cs.AI

TL;DR: 本文提出"可容许对齐"概念，将AI对齐重新定义为在不确定性下对结果分布的可容许行动和决策选择属性，并通过MAP-AI系统架构实现基于蒙特卡洛估计的对齐评估。


<details>
  <summary>Details</summary>
Motivation: 传统AI对齐方法通常基于静态或二元条件，难以处理不确定性、干预效应、价值模糊性和治理约束。需要一种能够评估策略在不确定性下分布行为的对齐框架。

Method: 提出MAP-AI系统架构，通过蒙特卡洛估计结果分布和可容许控制策略选择来实施对齐。框架评估决策策略在多个可能未来场景中的表现，明确建模不确定性、干预效应、价值模糊性和治理约束。

Result: 建立了基于分布属性的对齐评估方法，包括期望效用、方差、尾部风险和不对齐概率。提供了一种可执行的方法论，用于评估企业和机构AI系统中的信任和对齐。

Conclusion: 可容许对齐为治理AI系统提供了实用基础，其影响不是由单个预测决定，而是由策略在分布和尾部事件中的行为决定。该方法可以在不重新训练或修改底层模型的情况下，改变策略在不确定性下的行为。

Abstract: This paper introduces Admissibility Alignment: a reframing of AI alignment as a property of admissible action and decision selection over distributions of outcomes under uncertainty, evaluated through the behavior of candidate policies. We present MAP-AI (Monte Carlo Alignment for Policy) as a canonical system architecture for operationalizing admissibility alignment, formalizing alignment as a probabilistic, decision-theoretic property rather than a static or binary condition.
  MAP-AI, a new control-plane system architecture for aligned decision-making under uncertainty, enforces alignment through Monte Carlo estimation of outcome distributions and admissibility-controlled policy selection rather than static model-level constraints. The framework evaluates decision policies across ensembles of plausible futures, explicitly modeling uncertainty, intervention effects, value ambiguity, and governance constraints. Alignment is assessed through distributional properties including expected utility, variance, tail risk, and probability of misalignment rather than accuracy or ranking performance. This approach distinguishes probabilistic prediction from decision reasoning under uncertainty and provides an executable methodology for evaluating trust and alignment in enterprise and institutional AI systems. The result is a practical foundation for governing AI systems whose impact is determined not by individual forecasts, but by policy behavior across distributions and tail events. Finally, we show how distributional alignment evaluation can be integrated into decision-making itself, yielding an admissibility-controlled action selection mechanism that alters policy behavior under uncertainty without retraining or modifying underlying models.

</details>


### [119] [COMPASS: A Framework for Evaluating Organization-Specific Policy Alignment in LLMs](https://arxiv.org/abs/2601.01836)
*Dasol Choi,DongGeon Lee,Brigitta Jesica Kartono,Helena Berndt,Taeyoun Kwon,Joonwon Jang,Haon Park,Hwanjo Yu,Minsuk Kahng*

Main category: cs.AI

TL;DR: COMPASS框架首次系统评估LLMs是否符合组织政策，发现模型在合法请求上表现良好（>95%准确率），但在禁止性政策上严重失效（仅拒绝13-40%的违规请求），揭示当前LLMs缺乏政策关键部署所需的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在医疗、金融等高风险企业应用中的部署，确保模型遵守组织特定政策变得至关重要。然而现有的安全评估只关注通用危害，缺乏针对组织政策的系统性评估框架。

Method: 提出COMPASS（公司/组织政策对齐评估）框架，应用于八个不同行业场景，生成并验证了5,920个查询，测试常规合规性和通过策略设计的边界案例进行对抗性鲁棒性评估。

Result: 评估七个最先进模型发现根本性不对称：模型可靠处理合法请求（>95%准确率），但在执行禁令方面灾难性失败，仅拒绝13-40%的对抗性禁止列表违规请求。

Conclusion: 当前LLMs缺乏政策关键部署所需的鲁棒性，COMPASS框架成为组织AI安全评估的重要工具，填补了现有安全评估的空白。

Abstract: As large language models are deployed in high-stakes enterprise applications, from healthcare to finance, ensuring adherence to organization-specific policies has become essential. Yet existing safety evaluations focus exclusively on universal harms. We present COMPASS (Company/Organization Policy Alignment Assessment), the first systematic framework for evaluating whether LLMs comply with organizational allowlist and denylist policies. We apply COMPASS to eight diverse industry scenarios, generating and validating 5,920 queries that test both routine compliance and adversarial robustness through strategically designed edge cases. Evaluating seven state-of-the-art models, we uncover a fundamental asymmetry: models reliably handle legitimate requests (>95% accuracy) but catastrophically fail at enforcing prohibitions, refusing only 13-40% of adversarial denylist violations. These results demonstrate that current LLMs lack the robustness required for policy-critical deployments, establishing COMPASS as an essential evaluation framework for organizational AI safety.

</details>


### [120] [Clinical Knowledge Graph Construction and Evaluation with Multi-LLMs via Retrieval-Augmented Generation](https://arxiv.org/abs/2601.01844)
*Udiptaman Das,Krishnasai B. Atmakuri,Duy Ho,Chi Lee,Yugyung Lee*

Main category: cs.AI

TL;DR: 提出一个端到端框架，使用多智能体提示和模式约束的检索增强生成策略，直接从自由文本构建临床知识图谱，特别针对肿瘤学领域。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常依赖结构化输入，缺乏对事实准确性和语义一致性的鲁棒验证，这在肿瘤学领域尤其成问题。大语言模型为从非结构化临床叙述构建知识图谱提供了新机会。

Method: 采用多智能体提示和模式约束的检索增强生成策略，包括：1）提示驱动的实体、属性和关系提取；2）基于熵的不确定性评分；3）本体对齐的RDF/OWL模式生成；4）多LLM共识验证用于幻觉检测和语义细化。

Result: 应用于两个肿瘤学队列（PDAC和BRCA），该方法在不依赖黄金标准标注的情况下，产生了可解释、SPARQL兼容且临床基础的知识图谱。实验结果显示在精确度、相关性和本体合规性方面相比基线方法有持续提升。

Conclusion: 该框架支持连续细化和自监督评估，实现了图谱质量的迭代改进，为直接从自由文本构建临床知识图谱提供了一种有效方法。

Abstract: Large language models (LLMs) offer new opportunities for constructing knowledge graphs (KGs) from unstructured clinical narratives. However, existing approaches often rely on structured inputs and lack robust validation of factual accuracy and semantic consistency, limitations that are especially problematic in oncology. We introduce an end-to-end framework for clinical KG construction and evaluation directly from free text using multi-agent prompting and a schema-constrained Retrieval-Augmented Generation (KG-RAG) strategy. Our pipeline integrates (1) prompt-driven entity, attribute, and relation extraction; (2) entropy-based uncertainty scoring; (3) ontology-aligned RDF/OWL schema generation; and (4) multi-LLM consensus validation for hallucination detection and semantic refinement. Beyond static graph construction, the framework supports continuous refinement and self-supervised evaluation, enabling iterative improvement of graph quality. Applied to two oncology cohorts (PDAC and BRCA), our method produces interpretable, SPARQL-compatible, and clinically grounded knowledge graphs without relying on gold-standard annotations. Experimental results demonstrate consistent gains in precision, relevance, and ontology compliance over baseline methods.

</details>


### [121] [Jenius Agent: Towards Experience-Driven Accuracy Optimization in Real-World Scenarios](https://arxiv.org/abs/2601.01857)
*Defei Xia,Bingfeng Pi,Shenbin Zhang,Song Hua,Yunfei Wei,Lei Zuo*

Main category: cs.AI

TL;DR: 本文提出Jenius-Agent框架，通过自适应提示生成、上下文感知工具编排和分层内存机制三大创新，提升LLM智能体的任务准确性20%，同时降低token成本、响应延迟和调用失败率。


<details>
  <summary>Details</summary>
Motivation: 随着基于大语言模型的智能体系统发展，提升自主智能体的任务性能（特别是在上下文理解、工具使用和响应生成方面）变得日益重要。尽管先前研究已推进了LLM智能体的整体设计，但对其内部推理和工具使用流程的系统优化仍未被充分探索。

Method: 提出基于真实世界实践经验的智能体框架，包含三大关键创新：1) 自适应提示生成策略，根据智能体状态和任务目标调整提示以提高可靠性和鲁棒性；2) 上下文感知工具编排模块，基于用户意图和上下文进行工具分类、语义检索和自适应调用；3) 分层内存机制，集成会话内存、任务历史和外部摘要，通过动态摘要和压缩提高相关性和效率。框架集成了基于模型上下文协议(MCP)的工具、文件输入/输出和执行反馈三大优化。

Result: 实验显示任务准确性提升20%，同时降低了token成本、响应延迟和调用失败率。该框架已在Jenius平台部署，为鲁棒、协议兼容的自主智能体提供了轻量级、可扩展的解决方案。

Conclusion: Jenius-Agent框架通过系统优化智能体的内部推理和工具使用流程，显著提升了任务性能，为构建高效、可靠的自主智能体提供了实用解决方案。

Abstract: As agent systems powered by large language models (LLMs) advance, improving the task performance of an autonomous agent, especially in context understanding, tool usage, and response generation, has become increasingly critical. Although prior studies have advanced the overall design of LLM-based agents, systematic optimization of their internal reasoning and tool-use pipelines remains underexplored. This paper introduces an agent framework grounded in real-world practical experience, with three key innovations: (1) an adaptive prompt generation strategy that aligns with the agent's state and task goals to improve reliability and robustness; (2) a context-aware tool orchestration module that performs tool categorization, semantic retrieval, and adaptive invocation based on user intent and context; and (3) a layered memory mechanism that integrates session memory, task history, and external summaries to improve relevance and efficiency through dynamic summarization and compression. An end-to-end framework named Jenius-Agent has been integrated with three key optimizations, including tools based on the Model Context Protocol (MCP), file input/output (I/O), and execution feedback. The experiments show a 20 percent improvement in task accuracy, along with a reduced token cost, response latency, and invocation failures. The framework is already deployed in Jenius (https://www.jenius.cn), providing a lightweight and scalable solution for robust, protocol-compatible autonomous agents.

</details>


### [122] [Theory Trace Card: Theory-Driven Socio-Cognitive Evaluation of LLMs](https://arxiv.org/abs/2601.01878)
*Farzan Karimi-Malekabadi,Suhaib Abdurahman,Zhivar Sourati,Jackson Trager,Morteza Dehghani*

Main category: cs.AI

TL;DR: 论文指出大语言模型的社会认知基准测试存在评估-部署差距，根本原因是缺乏明确的理论基础，导致窄任务表现被误读为广泛能力。作者提出理论追踪卡来解决这一问题。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的社会认知基准测试虽然得分高，但无法预测真实世界行为。现有研究将这种评估-部署差距归因于测量和效度问题，但作者认为更深层原因是缺乏明确的理论基础，导致窄任务表现被过度泛化为广泛能力。

Method: 1. 诊断并形式化"理论缺口"问题，指出这是破坏测量并导致基准结果系统性过度泛化的根本原因。2. 引入理论追踪卡（TTC），这是一种轻量级文档工具，明确记录评估的理论基础、目标能力组件、操作化过程及其局限性。

Result: 理论追踪卡通过明确理论、任务操作化、评分和局限性之间的完整效度链，增强了社会认知评估的可解释性和可重用性，无需修改基准测试或要求单一理论共识。

Conclusion: 社会认知评估需要明确的理论基础来避免系统性效度错觉。理论追踪卡提供了一种实用方法，通过记录评估的理论假设和局限性，提高基准测试的透明度和解释性，从而更准确地评估大语言模型的真实能力。

Abstract: Socio-cognitive benchmarks for large language models (LLMs) often fail to predict real-world behavior, even when models achieve high benchmark scores. Prior work has attributed this evaluation-deployment gap to problems of measurement and validity. While these critiques are insightful, we argue that they overlook a more fundamental issue: many socio-cognitive evaluations proceed without an explicit theoretical specification of the target capability, leaving the assumptions linking task performance to competence implicit. Without this theoretical grounding, benchmarks that exercise only narrow subsets of a capability are routinely misinterpreted as evidence of broad competence: a gap that creates a systemic validity illusion by masking the failure to evaluate the capability's other essential dimensions. To address this gap, we make two contributions. First, we diagnose and formalize this theory gap as a foundational failure that undermines measurement and enables systematic overgeneralization of benchmark results. Second, we introduce the Theory Trace Card (TTC), a lightweight documentation artifact designed to accompany socio-cognitive evaluations, which explicitly outlines the theoretical basis of an evaluation, the components of the target capability it exercises, its operationalization, and its limitations. We argue that TTCs enhance the interpretability and reuse of socio-cognitive evaluations by making explicit the full validity chain, which links theory, task operationalization, scoring, and limitations, without modifying benchmarks or requiring agreement on a single theory.

</details>


### [123] [MMP-A*: Multimodal Perception Enhanced Incremental Heuristic Search on Path Planning](https://arxiv.org/abs/2601.01910)
*Minh Hieu Ha,Khanh Ly Ta,Hung Phan,Tung Doan,Tung Dao,Dao Tran,Huynh Thi Thanh Binh*

Main category: cs.AI

TL;DR: MMP-A*是一个多模态路径规划框架，结合视觉语言模型的空间定位能力和自适应衰减机制，在复杂环境中实现接近最优的轨迹规划，同时大幅降低计算和内存开销。


<details>
  <summary>Details</summary>
Motivation: 传统A*算法在大规模场景中计算和内存成本过高，而最近使用大语言模型进行路径点引导的方法仅依赖文本推理，缺乏空间定位能力，在拓扑复杂环境中容易产生错误路径点，无法解释模糊的物理边界，导致昂贵的纠正扩展并破坏计算效率。

Method: 提出MMP-A*多模态框架，整合视觉语言模型的空间定位能力与新型自适应衰减机制。该框架将高层推理锚定在物理几何中，产生连贯的路径点引导。自适应衰减机制动态调节不确定路径点在启发式函数中的影响，确保几何有效性同时大幅减少内存开销。

Result: 在具有严重杂乱和拓扑复杂性的挑战性环境中测试，实验结果表明MMP-A*实现了接近最优的轨迹规划，同时显著降低了操作成本。

Conclusion: MMP-A*展示了作为自主导航的感知基础和计算高效范式的潜力，通过整合空间定位能力和自适应机制，解决了纯文本规划器的局限性。

Abstract: Autonomous path planning requires a synergy between global reasoning and geometric precision, especially in complex or cluttered environments. While classical A* is valued for its optimality, it incurs prohibitive computational and memory costs in large-scale scenarios. Recent attempts to mitigate these limitations by using Large Language Models for waypoint guidance remain insufficient, as they rely only on text-based reasoning without spatial grounding. As a result, such models often produce incorrect waypoints in topologically complex environments with dead ends, and lack the perceptual capacity to interpret ambiguous physical boundaries. These inconsistencies lead to costly corrective expansions and undermine the intended computational efficiency.
  We introduce MMP-A*, a multimodal framework that integrates the spatial grounding capabilities of vision-language models with a novel adaptive decay mechanism. By anchoring high-level reasoning in physical geometry, the framework produces coherent waypoint guidance that addresses the limitations of text-only planners. The adaptive decay mechanism dynamically regulates the influence of uncertain waypoints within the heuristic, ensuring geometric validity while substantially reducing memory overhead. To evaluate robustness, we test the framework in challenging environments characterized by severe clutter and topological complexity. Experimental results show that MMP-A* achieves near-optimal trajectories with significantly reduced operational costs, demonstrating its potential as a perception-grounded and computationally efficient paradigm for autonomous navigation.

</details>


### [124] [OpenSocInt: A Multi-modal Training Environment for Human-Aware Social Navigation](https://arxiv.org/abs/2601.01939)
*Victor Sanchez,Chris Reinke,Ahamed Mohamed,Xavier Alameda-Pineda*

Main category: cs.AI

TL;DR: OpenSocInt是一个开源的多模态社交交互模拟器软件包，提供模块化架构用于训练社交智能体，已应用于社交导航任务


<details>
  <summary>Details</summary>
Motivation: 开发一个开源的多模态社交交互模拟框架，支持探索不同感知特征、编码融合方式以及多种智能体类型，促进社交智能研究

Method: 创建了OpenSocInt开源软件包，包含多模态社交交互模拟器和模块化架构，支持不同感知特征的编码与融合，以及多种智能体的训练

Result: 软件已公开发布在GitLab上（GPL许可），并通过社交导航任务的实验协议展示了其应用价值

Conclusion: OpenSocInt为多模态社交交互研究提供了一个灵活的开源平台，支持探索不同感知特征和智能体架构，有助于推动社交智能领域的发展

Abstract: In this paper, we introduce OpenSocInt, an open-source software package providing a simulator for multi-modal social interactions and a modular architecture to train social agents. We described the software package and showcased its interest via an experimental protocol based on the task of social navigation. Our framework allows for exploring the use of different perceptual features, their encoding and fusion, as well as the use of different agents. The software is already publicly available under GPL at https://gitlab.inria.fr/robotlearn/OpenSocInt/.

</details>


### [125] [CNC-TP: Classifier Nominal Concept Based on Top-Pertinent Attributes](https://arxiv.org/abs/2601.01976)
*Yasmine Souissi,Fabrice Boissier,Nida Meddouri*

Main category: cs.AI

TL;DR: 本文对基于形式概念分析（FCA）的分类器进行了综述，提出了一种从名义数据计算闭包算子的新方法，并构建了专注于最相关概念的部分概念格。


<details>
  <summary>Details</summary>
Motivation: 知识发现（KDD）旨在从海量数据中提取隐藏的有意义知识，其中分类是核心数据挖掘技术之一。形式概念分析（FCA）作为一种可解释和可解释学习的方法，基于概念格的数学结构，能够生成形式概念并发现隐藏关系，因此值得深入研究其分类器应用。

Method: 1. 对基于FCA的分类器进行最新综述；2. 探索从名义数据计算闭包算子的各种方法；3. 提出一种构建部分概念格的新方法，专注于最相关概念。

Result: 通过实验验证了所提方法的效率，展示了该方法在实际应用中的有效性。

Conclusion: FCA作为一种可解释的分类方法具有重要价值，提出的部分概念格构建方法能够有效提高分类效率，为知识发现领域提供了新的技术途径。

Abstract: Knowledge Discovery in Databases (KDD) aims to exploit the vast amounts of data generated daily across various domains of computer applications. Its objective is to extract hidden and meaningful knowledge from datasets through a structured process comprising several key steps: data selection, preprocessing, transformation, data mining, and visualization. Among the core data mining techniques are classification and clustering. Classification involves predicting the class of new instances using a classifier trained on labeled data. Several approaches have been proposed in the literature, including Decision Tree Induction, Bayesian classifiers, Nearest Neighbor search, Neural Networks, Support Vector Machines, and Formal Concept Analysis (FCA). The last one is recognized as an effective approach for interpretable and explainable learning. It is grounded in the mathematical structure of the concept lattice, which enables the generation of formal concepts and the discovery of hidden relationships among them. In this paper, we present a state-of-theart review of FCA-based classifiers. We explore various methods for computing closure operators from nominal data and introduce a novel approach for constructing a partial concept lattice that focuses on the most relevant concepts. Experimental results are provided to demonstrate the efficiency of the proposed method.

</details>


### [126] [ChaosBench-Logic: A Benchmark for Logical and Symbolic Reasoning on Chaotic Dynamical Systems](https://arxiv.org/abs/2601.01982)
*Noel Thomas*

Main category: cs.AI

TL;DR: ChaosBench-Logic是一个评估大语言模型在混沌动力系统中逻辑推理能力的基准测试，包含30个系统、11个语义谓词和621个问题，涵盖7种推理类型，发现前沿LLMs在单项准确率上达到91-94%，但在组合推理上得分为0%，对话准确率在53.1%-75.5%之间。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在自然语言任务上表现出色，但在需要精确逻辑和符号推理的领域仍然脆弱。混沌动力系统提供了一个特别苛刻的测试环境，因为混沌是确定性的，但经常被误解为随机性或复杂性。需要建立一个基准来评估LLMs在这种复杂逻辑推理任务上的能力。

Method: 引入ChaosBench-Logic基准，使用统一的一阶逻辑本体论评估30个不同的动力系统。每个系统用11个语义谓词的真值分配进行标注，生成了621个问题，涵盖七个推理类别：多跳蕴含、跨系统类比、反事实推理、偏见探测和多轮对话。定义了逻辑准确性、蕴含一致性、对话连贯性和矛盾性等指标，并发布了开源评估管道。

Result: 前沿LLMs（GPT-4、Claude 3.5 Sonnet、Gemini 2.5 Flash和开源的LLaMA-3 70B）在单项准确率上达到91-94%，但在组合项目上得分为0%，表现出脆弱的全局一致性。对话级准确率从53.1%（GPT-4 CoT）到75.5%（LLaMA-3零样本）不等。

Conclusion: ChaosBench-Logic为诊断LLMs在复杂逻辑推理中的失败提供了一个严格的测试平台，并为开发能够改善LLMs科学推理能力的神经符号方法奠定了基础。虽然LLMs在单项任务上表现良好，但在组合推理和全局一致性方面仍有显著缺陷。

Abstract: Large language models (LLMs) excel at natural language tasks but remain brittle in domains requiring precise logical and symbolic reasoning. Chaotic dynamical systems provide an especially demanding test because chaos is deterministic yet often misinterpreted as randomness or complexity. We introduce ChaosBench-Logic, a benchmark that evaluates LLM reasoning across 30 diverse dynamical systems using a unified first-order logic (FOL) ontology. Each system is annotated with truth assignments for 11 semantic predicates, and 621 questions are generated across seven reasoning categories, including multi-hop implications, cross-system analogies, counterfactual reasoning, bias probes, and multi-turn dialogues. We define metrics for logical accuracy, implication consistency, dialogue coherence, and contradiction, and we release an open-source evaluation pipeline. Initial experiments show that frontier LLMs such as GPT-4, Claude 3.5 Sonnet, Gemini 2.5 Flash, and the open-source LLaMA-3 70B achieve 91-94% per-item accuracy, yet still score 0% on compositional items and exhibit fragile global coherence. Dialogue-level accuracy ranges from 53.1% (GPT-4 CoT) to 75.5% (LLaMA-3 zero-shot). ChaosBench-Logic provides a rigorous testbed for diagnosing such failures and a foundation for developing neuro-symbolic approaches that improve scientific reasoning in LLMs.

</details>


### [127] [MindChat: A Privacy-preserving Large Language Model for Mental Health Support](https://arxiv.org/abs/2601.01993)
*Dong Xue,Jicheng Tu,Ming Wang,Xin Yan,Fangzhou Liu,Jie Hu*

Main category: cs.AI

TL;DR: MindChat是一个保护隐私的心理健康支持大语言模型，配合MindCorpus合成咨询数据集，通过联邦学习和差分隐私技术保护用户隐私，在咨询能力评估中表现优异。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在心理健康支持方面有潜力，但训练受到真实咨询对话稀缺性和敏感性的限制，需要开发既能保护隐私又能提供高质量咨询服务的解决方案。

Method: 1. 使用多智能体角色扮演框架构建MindCorpus合成咨询数据集；2. 采用双闭环反馈设计：回合级批判修订和会话级策略优化；3. 通过联邦学习配合LoRA适配器进行微调；4. 加入差分隐私优化降低成员推断和记忆风险。

Result: MindCorpus提高了训练效果，MindChat在自动LLM评估和人工评估协议下与现有通用和咨询导向的LLM基线竞争，同时在成员推断攻击下表现出减少的隐私泄露。

Conclusion: 该研究提出了一个保护隐私的心理健康支持LLM系统，通过合成数据生成和隐私保护技术的结合，解决了真实咨询数据稀缺和敏感性的问题，为心理健康领域的AI应用提供了可行的解决方案。

Abstract: Large language models (LLMs) have shown promise for mental health support, yet training such models is constrained by the scarcity and sensitivity of real counseling dialogues. In this article, we present MindChat, a privacy-preserving LLM for mental health support, together with MindCorpus, a synthetic multi-turn counseling dataset constructed via a multi-agent role-playing framework. To synthesize high-quality counseling data, the developed dialogue-construction framework employs a dual closed-loop feedback design to integrate psychological expertise and counseling techniques through role-playing: (i) turn-level critique-and-revision to improve coherence and counseling appropriateness within a session, and (ii) session-level strategy refinement to progressively enrich counselor behaviors across sessions. To mitigate privacy risks under decentralized data ownership, we fine-tune the base model using federated learning with parameter-efficient LoRA adapters and incorporate differentially private optimization to reduce membership and memorization risks. Experiments on synthetic-data quality assessment and counseling capability evaluation show that MindCorpus improves training effectiveness and that MindChat is competitive with existing general and counseling-oriented LLM baselines under both automatic LLM-judge and human evaluation protocols, while exhibiting reduced privacy leakage under membership inference attacks.

</details>


### [128] [XAI-MeD: Explainable Knowledge Guided Neuro-Symbolic Framework for Domain Generalization and Rare Class Detection in Medical Imaging](https://arxiv.org/abs/2601.02008)
*Midhat Urooj,Ayan Banerjee,Sandeep Gupta*

Main category: cs.AI

TL;DR: XAIMeD是一个可解释的医疗AI框架，通过神经符号架构整合临床专家知识，提升分布偏移下的鲁棒性、罕见类别敏感性，并提供临床对齐的解释。


<details>
  <summary>Details</summary>
Motivation: 解决医疗AI中的可解释性、领域泛化和罕见类别可靠性等关键挑战，传统深度模型在真实世界分布偏移下表现不佳，且对不常见临床条件存在偏见。

Method: 提出XAIMeD框架，将临床专业知识编码为原子医学命题的逻辑连接，转化为机器可检查的类别特定规则；通过加权特征满足分数量化诊断效用，建立符号推理分支补充神经预测；采用置信度加权融合整合符号和深度输出，以及基于熵不平衡增益和罕见类别基尼系数的自适应路由机制。

Result: 在四个挑战性任务上评估，包括从rs-fMRI定位癫痫发作起始区和跨6个多中心数据集的糖尿病视网膜病变分级，显示显著性能提升：跨领域泛化提高6%，罕见类别F1分数提升10%，远超最先进的深度学习基线。

Conclusion: XAIMeD提供了一个原则性、临床忠实且可解释的多模态医疗AI方法，临床基础的符号组件作为有效的正则化器，确保对分布偏移的鲁棒性。

Abstract: Explainability domain generalization and rare class reliability are critical challenges in medical AI where deep models often fail under real world distribution shifts and exhibit bias against infrequent clinical conditions This paper introduces XAIMeD an explainable medical AI framework that integrates clinically accurate expert knowledge into deep learning through a unified neuro symbolic architecture XAIMeD is designed to improve robustness under distribution shift enhance rare class sensitivity and deliver transparent clinically aligned interpretations The framework encodes clinical expertise as logical connectives over atomic medical propositions transforming them into machine checkable class specific rules Their diagnostic utility is quantified through weighted feature satisfaction scores enabling a symbolic reasoning branch that complements neural predictions A confidence weighted fusion integrates symbolic and deep outputs while a Hunt inspired adaptive routing mechanism guided by Entropy Imbalance Gain EIG and Rare Class Gini mitigates class imbalance high intra class variability and uncertainty We evaluate XAIMeD across diverse modalities on four challenging tasks i Seizure Onset Zone SOZ localization from rs fMRI ii Diabetic Retinopathy grading across 6 multicenter datasets demonstrate substantial performance improvements including 6 percent gains in cross domain generalization and a 10 percent improved rare class F1 score far outperforming state of the art deep learning baselines Ablation studies confirm that the clinically grounded symbolic components act as effective regularizers ensuring robustness to distribution shifts XAIMeD thus provides a principled clinically faithful and interpretable approach to multimodal medical AI.

</details>


### [129] [Simulated Reasoning is Reasoning](https://arxiv.org/abs/2601.02043)
*Hendrik Kempt,Alon Lavie*

Main category: cs.AI

TL;DR: 论文认为基础模型通过模仿"大声思考"过程、测试生成路径并进行迭代，能够实现某种形式的推理，这与传统符号推理不同，缺乏常识基础且具有脆弱性，需要重新评估推理概念和安全考量。


<details>
  <summary>Details</summary>
Motivation: 传统上认为推理是通过符号推理实现理解的路径，但基础模型展示了一种不同的推理方式：通过模仿"大声思考"过程、测试生成路径并进行迭代来解决问题。这种推理方式虽然有效，但缺乏人类推理的常识基础和稳定性，需要重新审视推理的本质及其必要条件。

Method: 本文采用哲学分析方法，探讨基础模型推理现象的不同哲学解释，论证"随机鹦鹉"隐喻已失去相关性，并反思从这些推理模型及其日益增长的能力中产生的安全和适当性考虑的不同规范要素。

Result: 基础模型展示了一种不同于传统符号推理的推理能力，能够通过模仿思考过程、测试和迭代路径来解决问题。这种推理虽然有效但具有脆弱性，缺乏常识基础。"随机鹦鹉"隐喻已不再适用，需要新的概念框架来理解这种推理现象。

Conclusion: 基础模型的推理能力挑战了传统推理概念，需要重新评估推理的必要条件。同时，这种推理的脆弱性对安全防御提出了新要求。论文呼吁放弃过时的"随机鹦鹉"隐喻，建立新的哲学框架来理解和规范这些模型的推理能力及其安全影响。

Abstract: Reasoning has long been understood as a pathway between stages of understanding. Proper reasoning leads to understanding of a given subject. This reasoning was conceptualized as a process of understanding in a particular way, i.e., "symbolic reasoning". Foundational Models (FM) demonstrate that this is not a necessary condition for many reasoning tasks: they can "reason" by way of imitating the process of "thinking out loud", testing the produced pathways, and iterating on these pathways on their own. This leads to some form of reasoning that can solve problems on its own or with few-shot learning, but appears fundamentally different from human reasoning due to its lack of grounding and common sense, leading to brittleness of the reasoning process. These insights promise to substantially alter our assessment of reasoning and its necessary conditions, but also inform the approaches to safety and robust defences against this brittleness of FMs. This paper offers and discusses several philosophical interpretations of this phenomenon, argues that the previously apt metaphor of the "stochastic parrot" has lost its relevance and thus should be abandoned, and reflects on different normative elements in the safety- and appropriateness-considerations emerging from these reasoning models and their growing capacity.

</details>


### [130] [FormuLLA: A Large Language Model Approach to Generating Novel 3D Printable Formulations](https://arxiv.org/abs/2601.02071)
*Adeshola Okubena,Yusuf Ali Mohammed,Moe Elbadawi*

Main category: cs.AI

TL;DR: 该研究探讨了将大型语言模型应用于药物3D打印制剂开发，通过微调四种LLM架构，评估其在基于API剂量推荐辅料和预测长丝机械性能方面的表现，发现Llama2模型最适合FDM制剂辅料推荐，并揭示了小数据集可能导致灾难性遗忘等问题。


<details>
  <summary>Details</summary>
Motivation: 当前AI驱动的药物3D打印研究大多局限于特定领域，未能全面解决制剂开发中的复杂挑战。随着人工通用智能概念的发展，研究者希望探索LLM在药物制剂开发中的应用潜力，特别是利用其超越传统预测建模的通用推理能力。

Method: 研究微调了四种大型语言模型架构，使用包含1400多种制剂的熔融沉积建模数据集。系统评估了微调和生成参数配置，重点关注模型在基于活性药物成分剂量推荐合适辅料以及预测长丝机械性能方面的能力。

Result: 结果显示Llama2模型最适合FDM制剂辅料推荐。模型选择和参数化显著影响性能，较小的LLM表现出灾难性遗忘现象。研究还发现：即使相对较小的1400+制剂数据集也可能导致模型灾难性遗忘；标准LLM指标仅评估语言性能而非制剂可加工性；基于生物医学相关数据训练的LLM不一定产生最佳结果。

Conclusion: 解决这些挑战对于推动LLM超越语言能力、发展成为药物制剂开发的可靠系统至关重要。研究强调了在药物3D打印领域应用LLM时需要克服的关键技术障碍，为未来开发更可靠的AI驱动制剂系统提供了重要见解。

Abstract: Pharmaceutical three-dimensional (3D) printing is an advanced fabrication technology with the potential to enable truly personalised dosage forms. Recent studies have integrated artificial intelligence (AI) to accelerate formulation and process development, drastically transforming current approaches to pharmaceutical 3D printing. To date, most AI-driven efforts remain narrowly focused, while failing to account for the broader formulation challenges inherent to the technology. Recent advances in AI have introduced artificial general intelligence concepts, wherein systems extend beyond conventional predictive modelling toward more generalised, human-like reasoning. In this work, we investigate the application of large language models (LLMs), fine-tuned on a fused deposition modelling (FDM) dataset comprising over 1400 formulations, to recommend suitable excipients based on active pharmaceutical ingredient (API) dose, and predict filament mechanical properties. Four LLM architectures were fine-tuned, with systematic evaluation of both fine-tuning and generative parameter configurations. Our results demonstrate that Llama2 was best suited for recommending excipients for FDM formulations. Additionally, model selection and parameterisation significantly influence performance, with smaller LLMs exhibiting instances of catastrophic forgetting. Furthermore, we demonstrate: (i) even with relatively small dataset of over 1400 formulations, it can lead to model catastrophic forgetting; (ii) standard LLM metrics only evaluate linguistic performance but not formulation processability; and (iii) LLMs trained on biomedically-related data do not always produce the best results. Addressing these challenges is essential to advancing LLMs beyond linguistic proficiency and toward reliable systems for pharmaceutical formulation development.

</details>


### [131] [Streaming Hallucination Detection in Long Chain-of-Thought Reasoning](https://arxiv.org/abs/2601.02170)
*Haolang Lu,Minghui Pan,Ripeng Li,Guoshun Nan,Jialin Zhuang,Zijie Zhao,Zhongxiang Sun,Kun Wang,Yang Liu*

Main category: cs.AI

TL;DR: 该论文提出了一种新的长链思维推理幻觉检测方法，将幻觉视为演化潜状态而非一次性错误事件，通过累积前缀级信号实时跟踪推理状态演化。


<details>
  <summary>Details</summary>
Motivation: 长链思维推理虽然能提升大语言模型性能，但其中的幻觉问题往往以微妙方式出现并在推理步骤间传播。传统方法将幻觉视为一次性错误事件，而作者认为长链推理中的幻觉应被理解为演化中的潜状态，需要更全局的视角来检测。

Method: 将步骤级幻觉判断视为局部观测，引入累积前缀级幻觉信号来跟踪整个推理轨迹上的全局状态演化。这种方法能够在长链思维推理中实现流式幻觉检测，提供实时、可解释的证据。

Result: 该方法能够实时检测长链推理中的幻觉演化，提供可解释的检测证据，相比传统的一次性错误检测方法，能更好地捕捉幻觉在推理过程中的传播和演化特性。

Conclusion: 长链思维推理中的幻觉应被视为演化潜状态而非一次性错误事件，通过累积前缀级信号跟踪全局状态演化的方法能够实现有效的流式幻觉检测，为长链推理的可靠性提供了新的监控框架。

Abstract: Long chain-of-thought (CoT) reasoning improves the performance of large language models, yet hallucinations in such settings often emerge subtly and propagate across reasoning steps. We suggest that hallucination in long CoT reasoning is better understood as an evolving latent state rather than a one-off erroneous event. Accordingly, we treat step-level hallucination judgments as local observations and introduce a cumulative prefix-level hallucination signal that tracks the global evolution of the reasoning state over the entire trajectory. Overall, our approach enables streaming hallucination detection in long CoT reasoning, providing real-time, interpretable evidence.

</details>


### [132] [Project Ariadne: A Structural Causal Framework for Auditing Faithfulness in LLM Agents](https://arxiv.org/abs/2601.02314)
*Sourena Khanzadeh*

Main category: cs.AI

TL;DR: 论文提出Project Ariadne框架，使用结构因果模型和反事实逻辑来审计LLM智能体推理的因果完整性，发现当前智能体存在"因果解耦"问题，推理痕迹只是"推理剧场"而非真实决策驱动。


<details>
  <summary>Details</summary>
Motivation: 随着LLM智能体越来越多地承担高风险自主决策任务，其推理过程的透明度成为关键安全关切。虽然思维链提示允许智能体生成人类可读的推理痕迹，但这些痕迹究竟是模型输出的忠实生成驱动因素还是仅仅是事后合理化，仍不清楚。

Method: 提出Project Ariadne框架，利用结构因果模型和反事实逻辑来审计智能体推理的因果完整性。与依赖表层文本相似性的现有可解释性方法不同，该框架对中间推理节点执行硬干预（do-演算）——系统地反转逻辑、否定前提和反转事实主张——以测量终端答案的因果敏感性。

Result: 对最先进模型的实证评估揭示了持续的"忠实性差距"。定义并检测到一种广泛的故障模式称为"因果解耦"，在事实和科学领域中，智能体表现出高达0.77的违反密度。在这些情况下，智能体尽管内部逻辑矛盾却得出相同结论，证明其推理痕迹只是"推理剧场"，而决策由潜在参数先验控制。

Conclusion: 当前智能体架构本质上容易产生不忠实的解释，提出Ariadne分数作为对齐陈述逻辑与模型行为的新基准。

Abstract: As Large Language Model (LLM) agents are increasingly tasked with high-stakes autonomous decision-making, the transparency of their reasoning processes has become a critical safety concern. While \textit{Chain-of-Thought} (CoT) prompting allows agents to generate human-readable reasoning traces, it remains unclear whether these traces are \textbf{faithful} generative drivers of the model's output or merely \textbf{post-hoc rationalizations}. We introduce \textbf{Project Ariadne}, a novel XAI framework that utilizes Structural Causal Models (SCMs) and counterfactual logic to audit the causal integrity of agentic reasoning. Unlike existing interpretability methods that rely on surface-level textual similarity, Project Ariadne performs \textbf{hard interventions} ($do$-calculus) on intermediate reasoning nodes -- systematically inverting logic, negating premises, and reversing factual claims -- to measure the \textbf{Causal Sensitivity} ($φ$) of the terminal answer. Our empirical evaluation of state-of-the-art models reveals a persistent \textit{Faithfulness Gap}. We define and detect a widespread failure mode termed \textbf{Causal Decoupling}, where agents exhibit a violation density ($ρ$) of up to $0.77$ in factual and scientific domains. In these instances, agents arrive at identical conclusions despite contradictory internal logic, proving that their reasoning traces function as "Reasoning Theater" while decision-making is governed by latent parametric priors. Our findings suggest that current agentic architectures are inherently prone to unfaithful explanation, and we propose the Ariadne Score as a new benchmark for aligning stated logic with model action.

</details>


### [133] [Falcon-H1R: Pushing the Reasoning Frontiers with a Hybrid Model for Efficient Test-Time Scaling](https://arxiv.org/abs/2601.02346)
*Falcon LLM Team,Iheb Chaabane,Puneesh Khanna,Suhail Mohmad,Slim Frikha,Shi Hu,Abdalgader Abubaker,Reda Alami,Mikhail Lubinets,Mohamed El Amine Seddik,Hakim Hacid*

Main category: cs.AI

TL;DR: Falcon-H1R是一个7B参数的推理优化模型，证明了小语言模型也能达到有竞争力的推理性能，在多项推理基准测试中匹配或超越比它大2-7倍的SOTA模型。


<details>
  <summary>Details</summary>
Motivation: 探索小语言模型在推理任务上的潜力，证明通过精心设计的数据管理和训练策略，紧凑模型也能实现强大的推理性能，而不需要增加模型规模。

Method: 采用混合并行架构设计实现快速推理，结合高效的有监督微调（SFT）和强化学习扩展策略，利用DeepConf方法实现最先进的测试时扩展效率。

Result: Falcon-H1R在多种推理密集型基准测试中，性能匹配或超越比它大2-7倍的SOTA模型，实现了推理效率的3D极限（更快推理、更高token效率、更高准确性）。

Conclusion: 紧凑模型通过有针对性的训练策略和架构选择，能够提供强大且可扩展的推理性能，为需要大量思维链生成和并行测试时扩展的先进推理系统提供了实用的骨干模型。

Abstract: This work introduces Falcon-H1R, a 7B-parameter reasoning-optimized model that establishes the feasibility of achieving competitive reasoning performance with small language models (SLMs). Falcon-H1R stands out for its parameter efficiency, consistently matching or outperforming SOTA reasoning models that are $2\times$ to $7\times$ larger across a variety of reasoning-intensive benchmarks. These results underscore the importance of careful data curation and targeted training strategies (via both efficient SFT and RL scaling) in delivering significant performance gains without increasing model size. Furthermore, Falcon-H1R advances the 3D limits of reasoning efficiency by combining faster inference (through its hybrid-parallel architecture design), token efficiency, and higher accuracy. This unique blend makes Falcon-H1R-7B a practical backbone for scaling advanced reasoning systems, particularly in scenarios requiring extensive chain-of-thoughts generation and parallel test-time scaling. Leveraging the recently introduced DeepConf approach, Falcon-H1R achieves state-of-the-art test-time scaling efficiency, offering substantial improvements in both accuracy and computational cost. As a result, Falcon-H1R demonstrates that compact models, through targeted model training and architectural choices, can deliver robust and scalable reasoning performance.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [134] [Beyond Demand Estimation: Consumer Surplus Evaluation via Cumulative Propensity Weights](https://arxiv.org/abs/2601.01029)
*Zeyu Bian,Max Biggs,Ruijiang Gao,Zhengling Qi*

Main category: stat.ML

TL;DR: 提出基于观测数据的消费者剩余审计框架，利用算法定价中的随机性，通过累积倾向权重避免需求函数显式估计和数值积分，实现快速收敛的机器学习方法应用。


<details>
  <summary>Details</summary>
Motivation: 传统方法通过估计需求函数并积分计算消费者剩余，但实践中面临参数模型设定错误、非参数方法数据需求大收敛慢等挑战。需要开发实用框架来审计AI驱动决策（如定向定价和算法贷款）的消费者剩余效应。

Method: 利用现代算法定价中探索与利用权衡产生的随机性，引入累积倾向权重（CPW）重新加权购买结果来重构积分，避免显式需求函数估计。提出增强累积倾向权重（ACPW）双重稳健估计器，只需需求模型或历史定价策略分布之一正确设定即可。扩展框架到不平等感知的剩余度量，量化利润与公平的权衡。

Result: 该方法使灵活机器学习方法能够用于消费者剩余估计，即使机器学习估计收敛较慢也能获得快速收敛率。通过综合数值研究验证了方法的有效性。

Conclusion: 开发了实用的观测数据消费者剩余审计框架，利用算法定价随机性避免传统方法的局限性，支持灵活机器学习应用，并能扩展到公平性考量，为监管者和企业提供量化利润与公平权衡的工具。

Abstract: This paper develops a practical framework for using observational data to audit the consumer surplus effects of AI-driven decisions, specifically in targeted pricing and algorithmic lending. Traditional approaches first estimate demand functions and then integrate to compute consumer surplus, but these methods can be challenging to implement in practice due to model misspecification in parametric demand forms and the large data requirements and slow convergence of flexible nonparametric or machine learning approaches. Instead, we exploit the randomness inherent in modern algorithmic pricing, arising from the need to balance exploration and exploitation, and introduce an estimator that avoids explicit estimation and numerical integration of the demand function. Each observed purchase outcome at a randomized price is an unbiased estimate of demand and by carefully reweighting purchase outcomes using novel cumulative propensity weights (CPW), we are able to reconstruct the integral. Building on this idea, we introduce a doubly robust variant named the augmented cumulative propensity weighting (ACPW) estimator that only requires one of either the demand model or the historical pricing policy distribution to be correctly specified. Furthermore, this approach facilitates the use of flexible machine learning methods for estimating consumer surplus, since it achieves fast convergence rates by incorporating an estimate of demand, even when the machine learning estimate has slower convergence rates. Neither of these estimators is a standard application of off-policy evaluation techniques as the target estimand, consumer surplus, is unobserved. To address fairness, we extend this framework to an inequality-aware surplus measure, allowing regulators and firms to quantify the profit-equity trade-off. Finally, we validate our methods through comprehensive numerical studies.

</details>


### [135] [Fibonacci-Driven Recursive Ensembles: Algorithms, Convergence, and Learning Dynamics](https://arxiv.org/abs/2601.01055)
*Ernest Fokoué*

Main category: stat.ML

TL;DR: 该论文发展了基于斐波那契型更新流的递归集成学习的算法和动力学基础，提出了二阶递归架构，其中每个预测器依赖于其两个直接前驱，相比经典提升方法具有记忆能力。


<details>
  <summary>Details</summary>
Motivation: 经典提升方法（如Freund和Schapire的AdaBoost）使用一阶加性更新，缺乏记忆能力。该研究旨在开发具有记忆的动态集成学习系统，通过递归架构整合过去结构并适应新残差信息。

Method: 引入递归权重更新算法族，涵盖斐波那契、三波那契和高阶递归，建立连续时间极限得到控制集成演化的微分方程系统。使用核岭回归、样条平滑器和随机傅里叶特征模型进行实验验证。

Result: 建立了全局收敛条件、谱稳定性准则和非渐近泛化边界。实验表明递归流在近似和泛化方面持续优于静态加权方法，统一了递归集成、结构化加权和统计学习中的动力系统视角。

Conclusion: 该研究完成了从斐波那契加权、几何加权理论到完全动态递归集成学习系统的三部曲，为具有记忆的动态集成学习提供了理论基础和算法框架。

Abstract: This paper develops the algorithmic and dynamical foundations of recursive ensemble learning driven by Fibonacci-type update flows. In contrast with classical boosting  Freund and Schapire (1997); Friedman (2001), where the ensemble evolves through first-order additive updates, we study second-order recursive architectures in which each predictor depends on its two immediate predecessors. These Fibonacci flows induce a learning dynamic with memory, allowing ensembles to integrate past structure while adapting to new residual information. We introduce a general family of recursive weight-update algorithms encompassing Fibonacci, tribonacci, and higher-order recursions, together with continuous-time limits that yield systems of differential equations governing ensemble evolution. We establish global convergence conditions, spectral stability criteria, and non-asymptotic generalization bounds under Rademacher Bartlett and Mendelson (2002) and algorithmic stability analyses. The resulting theory unifies recursive ensembles, structured weighting, and dynamical systems viewpoints in statistical learning. Experiments with kernel ridge regression Rasmussen and Williams (2006), spline smoothers Wahba (1990), and random Fourier feature models Rahimi and Recht (2007) demonstrate that recursive flows consistently improve approximation and generalization beyond static weighting. These results complete the trilogy begun in Papers I and II: from Fibonacci weighting, through geometric weighting theory, to fully dynamical recursive ensemble learning systems.

</details>


### [136] [Conformal Blindness: A Note on $A$-Cryptic change-points](https://arxiv.org/abs/2601.01147)
*Johan Hallberg Szabadváry*

Main category: stat.ML

TL;DR: 论文研究了Conformal Test Martingales在检测数据可交换性假设时的局限性，发现了即使存在显著的可交换性破坏，p值序列仍可能保持均匀分布的现象，称为"conformal blindness"。


<details>
  <summary>Details</summary>
Motivation: 虽然可交换性假设意味着p值均匀分布，但反之不成立。这引发了一个问题：是否存在显著的可交换性破坏，但p值仍保持均匀分布，使得CTMs无法检测到这种变化？论文旨在探索这种可能性。

Method: 通过理论构造，针对理想的"oracle"一致性度量（由真实条件密度给出），证明了A-cryptic change-point的存在性。使用二元高斯分布，识别出一条边际均值变化但不改变一致性分数分布的直线，从而产生完全均匀的p值。

Result: 模拟实验证实，即使存在巨大的分布偏移，CTM也可能完全无法检测到，这突显了CTMs的基本局限性，并强调了与潜在偏移对齐的一致性度量的关键作用。

Conclusion: 论文揭示了conformal blindness现象，表明CTMs在检测可交换性破坏时存在根本性限制。一致性度量与潜在分布偏移的对齐至关重要，否则即使存在显著变化，CTMs也可能完全失效。

Abstract: Conformal Test Martingales (CTMs) are a standard method within the Conformal Prediction framework for testing the crucial assumption of data exchangeability by monitoring deviations from uniformity in the p-value sequence. Although exchangeability implies uniform p-values, the converse does not hold. This raises the question of whether a significant break in exchangeability can occur, such that the p-values remain uniform, rendering CTMs blind. We answer this affirmatively, demonstrating the phenomenon of \emph{conformal blindness}.
  Through explicit construction, for the theoretically ideal ``oracle'' conformity measure (given by the true conditional density), we demonstrate the possibility of an \emph{$A$-cryptic change-point} (where $A$ refers to the conformity measure). Using bivariate Gaussian distributions, we identify a line along which a change in the marginal means does not alter the distribution of the conformity scores, thereby producing perfectly uniform p-values.
  Simulations confirm that even a massive distribution shift can be perfectly cryptic to the CTM, highlighting a fundamental limitation and emphasising the critical role of the alignment of the conformity measure with potential shifts.

</details>


### [137] [Evidence Slopes and Effective Dimension in Singular Linear Models](https://arxiv.org/abs/2601.01238)
*Kalyaan Rao*

Main category: stat.ML

TL;DR: 贝叶斯模型选择中，拉普拉斯近似和BIC假设有效维度等于参数数量，但在过参数化或秩不足模型中会失效。本文研究线性高斯秩模型，证明拉普拉斯/BIC误差随(d/2-λ)log n线性增长，其中d是参数维度，λ是实对数典范阈值(RLCT)。RLCT感知的修正能恢复正确的证据斜率。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯模型选择方法（拉普拉斯近似和BIC）假设有效模型维度等于参数数量，但在过参数化或秩不足的奇异模型中，这一假设不成立。奇异学习理论用实对数典范阈值(RLCT)作为有效维度，可以严格小于参数数量。需要研究在具体模型中，传统方法的误差如何量化，以及RLCT如何修正这些误差。

Method: 研究线性高斯秩模型和线性子空间（字典）模型，这些模型的精确边缘似然有闭式解，且RLCT可解析计算。通过理论分析和实证研究，量化拉普拉斯/BIC的误差，并开发RLCT感知的修正方法。分析误差如何随(d/2-λ)log n线性增长，其中d是参数维度，λ是RLCT。

Result: 理论上和实证上都证明拉普拉斯/BIC的误差随(d/2-λ)log n线性增长。RLCT感知的修正能恢复正确的证据斜率，并且对表示相同数据子空间的过完备重新参数化具有不变性。在简单线性设置中，证据斜率可作为有效维度的实用估计器。

Conclusion: 本文提供了奇异模型中拉普拉斯近似失败的具体有限样本特征，并证明在简单线性设置中，证据斜率可作为有效维度的实用估计器。RLCT感知的修正方法能正确恢复证据斜率，对过完备重新参数化具有鲁棒性，为奇异模型中的贝叶斯模型选择提供了理论基础和实用工具。

Abstract: Bayesian model selection commonly relies on Laplace approximation or the Bayesian Information Criterion (BIC), which assume that the effective model dimension equals the number of parameters. Singular learning theory replaces this assumption with the real log canonical threshold (RLCT), an effective dimension that can be strictly smaller in overparameterized or rank-deficient models.
  We study linear-Gaussian rank models and linear subspace (dictionary) models in which the exact marginal likelihood is available in closed form and the RLCT is analytically tractable. In this setting, we show theoretically and empirically that the error of Laplace/BIC grows linearly with (d/2 minus lambda) times log n, where d is the ambient parameter dimension and lambda is the RLCT. An RLCT-aware correction recovers the correct evidence slope and is invariant to overcomplete reparameterizations that represent the same data subspace.
  Our results provide a concrete finite-sample characterization of Laplace failure in singular models and demonstrate that evidence slopes can be used as a practical estimator of effective dimension in simple linear settings.

</details>


### [138] [Fast Gibbs Sampling on Bayesian Hidden Markov Model with Missing Observations](https://arxiv.org/abs/2601.01442)
*Dongrong Li,Tianwei Yu,Xiaodan Fan*

Main category: stat.ML

TL;DR: 提出一种折叠Gibbs采样器，通过同时积分缺失观测值和潜在状态，高效地从HMM后验分布中采样，在缺失数据较多时具有显著计算优势。


<details>
  <summary>Details</summary>
Motivation: HMM在处理序列数据时广泛应用，但实际数据中普遍存在缺失观测值，使得模型应用复杂化。传统的EM算法和Gibbs采样器存在非凸性、高计算复杂度和混合速度慢等问题。

Method: 提出一种折叠Gibbs采样器，通过同时积分掉缺失观测值和对应的潜在状态，直接从HMM后验分布中采样。该方法具有三个优势：估计精度与现有方法相当；每次迭代能产生更大的有效样本量；当缺失条目较多时，每次迭代的计算复杂度显著降低。

Result: 通过数值模拟和真实数据分析的实证评估表明，所提算法在时间复杂度和采样效率（以ESS衡量）方面始终优于现有算法。

Conclusion: 所提出的采样算法在计算和理论上都更快，特别是在存在大量缺失条目时具有明显优势，为处理含缺失数据的HMM提供了一种高效解决方案。

Abstract: The Hidden Markov Model (HMM) is a widely-used statistical model for handling sequential data. However, the presence of missing observations in real-world datasets often complicates the application of the model. The EM algorithm and Gibbs samplers can be used to estimate the model, yet suffering from various problems including non-convexity, high computational complexity and slow mixing. In this paper, we propose a collapsed Gibbs sampler that efficiently samples from HMMs' posterior by integrating out both the missing observations and the corresponding latent states. The proposed sampler is fast due to its three advantages. First, it achieves an estimation accuracy that is comparable to existing methods. Second, it can produce a larger Effective Sample Size (ESS) per iteration, which can be justified theoretically and numerically. Third, when the number of missing entries is large, the sampler has a significant smaller computational complexity per iteration compared to other methods, thus is faster computationally. In summary, the proposed sampling algorithm is fast both computationally and theoretically and is particularly advantageous when there are a lot of missing entries. Finally, empirical evaluations based on numerical simulations and real data analysis demonstrate that the proposed algorithm consistently outperforms existing algorithms in terms of time complexity and sampling efficiency (measured in ESS).

</details>


### [139] [Variance-Reduced Diffusion Sampling via Conditional Score Expectation Identity](https://arxiv.org/abs/2601.01594)
*Alois Duston,Tan Bui-Thanh*

Main category: stat.ML

TL;DR: 提出并证明了条件得分期望(CSE)恒等式，基于此开发了CSE得分估计器，通过最优混合CSE和Tweedie估计器降低方差，并扩展到贝叶斯逆问题


<details>
  <summary>Details</summary>
Motivation: 针对仿射扩散过程的边际得分估计问题，传统Tweedie估计器存在局限性，需要更有效的得分估计方法以降低方差并提高采样质量

Method: 1. 提出并证明条件得分期望(CSE)恒等式；2. 基于CSE开发自归一化重要性采样(SNIS)得分估计器；3. 分析CSE与Tweedie估计器的关系；4. 推导方差最小化的最优混合得分估计器；5. 扩展到贝叶斯逆问题，使用似然信息化的SNIS权重

Result: 1. 证明了CSE恒等式；2. 对于高斯目标，CSE与Tweedie估计器呈负相关；3. 小时间步长下一般目标也有相同行为；4. 最优混合估计器相比基线方法降低了方差；5. 在固定计算预算下提高了采样质量；6. 在高维图像重建和PDE控制逆问题上展示了改进的重建质量和样本多样性

Conclusion: CSE恒等式为得分估计提供了新视角，基于CSE的最优混合估计器能有效降低方差并提高采样效率，该框架可成功扩展到贝叶斯逆问题，在高维任务中表现出色

Abstract: We introduce and prove a \textbf{Conditional Score Expectation (CSE)} identity: an exact relation for the marginal score of affine diffusion processes that links scores across time via a conditional expectation under the forward dynamics. Motivated by this identity, we propose a CSE-based statistical estimator for the score using a Self-Normalized Importance Sampling (SNIS) procedure with prior samples and forward noise. We analyze its relationship to the standard Tweedie estimator, proving anti-correlation for Gaussian targets and establishing the same behavior for general targets in the small time-step regime. Exploiting this structure, we derive a variance-minimizing blended score estimator given by a state--time dependent convex combination of the CSE and Tweedie estimators. Numerical experiments show that this optimal-blending estimator reduces variance and improves sample quality for a fixed computational budget compared to either baseline. We further extend the framework to Bayesian inverse problems via likelihood-informed SNIS weights, and demonstrate improved reconstruction quality and sample diversity on high-dimensional image reconstruction tasks and PDE-governed inverse problems.

</details>


### [140] [Deep Linear Discriminant Analysis Revisited](https://arxiv.org/abs/2601.01619)
*Maxat Tezekbayev,Rustem Takhanov,Arman Bolatov,Zhenisbek Assylbekov*

Main category: stat.ML

TL;DR: 论文发现深度线性判别分析（LDA）在最大似然训练下存在病态解，而交叉熵训练虽准确但破坏了生成模型结构。作者提出判别式负对数似然（DNLL）损失函数，在保持判别性能的同时恢复生成模型的概率解释。


<details>
  <summary>Details</summary>
Motivation: 传统深度LDA训练存在两个问题：最大似然训练会导致类别均值漂移、协方差坍缩，学习到的表示几乎没有判别性；而交叉熵训练虽然准确率高，但使分类头与底层生成模型解耦，导致参数估计高度不一致。需要一种既能保持生成模型结构又能获得良好判别性能的方法。

Method: 提出判别式负对数似然（DNLL）损失函数，在标准LDA负对数似然基础上增加一个惩罚项，该惩罚项明确阻止多个类别同时可能出现的区域。DNLL可以解释为在混合密度上添加简单惩罚的LDA对数似然。

Result: 使用DNLL训练的深度LDA产生干净、良好分离的潜在空间，在合成数据和标准图像基准测试中匹配softmax分类器的测试准确率，并产生显著更好的校准预测概率，为深度判别模型恢复了连贯的概率解释。

Conclusion: DNLL损失函数成功调和了生成模型结构与判别性能之间的矛盾，使深度判别模型既能保持生成模型的概率解释，又能获得与softmax分类器相当的判别准确率。

Abstract: We show that for unconstrained Deep Linear Discriminant Analysis (LDA) classifiers, maximum-likelihood training admits pathological solutions in which class means drift together, covariances collapse, and the learned representation becomes almost non-discriminative. Conversely, cross-entropy training yields excellent accuracy but decouples the head from the underlying generative model, leading to highly inconsistent parameter estimates. To reconcile generative structure with discriminative performance, we introduce the \emph{Discriminative Negative Log-Likelihood} (DNLL) loss, which augments the LDA log-likelihood with a simple penalty on the mixture density. DNLL can be interpreted as standard LDA NLL plus a term that explicitly discourages regions where several classes are simultaneously likely. Deep LDA trained with DNLL produces clean, well-separated latent spaces, matches the test accuracy of softmax classifiers on synthetic data and standard image benchmarks, and yields substantially better calibrated predictive probabilities, restoring a coherent probabilistic interpretation to deep discriminant models.

</details>


### [141] [Simplex Deep Linear Discriminant Analysis](https://arxiv.org/abs/2601.01679)
*Maxat Tezekbayev,Arman Bolatov,Zhenisbek Assylbekov*

Main category: stat.ML

TL;DR: 本文重新审视深度线性判别分析(Deep LDA)，从似然角度分析其训练问题，发现无约束的端到端最大似然估计会导致类别重叠或崩溃，提出几何约束的Deep LDA方案，在保持竞争力的准确率同时获得可解释的潜在空间几何结构。


<details>
  <summary>Details</summary>
Motivation: 传统LDA是简单的线性高斯模型，但当将LDA头部连接到神经编码器时，如何通过最大似然估计训练这种深度分类器成为一个问题。作者旨在解决深度LDA在端到端训练中的退化问题。

Method: 首先分析无约束Deep LDA的MLE训练问题，发现会导致类别重叠或崩溃。然后提出约束Deep LDA方案：将类别均值固定在潜在空间的正则单纯形顶点上，限制共享协方差为球形，只学习先验和单个方差参数以及编码器参数。

Result: 在图像数据集(Fashion-MNIST, CIFAR-10, CIFAR-100)上，约束Deep LDA模型达到与softmax基线相当的准确率，同时提供了简单、可解释的潜在空间几何结构，在二维投影中清晰可见。

Conclusion: 通过几何约束解决深度LDA的MLE训练退化问题，在保持分类性能的同时获得结构化的潜在表示，为深度分类器提供了新的可解释性视角。

Abstract: We revisit Deep Linear Discriminant Analysis (Deep LDA) from a likelihood-based perspective. While classical LDA is a simple Gaussian model with linear decision boundaries, attaching an LDA head to a neural encoder raises the question of how to train the resulting deep classifier by maximum likelihood estimation (MLE). We first show that end-to-end MLE training of an unconstrained Deep LDA model ignores discrimination: when both the LDA parameters and the encoder parameters are learned jointly, the likelihood admits a degenerate solution in which some of the class clusters may heavily overlap or even collapse, and classification performance deteriorates. Batchwise moment re-estimation of the LDA parameters does not remove this failure mode. We then propose a constrained Deep LDA formulation that fixes the class means to the vertices of a regular simplex in the latent space and restricts the shared covariance to be spherical, leaving only the priors and a single variance parameter to be learned along with the encoder. Under these geometric constraints, MLE becomes stable and yields well-separated class clusters in the latent space. On images (Fashion-MNIST, CIFAR-10, CIFAR-100), the resulting Deep LDA models achieve accuracy competitive with softmax baselines while offering a simple, interpretable latent geometry that is clearly visible in two-dimensional projections.

</details>


### [142] [Sparse Convex Biclustering](https://arxiv.org/abs/2601.01757)
*Jiakun Jiang,Dewei Xiang,Chenliang Gu,Wei Liu,Binhuan Wang*

Main category: stat.ML

TL;DR: 提出SpaCoBi方法，通过凸优化框架和稳定性调优准则，在高维大规模数据中实现更准确、鲁棒的双聚类


<details>
  <summary>Details</summary>
Motivation: 现有双聚类方法在处理现代大规模高维数据时面临挑战：高维特征噪声累积、非凸优化公式限制、计算复杂度高，导致准确性和稳定性随数据规模增大而下降

Method: 提出稀疏凸双聚类(SpaCoBi)，采用凸优化框架，在双聚类过程中惩罚噪声，引入基于稳定性的调优准则，平衡聚类保真度和稀疏性

Result: 通过综合数值研究（包括模拟实验和小鼠嗅球数据分析）表明，SpaCoBi在准确性方面显著优于现有最先进方法

Conclusion: SpaCoBi为高维大规模数据集中的双聚类提供了一个鲁棒高效的解决方案

Abstract: Biclustering is an essential unsupervised machine learning technique for simultaneously clustering rows and columns of a data matrix, with widespread applications in genomics, transcriptomics, and other high-dimensional omics data. Despite its importance, existing biclustering methods struggle to meet the demands of modern large-scale datasets. The challenges stem from the accumulation of noise in high-dimensional features, the limitations of non-convex optimization formulations, and the computational complexity of identifying meaningful biclusters. These issues often result in reduced accuracy and stability as the size of the dataset increases. To overcome these challenges, we propose Sparse Convex Biclustering (SpaCoBi), a novel method that penalizes noise during the biclustering process to improve both accuracy and robustness. By adopting a convex optimization framework and introducing a stability-based tuning criterion, SpaCoBi achieves an optimal balance between cluster fidelity and sparsity. Comprehensive numerical studies, including simulations and an application to mouse olfactory bulb data, demonstrate that SpaCoBi significantly outperforms state-of-the-art methods in accuracy. These results highlight SpaCoBi as a robust and efficient solution for biclustering in high-dimensional and large-scale datasets.

</details>


### [143] [A Multilayered Approach to Classifying Customer Responsiveness and Credit Risk](https://arxiv.org/abs/2601.01970)
*Ayomide Afolabi,Ebere Ogburu,Symon Kimitei*

Main category: stat.ML

TL;DR: 研究评估了三种模型（响应模型、风险模型、响应-风险模型）中不同分类器在信用卡邮件营销和违约预测中的性能表现，发现Extra Trees在响应识别上表现最佳，Random Forest在风险识别和多分类任务中表现最优。


<details>
  <summary>Details</summary>
Motivation: 解决信用卡业务中的两个核心问题：1）如何有效识别对邮件营销活动有响应的潜在客户；2）如何准确预测客户的违约风险。通过优化不同性能指标来满足特定的信用风险和邮件响应业务需求。

Method: 采用三种不同的模型框架：响应模型（预测客户对信用卡邮件营销的响应）、风险模型（预测客户违约风险）、响应-风险多分类模型。在每种模型中使用多种分类器（包括Extra Trees和Random Forest等）进行性能比较，针对不同业务目标优化相应的性能指标。

Result: 1. 响应模型中：Extra Trees分类器获得最高召回率79.1%，在识别潜在响应者方面表现最佳；2. 风险模型中：Random Forest分类器获得最高特异性84.1%，在识别低违约风险客户方面表现突出；3. 响应-风险多分类模型中：Random Forest分类器获得最高准确率83.2%，在同时识别响应者和低风险用户方面表现最优。

Conclusion: 不同分类器在不同业务场景下各有优势：Extra Trees适合识别邮件营销响应者，Random Forest适合风险预测和多分类任务。研究为信用卡业务提供了针对性的分类器选择策略，可根据具体业务目标（响应率最大化或风险最小化）选择最合适的模型。

Abstract: This study evaluates the performance of various classifiers in three distinct models: response, risk, and response-risk, concerning credit card mail campaigns and default prediction. In the response model, the Extra Trees classifier demonstrates the highest recall level (79.1%), emphasizing its effectiveness in identifying potential responders to targeted credit card offers. Conversely, in the risk model, the Random Forest classifier exhibits remarkable specificity of 84.1%, crucial for identifying customers least likely to default. Furthermore, in the multi-class response-risk model, the Random Forest classifier achieves the highest accuracy (83.2%), indicating its efficacy in discerning both potential responders to credit card mail campaign and low-risk credit card users. In this study, we optimized various performance metrics to solve a specific credit risk and mail responsiveness business problem.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [144] [Harm in AI-Driven Societies: An Audit of Toxicity Adoption on Chirper.ai](https://arxiv.org/abs/2601.01090)
*Erica Coppolillo,Luca Luceri,Emilio Ferrara*

Main category: cs.MA

TL;DR: 研究LLM驱动的AI智能体在完全由AI组成的社交平台Chirper.ai上的毒性采纳行为，发现暴露于有害内容会显著增加智能体产生毒性回应的概率，但相当一部分毒性是自发产生的。


<details>
  <summary>Details</summary>
Motivation: 虽然已有研究记录了LLM生成有毒内容的现象，但对于暴露于有害内容如何随时间塑造智能体行为，特别是在完全由交互AI智能体组成的环境中，了解甚少。本研究旨在探索LLM驱动的智能体在AI驱动的社交平台上的毒性采纳行为。

Method: 在Chirper.ai（一个完全由AI驱动的社交平台）上进行大规模实证分析，将交互建模为刺激（帖子）和回应（评论），通过可观察的交互而非推断的推荐机制来操作化暴露概念。研究考察了回应毒性与刺激毒性的关系、重复暴露对毒性回应可能性的影响，以及是否仅从暴露就能预测毒性行为。

Result: 研究发现：毒性回应更可能出现在毒性刺激之后，但相当一部分毒性是自发产生的，与暴露无关；累积的毒性暴露显著增加了毒性回应的概率；引入的两个影响指标（影响驱动回应率和自发回应率）揭示了诱导毒性与自发毒性之间的强烈权衡；仅凭毒性刺激的数量就能准确预测智能体最终是否会产生毒性内容。

Conclusion: 暴露是部署LLM智能体的关键风险因素，监测遇到的内容可能为审计和缓解野外有害行为提供轻量级但有效的机制。这些发现强调了在AI驱动的社交环境中监控和管理内容暴露的重要性。

Abstract: Large Language Models (LLMs) are increasingly embedded in autonomous agents that participate in online social ecosystems, where interactions are sequential, cumulative, and only partially controlled. While prior work has documented the generation of toxic content by LLMs, far less is known about how exposure to harmful content shapes agent behavior over time, particularly in environments composed entirely of interacting AI agents. In this work, we study toxicity adoption of LLM-driven agents on Chirper.ai, a fully AI-driven social platform. Specifically, we model interactions in terms of stimuli (posts) and responses (comments), and by operationalizing exposure through observable interactions rather than inferred recommendation mechanisms.
  We conduct a large-scale empirical analysis of agent behavior, examining how response toxicity relates to stimulus toxicity, how repeated exposure affects the likelihood of toxic responses, and whether toxic behavior can be predicted from exposure alone. Our findings show that while toxic responses are more likely following toxic stimuli, a substantial fraction of toxicity emerges spontaneously, independent of exposure. At the same time, cumulative toxic exposure significantly increases the probability of toxic responding. We further introduce two influence metrics, the Influence-Driven Response Rate and the Spontaneous Response Rate, revealing a strong trade-off between induced and spontaneous toxicity. Finally, we show that the number of toxic stimuli alone enables accurate prediction of whether an agent will eventually produce toxic content.
  These results highlight exposure as a critical risk factor in the deployment of LLM agents and suggest that monitoring encountered content may provide a lightweight yet effective mechanism for auditing and mitigating harmful behavior in the wild.

</details>


### [145] [CONSENT: A Negotiation Framework for Leveraging User Flexibility in Vehicle-to-Building Charging under Uncertainty](https://arxiv.org/abs/2601.01581)
*Rishav Sen,Fangqi Liu,Jose Paolo Talusan,Ava Pettet,Yoshinori Suzue,Mark Bailey,Ayan Mukhopadhyay,Abhishek Dubey*

Main category: cs.MA

TL;DR: 提出基于谈判的电动汽车充电框架，通过激励措施协调建筑运营商和车主利益，实现成本节约和用户便利的双赢


<details>
  <summary>Details</summary>
Motivation: 电动汽车快速增长导致车辆到建筑(V2B)场景中建筑运营商面临高能耗成本与车主追求便利性和满电需求之间的冲突，需要协调双方利益

Method: 设计谈判框架，通过提供激励措施让车主在离场时间或充电状态上提供灵活性，保证自愿参与、策略证明和预算可行性，使用用户调查数据和实际运营数据进行校准验证

Result: 模拟显示谈判协议实现双赢：建筑运营商成本比优化的非谈判智能充电策略降低3.5%以上，用户充电费用比公用事业零售电价降低22%

Conclusion: 该框架通过协调运营商和电动汽车用户目标，在能源和交通系统间建立战略桥梁，将电动汽车充电从运营摩擦转变为协作和共享节约的平台

Abstract: The growth of Electric Vehicles (EVs) creates a conflict in vehicle-to-building (V2B) settings between building operators, who face high energy costs from uncoordinated charging, and drivers, who prioritize convenience and a full charge. To resolve this, we propose a negotiation-based framework that, by design, guarantees voluntary participation, strategy-proofness, and budget feasibility. It transforms EV charging into a strategic resource by offering drivers a range of incentive-backed options for modest flexibility in their departure time or requested state of charge (SoC). Our framework is calibrated with user survey data and validated using real operational data from a commercial building and an EV manufacturer. Simulations show that our negotiation protocol creates a mutually beneficial outcome: lowering the building operator's costs by over 3.5\% compared to an optimized, non-negotiating smart charging policy, while simultaneously reducing user charging expenses by 22\% below the utility's retail energy rate. By aligning operator and EV user objectives, our framework provides a strategic bridge between energy and mobility systems, transforming EV charging from a source of operational friction into a platform for collaboration and shared savings.

</details>


### [146] [ARIES: A Scalable Multi-Agent Orchestration Framework for Real-Time Epidemiological Surveillance and Outbreak Monitoring](https://arxiv.org/abs/2601.01831)
*Aniket Wattamwar,Sampson Akwafuo*

Main category: cs.MA

TL;DR: ARIES是一个专门用于流行病学监测的自主多智能体框架，通过分层命令结构协调多个子智能体，自动查询WHO、CDC和学术论文数据，实现实时威胁检测和信号分析。


<details>
  <summary>Details</summary>
Motivation: 当前全球健康监测面临知识鸿沟问题，通用AI存在幻觉问题且无法处理专业数据孤岛，需要专门的高风险流行病学领域解决方案。

Method: 基于分层命令结构的多智能体框架，使用GPT协调可扩展的子智能体群，自动查询WHO、CDC和同行评审研究论文，实现数据提取和逻辑合成。

Result: ARIES能够超越通用模型，提供专门推理能力，近乎实时地识别新发威胁和信号分歧，为下一代疫情响应提供可扩展的解决方案。

Conclusion: 任务特定的智能体群架构证明能够超越通用模型，为全球健康情报提供稳健、可扩展的下一代疫情响应系统。

Abstract: Global health surveillance is currently facing a challenge of Knowledge Gaps. While general-purpose AI has proliferated, it remains fundamentally unsuited for the high-stakes epidemiological domain due to chronic hallucinations and an inability to navigate specialized data silos. This paper introduces ARIES (Agentic Retrieval Intelligence for Epidemiological Surveillance), a specialized, autonomous multi-agent framework designed to move beyond static, disease-specific dashboards toward a dynamic intelligence ecosystem. Built on a hierarchical command structure, ARIES utilizes GPTs to orchestrate a scalable swarm of sub-agents capable of autonomously querying World Health Organization (WHO), Center for Disease Control and Prevention (CDC), and peer-reviewed research papers. By automating the extraction and logical synthesis of surveillance data, ARIES provides a specialized reasoning that identifies emergent threats and signal divergence in near real-time. This modular architecture proves that a task-specific agentic swarm can outperform generic models, offering a robust, extensible for next-generation outbreak response and global health intelligence.

</details>
